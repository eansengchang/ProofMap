[
    {
        "type": "definition",
        "id": "Definition 0.1",
        "name": "Galton-Watson Branching Process",
        "topic": "Branching Processes",
        "previous_results": [],
        "statement": "Let $(X_{n,r})_{n,r\\\\geq1}$ be an infinite array of independent identically distributed random variables, each with the same distribution as $X$, where $P[X = k] = p_k$, $k = 0, 1, 2, \\\\ldots$ The sequence $(Z_n)_{n\\\\geq0}$ of random variables defined by\\n1. $Z_0 = 1$,\\n2. $Z_n = X_{n,1} + \\\\cdot \\\\cdot \\\\cdot + X_{n,Z_{n-1}}$ for $n \\\\geq 1$\\nis the Galton\u2013Watson branching process (started from a single ancestor) with offspring distribution $X$.",
        "proof": ""
    },
    {
        "type": "claim",
        "id": "Claim 0.2",
        "name": "PGF Composition",
        "topic": "Branching Processes",
        "previous_results": [],
        "statement": "Let $f_n(\\\\theta) = E[\\\\theta^{Z_n}]$. Then $f_n$ is the $n$-fold composition of $f$ with itself (where by convention a 0-fold composition is the identity).",
        "proof": "We proceed by induction. First note that $f_0(\\\\theta) = \\\\theta$, so $f_0$ is the identity. Assume that $n \\\\geq 1$ and $f_{n-1} = f \\\\circ \\\\cdots \\\\circ f$ is the $(n-1)$-fold composition of $f$ with itself. To compute $f_n$, first note that\\n\\n$E[\\\\theta^{Z_n} | Z_{n-1} = k] = E[\\\\theta^{X_{n,1}+\\\\cdots+X_{n,k}}] = E[\\\\theta^{X_{n,1}}] \\\\cdots E[\\\\theta^{X_{n,k}}] = f(\\\\theta)^k$,\\n\\n(by independence and since each $X_{n,i}$ has the same distribution as $X$). Hence\\n\\n$E[\\\\theta^{Z_n} | Z_{n-1}] = f(\\\\theta)^{Z_{n-1}}$.\\n\\nThis is our first example of a conditional expectation. Notice that the right hand side is a random variable. Now\\n\\n$f_n(\\\\theta) = E[\\\\theta^{Z_n}] = E[E[\\\\theta^{Z_n} | Z_{n-1}]] = E[f(\\\\theta)^{Z_{n-1}}] = f_{n-1}(f(\\\\theta))$,\\n\\nand the claim follows by induction."
    },
    {
        "type": "claim",
        "id": "Claim 0.3",
        "name": "Extinction Probability",
        "topic": "Branching Processes",
        "previous_results": [
            "Claim 0.2"
        ],
        "statement": "Let $q = P[Z_n = 0 \\\\text{ for some } n]$. Then $q$ is the smallest root in $[0, 1]$ of the equation $\\\\theta = f(\\\\theta)$. In particular, assuming $p_1 = P[X = 1] < 1$,\\n- if $m = E[X] \\\\leq 1$, then $q = 1$,\\n- if $m = E[X] > 1$, then $q < 1$.",
        "proof": "Let $q_n = P[Z_n = 0] = f_n(0)$. Since $\\\\{Z_n = 0\\\\} \\\\subseteq \\\\{Z_{n+1} = 0\\\\}$ we see that $q_n$ is an increasing function of $n$ and, intuitively,\\n\\n$q = \\\\lim_{n\\\\to\\\\infty} q_n = \\\\lim_{n\\\\to\\\\infty} f_n(0)$.\\n\\nSince $f_{n+1}(0) = f(f_n(0))$ and $f$ is continuous, this implies that $q$ satisfies $q = f(q)$.\\n\\nNow observe that $f$ is convex (i.e., $f''\\\\geq 0$) and $f(1) = 1$, so only two things can happen, depending upon the value of $m = f'(1)$:\\n\\nIn the case $m > 1$, to see that $q$ must be the smaller root $\\\\theta_0$, note that $f$ is increasing, and $0 = q_0 \\\\leq \\\\theta_0$. It follows by induction that $q_n \\\\leq \\\\theta_0$ for all $n$, so $q \\\\leq \\\\theta_0$."
    },
    {
        "type": "proposition",
        "id": "Proposition 0.4",
        "name": "Binomial Model Pricing",
        "topic": "Mathematical Finance",
        "previous_results": [],
        "statement": "Suppose there exist two constants $u, d$ such that $0 < 1 - d < 1 < 1 + r < 1 + u$ and $S_{n+1} \\\\in \\\\{(1 + u)S_n, (1 - d)S_n\\\\}$ a.s., for all $n \\\\geq 0$. Then for any $f$, there exists $V_0, H$ such that $f = V_N$ a.s. In addition, there exists a unique probability measure $Q$ such that $(\\\\tilde{S}_n)_{n\\\\geq0}$ is a $Q$-martingale and $V_0 = (1 + r)^{-N}E_Q[f(S_0, \\\\ldots, S_N)]$.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 1.1",
        "name": "Algebras and \u03c3-algebras",
        "topic": "Measurable Sets and Functions",
        "previous_results": [],
        "statement": "Let $\\\\Omega$ be a set and let $A \\\\subseteq P(\\\\Omega)$ be a collection of subsets of $\\\\Omega$.\\n\\n1. We say that $A$ is an algebra if $/0 \\\\in A$ and for all $A, B \\\\in A$, $A^c = \\\\Omega \\\\\\\\ A \\\\in A$ and $A \\\\cup B \\\\in A$.\\n\\n2. We say that $A$ is a $\\\\sigma$-algebra (or a $\\\\sigma$-field) if $/0 \\\\in A$, $A \\\\in A$ implies $A^c \\\\in A$, and for all sequences $(A_n)_{n\\\\geq1}$ of elements of $A$, $\\\\bigcup_{n=1}^{\\\\infty} A_n \\\\in A$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 1.2",
        "name": "\u03c3-algebra Examples",
        "topic": "Measurable Sets and Functions",
        "previous_results": [
            "Definition 1.1"
        ],
        "statement": "Here are some examples of $\\\\sigma$-algebras:\\n\\n(i) $\\\\{/0, \\\\Omega\\\\}$ is a $\\\\sigma$-algebra. It is often referred to as the trivial $\\\\sigma$-algebra and it is the smallest possible $\\\\sigma$-algebra since, by definition, $\\\\{/0, \\\\Omega\\\\} \\\\subseteq F$ for any $\\\\sigma$-algebra $F$.\\n\\n(ii) The power set $P(\\\\Omega)$ is a $\\\\sigma$-algebra but is usually too large to work with.\\n\\n(iii) Let $E \\\\subset \\\\Omega$ be any set and $F$ be a $\\\\sigma$-algebra. Then $\\\\{E \\\\cap A : A \\\\in F\\\\}$ is a $\\\\sigma$-algebra. It is sometimes called the trace $\\\\sigma$-algebra.\\n\\n(iv) The collection of all sets $A \\\\in P(\\\\Omega)$ such that either $A$ or $A^c$ is countable is a $\\\\sigma$-algebra.\\n\\n(v) For a nontrivial set $A \\\\subseteq \\\\Omega$, i.e., $A$ is neither empty nor the full space, $\\\\sigma(A) := \\\\{/0, \\\\Omega, A, A^c\\\\}$ is a $\\\\sigma$-algebra. It just allows us to say if the event $A$ happened or not but nothing else.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 1.3",
        "name": "Intersection of \u03c3-algebras",
        "topic": "Measurable Sets and Functions",
        "previous_results": [
            "Definition 1.1"
        ],
        "statement": "Let $I$ be an index set and $\\\\{F_i : i \\\\in I\\\\}$ a collection of $\\\\sigma$-algebras. Then\\n\\n$F := \\\\bigcap_{i\\\\in I} F_i = \\\\{A \\\\subseteq \\\\Omega : A \\\\in F_i \\\\text{ for all } i \\\\in I\\\\}$\\n\\nis a $\\\\sigma$-algebra.",
        "proof": "Exercise."
    },
    {
        "type": "definition",
        "id": "Definition 1.4",
        "name": "Generated \u03c3-algebra",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Lemma 1.3"
        ],
        "statement": "Let $A$ be a collection of subsets of $\\\\Omega$. The smallest $\\\\sigma$-algebra containing all the sets in $A$ is denoted $\\\\sigma(A)$ and is called the $\\\\sigma$-algebra generated by $A$.",
        "proof": "Note that Lemma 1.3 ensures that $\\\\sigma(A)$ is well defined and is simply given by the intersection of all the $\\\\sigma$-algebras $F$ such that $A \\\\subseteq F$, a non-empty collection since $A \\\\subseteq P(\\\\Omega)$. This result allows us instantly to generate many more interesting $\\\\sigma$-algebras."
    },
    {
        "type": "definition",
        "id": "Definition 1.5",
        "name": "Borel \u03c3-algebra",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Definition 1.4"
        ],
        "statement": "Let $E$ be a topological space with topology (i.e., collection of open sets) $T$. The $\\\\sigma$-algebra generated by the open sets in $E$ is called the Borel $\\\\sigma$-algebra on $E$ and is denoted $B(E) = \\\\sigma(T)$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 1.6",
        "name": "Borel \u03c3-algebra on R",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Definition 1.5"
        ],
        "statement": "The following collections of sets\\n\\n$\\\\bullet$ open sets in $\\\\mathbb{R}$,\\n\\n$\\\\bullet$ open intervals in $\\\\mathbb{R}$,\\n\\n$\\\\bullet$ $\\\\{(-\\\\infty, a] : a \\\\in \\\\mathbb{R}\\\\}$,\\n\\n$\\\\bullet$ $\\\\{(-\\\\infty, a) : a \\\\in \\\\mathbb{R}\\\\}$\\n\\nall generate the same $\\\\sigma$-algebra, namely $B(\\\\mathbb{R})$.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 1.7",
        "name": "Product space",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Definition 1.4"
        ],
        "statement": "Let $I$ be an index set and $(\\\\Omega_i, F_i)_{i\\\\in I}$ a collection of measurable spaces. Let $\\\\Omega = \\\\prod_{i\\\\in I} \\\\Omega_i$ and $F$ be the $\\\\sigma$-algebra generated by cylinder sets $A = \\\\prod_{i\\\\in I} A_i$, where $A_i \\\\in F_i$ for all $i \\\\in I$ and $A_i = \\\\Omega_i$ except for finitely many $i \\\\in I$. The measurable space $(\\\\Omega, F)$ is called the product space. The $\\\\sigma$-algebra $F$ is called the product $\\\\sigma$-algebra and is sometimes denoted $\\\\times_{i\\\\in I}F_i$.",
        "proof": "When $I = \\\\{1, 2\\\\}$, we simply write $\\\\Omega = \\\\Omega_1 \\\\times \\\\Omega_2$ and $F = F_1 \\\\times F_2$. Note that '$\\\\times$' has a different meaning for these 'products': $\\\\Omega$ is the Cartesian product of $\\\\Omega_1$ and $\\\\Omega_2$ but $F$ is not the Cartesian product of $F_1$ and $F_2$. It is often the case that the same $\\\\sigma(A)$ may be generated by many different classes of sets $A$. For example, the product $\\\\sigma$-algebra is already generated by sets where $A_i \\\\neq \\\\Omega_i$ for only one coordinate $i \\\\in I$. This is obvious since $\\\\sigma$-algebras are closed under finite intersections so we may get the more general cylinder sets from these simple ones. Example 1.6 was also an instance of this phenomena. This example in fact extends to higher dimensions, i.e., to products of $\\\\mathbb{R}$. Indeed, each open subset of $\\\\mathbb{R}^n$ is a countable union of open hypercubes (products of open intervals) and hence $B(\\\\mathbb{R}^d)$ is generated by $d$-fold products of open intervals. It follows that $B(\\\\mathbb{R}^d) = \\\\times_{i=1}^d B(\\\\mathbb{R})$ and properties of product spaces will allow us to just focus on real-valued objects. While this will carry over to countable product spaces, it may fail for more general index sets."
    },
    {
        "type": "example",
        "id": "Example 1.8",
        "name": "Repeated coin tossing",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Definition 1.7"
        ],
        "statement": "Consider the experiment consisting in repeated coin tossing. Each toss is naturally represented by $(\\\\Omega_{\\\\text{toss}}, F_{\\\\text{toss}})$ with $\\\\Omega_{\\\\text{toss}} = \\\\{H, T\\\\}$ and $F_{\\\\text{toss}} = \\\\sigma(\\\\{H\\\\}) = \\\\sigma(\\\\{T\\\\}) = \\\\{\\\\emptyset, \\\\Omega_{\\\\text{toss}}, \\\\{H\\\\}, \\\\{T\\\\}\\\\} = P(\\\\Omega_{\\\\text{toss}})$. Repeated coin tossing is then captured by the product space $(\\\\Omega, F) = (\\\\prod_{n=1}^{\\\\infty} \\\\Omega_n, \\\\times_{n=1}^{\\\\infty} F_n)$ where each $(\\\\Omega_n, F_n) = (\\\\Omega_{\\\\text{toss}}, F_{\\\\text{toss}})$. Put differently, $\\\\Omega = \\\\{H, T\\\\}^{\\\\mathbb{N}}$ and $\\\\omega = (\\\\omega_1, \\\\omega_2, \\\\ldots) \\\\in \\\\Omega$ encodes the outcomes of successive tosses. The product $\\\\sigma$-algebra $F$ on $\\\\Omega$ is generated by events which only depend on the outcomes of finitely many tosses. As observed above, it is in fact generated by the events $A_n = \\\\{\\\\omega \\\\in \\\\Omega : \\\\omega_n = H\\\\}$, i.e., by events which allows us to encode the result of the $n$th toss, $n \\\\in \\\\mathbb{N}$. It is clear that for our measurable space to describe our experiment we have to have these in $F$. It turns out we can not have much more: $F$ is strictly smaller than $P(\\\\Omega)$ and it may be impossible to understand and codify the likelihood of evens from outside of $F$. However, $F$ proves already to be (perhaps surprisingly) rich. In particular the event $A$ that the asymptotic frequency of heads is equal to $\\\\frac{1}{2}$, or more formally $A = \\\\{\\\\omega \\\\in \\\\Omega : \\\\frac{|\\\\{k \\\\leq n : \\\\omega_k = H\\\\}|}{n} \\\\to \\\\frac{1}{2}\\\\}$ is an element in $F$, see the problem sheet.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 1.9",
        "name": "\u03c0- and \u03bb-systems",
        "topic": "Measurable sets and functions",
        "previous_results": [],
        "statement": "Let $\\\\Omega$ be a set and $\\\\mathcal{A}$ be a collection of subsets of $\\\\Omega$.\\n\\n\u2022 A collection of sets $\\\\mathcal{A}$ is called a $\\\\pi$-system if it is stable under intersections, i.e., $A, B \\\\in \\\\mathcal{A}$ implies $A \\\\cap B \\\\in \\\\mathcal{A}$.\\n\\n\u2022 A collection of sets $\\\\mathcal{M}$ is called a $\\\\lambda$-system if\\n  \u2013 $\\\\Omega \\\\in \\\\mathcal{M}$,\\n  \u2013 if $A, B \\\\in \\\\mathcal{M}$ with $A \\\\subseteq B$ then $B \\\\setminus A \\\\in \\\\mathcal{M}$,\\n  \u2013 if $\\\\{A_n\\\\}_{n\\\\geq1} \\\\subseteq \\\\mathcal{M}$ with $A_n \\\\subseteq A_{n+1}$ for all $n \\\\geq 1$ then $\\\\bigcup_{n\\\\geq1} A_n \\\\in \\\\mathcal{M}$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 1.10",
        "name": "\u03c0-system of R",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Example 1.6"
        ],
        "statement": "The collection $\\\\pi(\\\\mathbb{R}) = \\\\{(-\\\\infty, x] : x \\\\in \\\\mathbb{R}\\\\}$ forms a $\\\\pi$-system and $\\\\sigma(\\\\pi(\\\\mathbb{R})) = \\\\mathcal{B}(\\\\mathbb{R})$ by Example 1.6 above.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 1.11",
        "name": "\u03c3-algebra characterization",
        "topic": "Measurable sets and functions",
        "previous_results": [],
        "statement": "A collection of sets $\\\\mathcal{F}$ is a $\\\\sigma$-algebra if and only if $\\\\mathcal{F}$ is both a $\\\\pi$-system and a $\\\\lambda$-system.",
        "proof": "Clearly a $\\\\sigma$-algebra is both a $\\\\pi$-system and a $\\\\lambda$-system so it remains to establish the converse. Let $\\\\mathcal{F}$ be both a $\\\\pi$-system and a $\\\\lambda$-system. Let $A, B \\\\in \\\\mathcal{F}$. Then, since $\\\\Omega \\\\in \\\\mathcal{F}$, we also have $A^c = \\\\Omega \\\\setminus A \\\\in \\\\mathcal{F}$ and further\\n\\n$A \\\\cup B = \\\\Omega \\\\setminus (A^c \\\\cap B^c) \\\\in \\\\mathcal{F}$.\\n\\nFinally, let $\\\\{A_n\\\\}_{n\\\\geq1} \\\\subseteq \\\\mathcal{F}$ be a sequence of sets in $\\\\mathcal{F}$. Then\\n$\\\\bigcup_{n\\\\geq1} A_n = \\\\bigcup_{n\\\\geq1} \\\\bigcup_{k=1}^n A_k \\\\in \\\\mathcal{F}$\\n\\nby the properties of $\\\\lambda$-sets as the sequence $B_n = \\\\bigcup_{k=1}^n A_k$ is increasing."
    },
    {
        "type": "lemma",
        "id": "Lemma 1.12",
        "name": "\u03c0-\u03bb systems lemma",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Lemma 1.3",
            "Lemma 1.11"
        ],
        "statement": "Let $\\\\mathcal{M}$ be a $\\\\lambda$-system and $\\\\mathcal{A}$ be a $\\\\pi$-system. Then, $\\\\mathcal{A} \\\\subseteq \\\\mathcal{M} \\\\Rightarrow \\\\sigma(\\\\mathcal{A}) \\\\subseteq \\\\mathcal{M}$.",
        "proof": "Let $\\\\lambda(\\\\mathcal{A})$ denote the intersection of all $\\\\lambda$-systems containing $\\\\mathcal{A}$. Then, in analogy to Lemma 1.3, $\\\\lambda(\\\\mathcal{A})$ itself is a $\\\\lambda$-system, it is the smallest $\\\\lambda$-system containing $\\\\mathcal{A}$. In particular, $\\\\lambda(\\\\mathcal{A}) \\\\subseteq \\\\mathcal{M}$. Naturally, a $\\\\sigma$-algebra is by definition a $\\\\lambda$-system. If we show that $\\\\lambda(\\\\mathcal{A})$ is itself a $\\\\sigma$-algebra it will imply that $\\\\lambda(\\\\mathcal{A}) = \\\\sigma(\\\\mathcal{A})$ and the proof will be complete. By Lemma 1.11, it suffices to show that $\\\\lambda(\\\\mathcal{A})$ is a $\\\\pi$-system.\\n\\nLet $\\\\mathcal{C} = \\\\{A \\\\in \\\\lambda(\\\\mathcal{A}) : A \\\\cap C \\\\in \\\\lambda(\\\\mathcal{A}) \\\\forall C \\\\in \\\\mathcal{A}\\\\}$. We first show that $\\\\mathcal{C}$ is a $\\\\lambda$-system. Clearly, $\\\\Omega \\\\in \\\\mathcal{C}$. Let $A, B \\\\in \\\\mathcal{C}$ with $A \\\\subseteq B$. Then $(B \\\\setminus A) \\\\cap C = B \\\\cap C \\\\setminus A \\\\cap C \\\\in \\\\lambda(\\\\mathcal{A})$ for all $C \\\\in \\\\mathcal{A}$ so that $B \\\\setminus A \\\\in \\\\mathcal{C}$. Finally, if $A_n$ is an increasing sequence in $\\\\mathcal{C}$ and $A = \\\\bigcup_{n\\\\geq1} A_n$ then $A \\\\cap C = \\\\bigcup_{n\\\\geq1} A_n \\\\cap C \\\\in \\\\lambda(\\\\mathcal{A})$ for all $C \\\\in \\\\mathcal{A}$ and hence $A \\\\in \\\\mathcal{C}$. By definition, $\\\\mathcal{C} \\\\subseteq \\\\lambda(\\\\mathcal{A})$ and, since $\\\\mathcal{A}$ is a $\\\\pi$-system, also $\\\\mathcal{A} \\\\subseteq \\\\mathcal{C}$. It follows that $\\\\mathcal{C} = \\\\lambda(\\\\mathcal{A})$.\\n\\nNow let $\\\\mathcal{D} = \\\\{A \\\\in \\\\lambda(\\\\mathcal{A}) : A \\\\cap C \\\\in \\\\lambda(\\\\mathcal{A}) \\\\forall C \\\\in \\\\lambda(\\\\mathcal{A})\\\\}$. As above, we can easily show that $\\\\mathcal{D}$ inherits the $\\\\lambda$-system structure from $\\\\lambda(\\\\mathcal{A})$. Further, $\\\\mathcal{C} = \\\\lambda(\\\\mathcal{A})$ above implies that $\\\\mathcal{A} \\\\subseteq \\\\mathcal{D}$. Minimality of $\\\\lambda(\\\\mathcal{A})$ again implies that $\\\\mathcal{D} = \\\\lambda(\\\\mathcal{A})$ and hence $\\\\lambda(\\\\mathcal{A})$ is a $\\\\pi$-system."
    },
    {
        "type": "definition",
        "id": "Definition 1.14",
        "name": "Measurable function",
        "topic": "Measurable sets and functions",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F})$ and $(E, \\\\mathcal{E})$ be measurable spaces. A function $f : \\\\Omega \\\\to E$ is said to be measurable, or a random variable, if\\n\\n$f^{-1}(A) = \\\\{\\\\omega \\\\in \\\\Omega : f(\\\\omega) \\\\in A\\\\} \\\\in \\\\mathcal{F} \\\\quad \\\\forall A \\\\in \\\\mathcal{E}$.\\n\\nIf this is not clear from the context, we shall say more precisely that $f$ is an $E$-valued random variable and we may specify the $\\\\sigma$-algebras $\\\\mathcal{F}$, $\\\\mathcal{E}$ with respect to which the measurability is taken. The terms measurable function and random variable are used interchangeably. Similarly, we will use both $f$ and $X$ as our generic notation for a function (one being canonical in analysis and the other in probability) and switch between the two at will.",
        "proof": ""
    },
    {
        "type": "proposition",
        "id": "Proposition 1.15",
        "name": "Composition of Measurable Functions",
        "topic": "Measurable sets and functions, a.k.a. events and random variables",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F )$, $(E, E )$ and $(H, H )$ be three measurable spaces. Let $f : \\\\Omega \\\\to E$ and $g : E \\\\to H$ be two random variables. Then $g \\\\circ f$ is a random variable from $(\\\\Omega, F )$ to $(H, H )$.",
        "proof": "For $A \\\\in H$, $g^{-1}(A) \\\\in E$ by measurability of $g$ and $(g \\\\circ f)^{-1}(A) = f^{-1}(g^{-1}(A)) \\\\in F$ by measurability of $f$."
    },
    {
        "type": "example",
        "id": "Example 1.16",
        "name": "Characteristic Functions",
        "topic": "Measurable sets and functions, a.k.a. events and random variables",
        "previous_results": [],
        "statement": "Let $E = \\\\{0, 1\\\\}$ and $E = P(E)$. A subset $A \\\\subset \\\\Omega$ is an event if and only if its characteristic function $1_A$ (equal to 1 for $\\\\omega \\\\in A$ and 0 otherwise) is a random variable.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 1.17",
        "name": "Generated \u03c3-algebra",
        "topic": "Measurable sets and functions, a.k.a. events and random variables",
        "previous_results": [
            "Lemma 1.3",
            "Definition 1.4"
        ],
        "statement": "Let $\\\\Omega$ be a set and $(f_i)_{i\\\\in I}$ a collection of functions from $\\\\Omega$ to measurable spaces $(E_i, E_i)_{i\\\\in I}$. The $\\\\sigma$-algebra generated by functions $(f_i)_{i\\\\in I}$, denoted $\\\\sigma(f_i : i \\\\in I)$, is the smallest $\\\\sigma$-algebra on $\\\\Omega$ with respect to which all $f_i$, $i \\\\in I$, are measurable.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 1.18",
        "name": "Generated \u03c3-algebra Characterization",
        "topic": "Measurable sets and functions, a.k.a. events and random variables",
        "previous_results": [],
        "statement": "Let $X$ be a random variable from $(\\\\Omega, F)$ to $(E, E)$ and suppose $E = \\\\sigma(A)$. Then $\\\\sigma(X) = \\\\{X^{-1}(A) : A \\\\in E\\\\} = \\\\sigma(X^{-1}(A) : A \\\\in A)$.",
        "proof": "It is easy to verify that the inverse $A \\\\to X^{-1}(A)$ preserves all the set operations. In particular, $\\\\{X^{-1}(A) : A \\\\in E\\\\}$ is a $\\\\sigma$-algebra. By definition, it is contained in $\\\\sigma(X)$ and by the minimality of the latter, the two are equal. Denote $\\\\sigma(X; A) = \\\\sigma(X^{-1}(A) : A \\\\in A)$. The inclusion $\\\\sigma(X; A) \\\\subseteq \\\\sigma(X)$ is clear. For the reverse, let $G = \\\\{A \\\\subseteq E : X^{-1}(A) \\\\in \\\\sigma(X; A)\\\\}$. We verify easily that $G$ is a $\\\\sigma$-algebra and since $A \\\\subseteq G$ we conclude that $E \\\\subseteq G$. It follows that $\\\\sigma(X) \\\\subseteq \\\\sigma(X; A)$ and hence we have an equality."
    },
    {
        "type": "corollary",
        "id": "Corollary 1.19",
        "name": "Real-valued Measurability",
        "topic": "Measurable sets and functions, a.k.a. events and random variables",
        "previous_results": [
            "Lemma 1.18",
            "Example 1.6"
        ],
        "statement": "A function $f : \\\\Omega \\\\to \\\\mathbb{R}$ or $f : \\\\Omega \\\\to \\\\overline{\\\\mathbb{R}}$ is measurable with respect to $F$ (and $B(\\\\mathbb{R})$ or $B(\\\\overline{\\\\mathbb{R}})$) if and only if $\\\\{x : f(x) \\\\leq t\\\\} \\\\in F$ for every $t \\\\in \\\\mathbb{R}$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 1.20",
        "name": "Coordinate Mappings",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Definition 1.7"
        ],
        "statement": "Consider the product space notation from Definition 1.7. Let $X_i$ denote the coordinate mappings, i.e., $X_i : \\\\Omega \\\\to \\\\Omega_i$ is given by $X_i(\\\\omega) = \\\\omega_i$. Then the product $\\\\sigma$-algebra is generated by these coordinate mappings, $F = \\\\times_{i\\\\in I}F_i = \\\\sigma (X_i : i \\\\in I)$. In particular, all $X_i$ are measurable. On the other hand, if $(E, E)$ is a measurable space and $Y_i : (E, E) \\\\to (\\\\Omega_i, F_i)$ are measurable then the mapping $Y : E \\\\to \\\\Omega$ given by $Y = (Y_i : i \\\\in I)$ is measurable (with respect to $F$).",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 1.21",
        "name": "Identity Mapping",
        "topic": "Measurable sets and functions",
        "previous_results": [],
        "statement": "Let $G \\\\subseteq F$. Then the identity mapping of $(\\\\Omega, F)$ onto $(\\\\Omega, G)$ is a random variable.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 1.22",
        "name": "Coin Tossing",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Example 1.8"
        ],
        "statement": "Recall the model for repetitive coin tossing described in Example 1.8. It involved a careful choice of $\\\\Omega$ which, in an intuitive sense, was minimal for our purposes. If we wanted to expand our experiment and toss a coin and a dice simultaneously we would not be able to do so using $\\\\Omega$. For this reason, it is usually a much better practice to work with a fixed large $(\\\\Omega, F)$ and to encode our experiments using random variables on $\\\\Omega$. For example, we could take $([0, 1], B([0, 1]))$ and let $X_n(\\\\omega) = 1_{\\\\lfloor 2^n\\\\omega\\\\rfloor \\\\text{ is even}}$, $n \\\\geq 1$, where 0 is even. It is easy to check that $X_n$ is a random variable and $X_n \\\\in \\\\{0, 1\\\\}$. We shall see these are just as good a way to express the coin tossing experiment.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 1.23",
        "name": "Baire Sigma-Algebra",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Corollary 1.19"
        ],
        "statement": "Let $(E, d)$ be a metric space and let $B(E)$ be the Borel $\\\\sigma$-algebra generated by its open sets. Then the Borel $\\\\sigma$-algebra on $E$ is equal to the Baire $\\\\sigma$-algebra on $E$:\\n\\n$B(E) = \\\\sigma (f : E \\\\to R| f \\\\text{ continuous})$.\\n\\nAs in Corollary 1.19, for $f$ to be measurable it is enough to check that $f^{-1}(O) \\\\in B(E)$ for an open interval $O$ and this follows from continuity. In particular, the \\",
        "supseteq$\\": "nclusion follows. For a closed set $F \\\\subseteq E$, let $f_F(x) = d(x, F)$ be the distance of $x$ to $F$. Then $f$ is continuous and $F = f^{-1}_F(\\\\{0\\\\})$ is an element of the right hand side. This gives the reverse inclusion \\",
        "subseteq$\\": "nd hence the equality.",
        "proof": ""
    },
    {
        "type": "proposition",
        "id": "Proposition 1.24",
        "name": "Measurable Functions",
        "topic": "Measurable sets and functions",
        "previous_results": [],
        "statement": "Let $(f_n)$ be a sequence of measurable functions on $(\\\\Omega, F)$ taking values in $R$, and let $h : R \\\\to R$ be Borel measurable. Then, whenever they make sense, the following are also measurable functions on $(\\\\Omega, F)$:\\n\\n$f_1 + f_2$,\\n\\n$f_1 f_2$, $\\\\max\\\\{f_1, f_2\\\\}$, $\\\\min\\\\{f_1, f_2\\\\}$,\\n\\n$f_1/f_2$,\\n\\n$h \\\\circ f$,\\n\\n$f_n$,\\n\\n$\\\\sup_n f_n$,\\n\\n$\\\\inf_n f_n$,\\n\\n$\\\\limsup_{n\\\\to\\\\infty} f_n$,\\n\\n$\\\\liminf_{n\\\\to\\\\infty} f_n$.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 1.25",
        "name": "Simple Function",
        "topic": "Measurable sets and functions",
        "previous_results": [],
        "statement": "A measurable function $f$ on $(\\\\Omega, F)$ is called a simple function if\\n\\n$f = \\\\sum_{k=1}^{n} a_k 1_{E_k}$\\n\\nfor some $n \\\\geq 1$ and where each $E_k \\\\in F$ and each $a_k \\\\in \\\\mathbb{R}$. The canonical form of $f$ is the unique decomposition as in (6) where the numbers $a_k$ are distinct and non-zero and the sets $E_k$ are disjoint and non-empty.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 1.26",
        "name": "Approximation by Simple Functions",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Proposition 1.24"
        ],
        "statement": "Let $(\\\\Omega, F)$ be a measurable space. A function $X : \\\\Omega \\\\to \\\\mathbb{R}$ is measurable if and only if it is a limit of simple functions. Further, if $f$ is bounded from below (resp. bounded), the limit can be taken to be increasing (resp. uniform).",
        "proof": "That a limit of simple functions is a measurable function follows from Proposition 1.24. Now let $X$ be a random variable and define\\n\\n$X_n = -2^n 1_{X\\\\leq -4^{n+1}} + \\\\sum_{k\\\\in\\\\mathbb{Z}\\\\cap[-4^{n+1},4^n-1]} \\\\frac{k}{2^n} 1_{\\\\frac{k}{2^n} < X \\\\leq \\\\frac{k+1}{2^n}} + 2^n 1_{2^n < X}$, $n \\\\geq 1$.\\n\\nLet $\\\\Omega^+_n := \\\\{\\\\omega \\\\in \\\\Omega : X(\\\\omega) \\\\leq 2^n\\\\}$, $\\\\Omega^-_n := \\\\{\\\\omega \\\\in \\\\Omega : X(\\\\omega) > -2^n\\\\}$ and $\\\\Omega_n = \\\\Omega^-_n \\\\cap \\\\Omega^+_n$. The result follows by noting that $\\\\sup_{\\\\omega\\\\in\\\\Omega_n} |X_n(\\\\omega) - X(\\\\omega)| \\\\leq 2^{-n}$ and $X_n \\\\leq X_{n+1}$ on $\\\\Omega^-_n$."
    },
    {
        "type": "theorem",
        "id": "Theorem 1.27",
        "name": "Measurability Characterization",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Lemma 1.18",
            "Lemma 1.26"
        ],
        "statement": "Let $X$ be a random variable on $(\\\\Omega, F)$ with values in a measurable space $(E, E)$ and let $g$ be a real-valued random variable on $(\\\\Omega, F)$. Then $g$ is $\\\\sigma(X)$-measurable if and only if $g = h \\\\circ X$ for some real-valued random variable on $(E, E)$.",
        "proof": "One direction is clear: $g = h \\\\circ X$ is a real-valued random variable. For the other direction, start with $g$ and suppose it takes at most countably many distinct values $(a_n)_{n\\\\geq 1}$. The sets $A_n = g^{-1}(\\\\{a_n\\\\})$ are pairwise disjoint and each is an element of $\\\\sigma(X)$ and hence, by Lemma 1.18, $A_n = X^{-1}(B_n)$ for some $B_n \\\\in E$. Note that we might have $B_n \\\\cap B_m \\\\neq \\\\emptyset$ but the points in the intersection are not in the range of values of $X$. Consequently, if we set $C_n := B_n \\\\setminus (\\\\cup_{k=1}^{n-1} B_k)$ then $C_n \\\\in E$ are pairwise disjoint and $X^{-1}(C_n) = A_n \\\\setminus (\\\\cup_{k=1}^{n-1} A_k) = A_n$. If we put $h = \\\\sum_{n\\\\geq 1} a_n 1_{C_n}$ then $g = h \\\\circ X$ as required.\\n\\nFor a general $g$, let $g_n \\\\uparrow g$ be the sequence of simple random variables converging to $g$ given by Lemma 1.26. By the above, we can write each $g_n = h_n \\\\circ X$. Let $H = \\\\{e \\\\in E : h_n(e) \\\\text{ converges}\\\\}$. Recall that both $\\\\limsup h_n$ and $\\\\liminf h_n$ are measurable and so $H = \\\\{\\\\limsup h_n = \\\\liminf h_n\\\\}$ is measurable. Further, $X(\\\\Omega) \\\\subseteq H$ since $g_n \\\\uparrow g$. It follows that $h(\\\\omega) := (\\\\lim_{n\\\\to\\\\infty} h_n(\\\\omega))1_H(\\\\omega)$ is measurable and satisfies $g = h \\\\circ X$."
    },
    {
        "type": "theorem",
        "id": "Theorem 1.28",
        "name": "Monotone Class Theorem",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Lemma 1.12",
            "Lemma 1.26"
        ],
        "statement": "Let $H$ be a class of bounded functions from $\\\\Omega$ to $\\\\mathbb{R}$ satisfying the following conditions:\\n\\n(i) $H$ is a vector space over $\\\\mathbb{R}$,\\n\\n(ii) the constant function $1$ is in $H$,\\n\\n(iii) if $(f_n)_{n\\\\geq 1} \\\\subseteq H$ such that $f_n \\\\nearrow f$ for a bounded function $f$, then $f \\\\in H$.\\n\\nIf $C \\\\subseteq H$ is stable under pointwise multiplication then $H$ contains all bounded $\\\\sigma(C)$-measurable functions.",
        "proof": "We first make the following simple observation: $H$ is closed under uniform limits.\\n\\nTo prove this, let $f_n$ be a sequence of functions in $H$ converging uniformly to some $f$. Passing to a subsequence, we can assume that $\\\\|f_n - f\\\\|_{\\\\sup} \\\\leq 2^{-n}$, where $\\\\|f\\\\|_{\\\\sup} = \\\\sup_{\\\\omega\\\\in\\\\Omega} |f(\\\\omega)|$. Now we can modify the sequence so that it is increasing. Set $g_n = f_n - 2^{1-n}$. Then $g_n - g_{n-1} = f_n - f_{n-1} + 2^{1-n} \\\\geq 2^{-n} \\\\geq 0$. Also, $\\\\|g_n\\\\|_{\\\\sup} = \\\\|f_1 + \\\\sum_{k=2}^{n} (f_k - f_{k-1}) - 2^{1-n}\\\\|_{\\\\sup} \\\\leq \\\\|f_1\\\\|_{\\\\sup} + 3$. The sequence is uniformly bounded so that its limit is also bounded and hence $H \\\\ni \\\\lim g_n = \\\\lim f_n = f$.\\n\\nFor the special case when $C = \\\\{1_A : A \\\\in A\\\\}$ for a $\\\\pi$-system $A$, Theorem 1.28 is a functional equivalent of Lemma 1.12. To see this, simply check that the properties of $H$ mean that the family of sets $E \\\\subseteq \\\\Omega$ for which $1_E \\\\in H$ forms a $\\\\lambda$-system. Lemma 1.12 now shows that $1_E \\\\in H$ for all $E \\\\in \\\\sigma(A)$ and Lemma 1.26 tells us that any bounded measurable function is a uniform limit of simple functions and hence, by the above lemma, is also in $H$, as required.\\n\\nFor the general statement, we reduce it to the special case treated above. Note that without any loss of generality we can assume that $1 \\\\in C$. Let $A_0$ be the algebra of functions generated by $C$. Given that $C$ is already closed under multiplication, $A_0$ is simply the linear span of $C$. Let $A$ be the closure of $A_0$ under uniform convergence. By the above lemma, $A \\\\subset H$ and we check that $A$ is still an algebra of functions. Take $f \\\\in A$ and since it is a bounded function we can take a closed interval $R \\\\subseteq \\\\mathbb{R}$ with $f(\\\\omega) \\\\in R$, $\\\\omega \\\\in \\\\Omega$. On $R$, by the Weierstrass approximation theorem, we can approximate the function $x \\\\mapsto |x|$ uniformly using a sequence of polynomials $p_n$. Note that $p_n \\\\circ f \\\\in A$ and hence also its uniform limit $|f|$. It then follows that $A$ is closed under $\\\\wedge$ and $\\\\vee$ (observe that $f^+ = (|f| + f)/2$ and $f \\\\vee g = f + (g - f)^+$ etc.). Now, for any $f \\\\in A$ and any $a \\\\in \\\\mathbb{R}$ we have $A \\\\ni n(f - a)^+ \\\\wedge 1 \\\\uparrow 1_{f^{-1}((a,\\\\infty))}$ and hence the limit is in $H$, i.e., $\\\\{1_D : D \\\\in D\\\\} \\\\subseteq H$, where $D = \\\\{f^{-1}((a, \\\\infty)) : f \\\\in A, a \\\\in \\\\mathbb{R}\\\\}$. Note that $\\\\{f > a\\\\} \\\\cap \\\\{g > b\\\\} = \\\\{(f - a)^+(g - b)^+ > 0\\\\}$ so that $D$ is a $\\\\pi$-system and by Lemma 1.18, $\\\\sigma(D) = \\\\sigma(f : f \\\\in A)$. This reduces the general result to the special case previously considered."
    },
    {
        "type": "lemma",
        "id": "Lemma 1.29",
        "name": "Product Space Measurability",
        "topic": "Measurable sets and functions",
        "previous_results": [
            "Lemma 1.18",
            "Proposition 1.15",
            "Lemma 1.26",
            "Theorem 1.28"
        ],
        "statement": "Let $(\\\\Omega, F)$ be the product space of two measurable spaces $(\\\\Omega_i, F_i)$, $i = 1, 2$. If $f : \\\\Omega \\\\to \\\\mathbb{R}$ is measurable then\\n\\n$\\\\bullet$ for each $\\\\omega_1 \\\\in \\\\Omega_1$, $\\\\Omega_2 \\\\ni \\\\omega_2 \\\\to f(\\\\omega_1, \\\\omega_2)$ is $F_2$-measurable and\\n\\n$\\\\bullet$ for each $\\\\omega_2 \\\\in \\\\Omega_2$, $\\\\Omega_1 \\\\ni \\\\omega_1 \\\\to f(\\\\omega_1, \\\\omega_2)$ is $F_1$-measurable.",
        "proof": "The first proof: using the Monotone Class Theorem. Let $H$ be the class of bounded functions $h : \\\\Omega \\\\to \\\\mathbb{R}$ which satisfy the assertion of the lemma. Clearly $H$ satisfies the assumptions of the Monotone Class Theorem (Theorem 1.28) and contains the functions $h = 1_{A_1 \\\\times A_2}$ for $A_i \\\\in F_i$, $i = 1, 2$. These rectangles generate $F$ and we conclude that $H$ contains all bounded measurable functions. For an unbounded $f$, we use the result for $f_n = (f \\\\vee -n) \\\\wedge n$, which is bounded, and use that limits of measurable functions are measurable.\\n\\nThe second proof: using $\\\\pi$-$\\\\lambda$ systems lemma. An application of $\\\\pi$-$\\\\lambda$ systems lemma shows that the statement holds for $f = 1_D$ for $D \\\\in F$, see Exercise 1.13. It thus also holds for simple functions. It remains to apply Lemma 1.26 and note that limits of measurable functions are measurable.\\n\\nThe third (and the simplest) proof. Fix $\\\\omega_1 \\\\in \\\\Omega_2$. Let $\\\\iota : \\\\Omega_2 \\\\to \\\\Omega$ be given by $\\\\iota(\\\\omega_2) = (\\\\omega_1, \\\\omega_2)$. For $A_i \\\\in F_i$, $i = 1, 2$, we have $\\\\iota^{-1}(A_1 \\\\times A_2) = A_2$ if $\\\\omega_1 \\\\in A_1$ and $\\\\emptyset$ otherwise. It follows by Lemma 1.18 that $\\\\iota$ is measurable. The map $\\\\Omega_2 \\\\ni \\\\omega_2 \\\\to f(\\\\omega_1, \\\\omega_2)$ is a composition of measurable functions, namely $f \\\\circ \\\\iota$, and is hence $F_2$-measurable by Proposition 1.15."
    },
    {
        "type": "definition",
        "id": "Definition 2.1",
        "name": "Set functions",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $A$ be a collection of subsets of $\\\\Omega$ containing the empty set $\\\\emptyset$. A set function on $A$ is a function $\\\\mu : A \\\\to [0, \\\\infty]$ with $\\\\mu(\\\\emptyset) = 0$. We say that $\\\\mu$ is countably additive, or $\\\\sigma$-additive, if for all sequences $(A_n)$ of disjoint sets in $A$ with $\\\\bigcup_{n=1}^{\\\\infty} A_n \\\\in A$\\n$$\\\\mu\\\\left(\\\\bigcup_{n=1}^{\\\\infty} A_n\\\\right) = \\\\sum_{n=1}^{\\\\infty} \\\\mu(A_n).$$",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 2.2",
        "name": "Measure space",
        "topic": "Measures",
        "previous_results": [],
        "statement": "A measure space is a triple $(\\\\Omega, F, \\\\mu)$ where $\\\\Omega$ is a set, $F$ is a $\\\\sigma$-algebra on $\\\\Omega$ and $\\\\mu : F \\\\to [0, \\\\infty]$ is a countably additive set function. Then $\\\\mu$ is a measure on $(\\\\Omega, F)$.",
        "proof": ""
    },
    {
        "type": "proposition",
        "id": "Proposition 2.3",
        "name": "Measure Properties",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, \\\\mu)$ be a measure space and $A, B, A_n, B_n \\\\in F$, $n \\\\geq 1$. Then\\n\\n(i) $A \\\\cap B = \\\\emptyset \\\\Rightarrow \\\\mu(A \\\\cup B) = \\\\mu(A) + \\\\mu(B)$ (additive)\\n\\n(ii) $A \\\\subseteq B \\\\Rightarrow \\\\mu(A) \\\\leq \\\\mu(B)$ (increasing)\\n\\n(iii) $\\\\mu(A \\\\cup B) + \\\\mu(A \\\\cap B) = \\\\mu(A) + \\\\mu(B)$\\n\\n(iv) $A_n \\\\uparrow A$, then $\\\\mu(A_n) \\\\uparrow \\\\mu(A)$ as $n \\\\to \\\\infty$ (continuous from below)\\n\\n(v) $B_n \\\\downarrow B$, $\\\\mu(B_k) < \\\\infty$ for some $k \\\\in \\\\mathbb{N}$, then $\\\\mu(B_n) \\\\downarrow \\\\mu(B)$ as $n \\\\to \\\\infty$ (continuous from above)\\n\\n(vi) $\\\\mu\\\\left(\\\\bigcup_{n\\\\geq1} A_n\\\\right) \\\\leq \\\\sum_{n\\\\geq1} \\\\mu(A_n)$ ($\\\\sigma$-subadditive)",
        "proof": "The proof is mostly a direct consequence of the defining properties of a measure and is left as an exercise. We just show (iv). Define sets $D_1 := A_1$ and $D_n := A_n \\\\setminus A_{n-1}$ for $n \\\\geq 1$ and note these are pairwise disjoint since $A_{n-1} \\\\subseteq A_n$. Further, $A_n = \\\\bigcup_{k\\\\leq n} D_k$. It follows that\\n\\n$$\\\\mu(A) = \\\\mu\\\\left(\\\\bigcup_{n\\\\geq1} A_n\\\\right) = \\\\mu\\\\left(\\\\bigcup_{n\\\\geq1} D_n\\\\right) = \\\\sum_{n\\\\geq1} \\\\mu(D_n) = \\\\lim_{n\\\\to\\\\infty} \\\\sum_{k=1}^n \\\\mu(D_k) = \\\\lim_{n\\\\to\\\\infty} \\\\mu(A_n),$$\\n\\nwhere the third equality is by countable additivity of $\\\\mu$ and the last equality is by finite additivity of $\\\\mu$."
    },
    {
        "type": "lemma",
        "id": "Lemma 2.4",
        "name": "Countable Additivity Characterization",
        "topic": "Measures",
        "previous_results": [
            "Proposition 2.3"
        ],
        "statement": "Let $\\\\mu : A \\\\to [0, \\\\infty)$ be an additive set function on an algebra $A$ taking only finite values. Then $\\\\mu$ is countably additive iff for every sequence $(A_n)$ of sets in $A$ with $A_n \\\\downarrow \\\\emptyset$ we have $\\\\mu(A_n) \\\\to 0$.",
        "proof": "One implication follows (essentially) from Proposition 2.3; the other is an exercise."
    },
    {
        "type": "definition",
        "id": "Definition 2.5",
        "name": "Types of measure space",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, \\\\mu)$ be a measure space.\\n\\n1. We say that $\\\\mu$ is finite if $\\\\mu(\\\\Omega) < \\\\infty$.\\n\\n2. If there is a sequence $(K_n)_{n\\\\geq1}$ of sets from $F$ with $\\\\mu(K_n) < \\\\infty$ for all $n$ and $\\\\bigcup_{n=1}^{\\\\infty} K_n = \\\\Omega$, then $\\\\mu$ is said to be $\\\\sigma$-finite.\\n\\n3. In the special case when $\\\\mu(\\\\Omega) = 1$, we say that $\\\\mu$ is a probability measure and $(\\\\Omega, F, \\\\mu)$ is a probability space; we often use the notation $(\\\\Omega, F, P)$ to emphasize this.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 2.6",
        "name": "Null sets",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, \\\\mu)$ be a measure space. We say that a set $A$ is null if $\\\\mu(A) = 0$. We say that a property holds almost everywhere (a.e.), or for almost every $\\\\omega \\\\in \\\\Omega$, if it holds outside of a null set.\\n\\nIf $P$ is a probability measure we typically say that a property holds almost surely (a.s.) instead of almost everywhere. For instance, we will say that two events are a.s. equal, $A = B$ a.s., if $P(A\\\\triangle B) = 0$. Similarly, for two random variables $X,Y$ we say that $X = Y$ a.s., if $P(X \\\\neq Y) = 0$. If the reference measure is not obvious we shall indicate it explicitly, e.g., by saying $\\\\mu$-null or $P$-a.s.\\n\\nThe structure of its null sets tells us a lot about a given measure. Intuitively speaking, if two measures have the same null sets, then one is a re-weighted version of the other. If their null sets differ then one can not go from one measure to another \u2013 no re-weighting will resurrect zero into a positive number. This intuition will be made precise in Theorem 4.9 but we can already define the relevant concept.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 2.7",
        "name": "Absolute continuity",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $\\\\mu, \\\\nu$ be two measures on a measurable space $(\\\\Omega, F)$. We say that $\\\\nu$ is absolutely continuous with respect to $\\\\mu$, and write $\\\\nu \\\\ll \\\\mu$, if $\\\\mu(A) = 0$ for some $A \\\\in F$ implies $\\\\nu(A) = 0$.\\nWe say that $\\\\mu$ and $\\\\nu$ are equivalent, and write $\\\\mu \\\\sim \\\\nu$, if $\\\\nu \\\\ll \\\\mu$ and $\\\\mu \\\\ll \\\\nu$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 2.8",
        "name": "Basic measures",
        "topic": "Measures",
        "previous_results": [],
        "statement": "(i) Let $(\\\\Omega, F)$ be a measurable space. The zero function, $\\\\mu(A) = 0$ for all $A \\\\in F$, defines a measure. Likewise, $\\\\nu$ given by $\\\\nu(\\\\emptyset) = 0$, $\\\\nu(A) = +\\\\infty$ for all $\\\\emptyset \\\\neq A \\\\in F$ also defines a measure. Clearly both are trivial examples and are well defined for any $\\\\sigma$-algebra $F$.\\n\\n(ii) Let $(\\\\Omega, F)$ be a measurable space and fix $\\\\omega \\\\in \\\\Omega$. Then $\\\\delta_\\\\omega$ defined via $\\\\delta_\\\\omega(A) = 1_{\\\\omega\\\\in A}$ defines a measure. It is called the Dirac measure in $\\\\omega$ or the point mass in $\\\\omega$.\\n\\n(iii) On $\\\\mathbb{R}$ consider the $\\\\sigma$-algebra $A$ of sets which are either countable or have a countable complement.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 1.2 (iv)",
        "name": "Counting Measure",
        "topic": "Measurable sets and functions",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F)$ be a measurable space. For $A \\\\in F$, set $\\\\mu(A) = |A|$, the number of elements in $A$, if $A$ is finite and $\\\\mu(A) = +\\\\infty$ if $A$ is infinite. Then $\\\\mu$ is the counting measure on $\\\\Omega$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 2.9",
        "name": "Discrete Measure Theory",
        "topic": "Measures",
        "previous_results": [
            "Example 2.8 (ii)"
        ],
        "statement": "Let $\\\\Omega$ be a countable set. A mass function on $\\\\Omega$ is any function $p : \\\\Omega \\\\to [0, \\\\infty]$. Given such a $p$ we can define a measure on $(\\\\Omega, P(\\\\Omega))$ by setting $\\\\mu(A) = \\\\sum_{x\\\\in A} p(x)$. In the notation of Example 2.8 (ii), $\\\\mu = \\\\sum_{x\\\\in\\\\Omega} p(x)\\\\delta_x$.\\n\\nConversely, given a measure $\\\\mu$ on $(\\\\Omega, P(\\\\Omega))$ we can define the corresponding mass function by $p(x) = \\\\mu(\\\\{x\\\\})$. Consequently, for a countable $\\\\Omega$, there is a one-to-one correspondence between measures on $(\\\\Omega, P(\\\\Omega))$ and mass functions on $\\\\Omega$.\\n\\nNote also, that if $\\\\mu$, $\\\\nu$ are two measures with their respective mass functions $p$, $r$ then $\\\\nu \\\\ll \\\\mu$ if and only if $p(x) = 0$ implies $r(x) = 0$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 2.10",
        "name": "Uniqueness of Extension",
        "topic": "Measures",
        "previous_results": [
            "Lemma 1.12"
        ],
        "statement": "Let $\\\\mu_1$ and $\\\\mu_2$ be measures on a measurable space $(\\\\Omega, F)$ and let $A \\\\subseteq F$ be a $\\\\pi$-system with $\\\\sigma (A) = F$. If $\\\\mu_1(\\\\Omega) = \\\\mu_2(\\\\Omega) < \\\\infty$ and $\\\\mu_1 = \\\\mu_2$ on $A$, then $\\\\mu_1 = \\\\mu_2$.",
        "proof": "In view of Lemma 1.12 it suffices to verify that $\\\\{A \\\\in F : \\\\mu_1(A) = \\\\mu_2(A)\\\\}$ is a $\\\\lambda$-system, which is left as an exercise."
    },
    {
        "type": "theorem",
        "id": "Theorem 2.11",
        "name": "Carath\u00e9odory Extension Theorem",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $\\\\Omega$ be a set and $A$ an algebra on $\\\\Omega$, and let $F = \\\\sigma (A)$. Let $\\\\mu_0 : A \\\\to [0, \\\\infty]$ be a countably additive set function. Then there exists a measure $\\\\mu$ on $(\\\\Omega, F)$ such that $\\\\mu = \\\\mu_0$ on $A$.",
        "proof": "The idea of proof is rather simple, even if the details are tedious. First one defines the outer measure $\\\\mu^*(B)$ of any $B \\\\subseteq \\\\Omega$ by\\n\\n$\\\\mu^*(B) = \\\\inf\\\\left\\\\{\\\\sum_{j=1}^{\\\\infty} \\\\mu_0(A_j) : A_j \\\\in A, \\\\bigcup_{j=1}^{\\\\infty} A_j \\\\supseteq B\\\\right\\\\}$.\\n\\nThen define a set $B$ to be measurable if for all sets $E$,\\n\\n$\\\\mu^*(E) = \\\\mu^*(E \\\\cap B) + \\\\mu^*(E \\\\cap B^c)$.\\n\\n[Alternatively, if $\\\\mu_0(\\\\Omega)$ is finite, then one can define $B$ to be measurable if $\\\\mu^*(B) + \\\\mu^*(B^c) = \\\\mu_0(\\\\Omega)$; this more intuitive definition expresses that it is possible to cover $B$ and $B^c$ 'efficiently' with sets from $A$.] One must check that $\\\\mu^*$ defines a countably additive set function on the collection of measurable sets extending $\\\\mu_0$, and that the measurable sets form a $\\\\sigma$-algebra that contains $A$."
    },
    {
        "type": "lemma",
        "id": "Lemma 2.13",
        "name": "Measure Restriction",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, \\\\mu)$ be a measure space and $G \\\\subseteq F$ a $\\\\sigma$-algebra. Then $(\\\\Omega, G, \\\\mu|_G)$, where $\\\\mu|_G$ is the restriction of $\\\\mu$ to $G$, is a measure space.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 2.14",
        "name": "Sum of Measures",
        "topic": "Measures",
        "previous_results": [
            "Theorem 4.24"
        ],
        "statement": "Let $(\\\\Omega, F)$ be a measurable space and $(\\\\mu_n)_{n\\\\geq 1}$ a sequence of probability measures on $F$. Fix a sequence of positive numbers $(a_n)_{n\\\\geq 0}$ with $\\\\sum_{n\\\\geq 1} a_n = 1$. Then $\\\\mu$, defined by $\\\\mu(A) = \\\\sum_{n\\\\geq 1} a_n\\\\mu_n(A)$ is also a probability measure on $F$.",
        "proof": "The above lemma follows once we know we can exchange the order of summation in a double (countable) sum of positive numbers. This will in particular follow from (generalised) Fubini's theorem (Theorem 4.24) which we will see later in these lectures."
    },
    {
        "type": "definition",
        "id": "Definition 2.15",
        "name": "Distribution Function",
        "topic": "Measures on (R, B(R))",
        "previous_results": [
            "Proposition 2.3"
        ],
        "statement": "Let $\\\\mu$ be a probability measure on $B(R)$. The distribution function of $\\\\mu$ is the function $F_\\\\mu : R \\\\to R$ defined by $F_\\\\mu (x) = \\\\mu((\u2212\\\\infty, x])$.\\n\\nThe function $F_\\\\mu$ has the following properties:\\n\\n(i) $F_\\\\mu$ is increasing, i.e., $x < y$ implies $F_\\\\mu (x) \\\\leq F_\\\\mu (y)$,\\n\\n(ii) $F_\\\\mu (x) \\\\to 0$ as $x \\\\to \u2212\\\\infty$ and $F_\\\\mu (x) \\\\to 1$ as $x \\\\to \\\\infty$, and\\n\\n(iii) $F_\\\\mu$ is right continuous: $y \\\\downarrow x$ implies $F_\\\\mu (y) \\\\to F_\\\\mu (x)$.",
        "proof": "To see the last, suppose that $y_n \\\\downarrow x$ and let $A_n = (\u2212\\\\infty, y_n]$. Then $A_n \\\\downarrow A = (\u2212\\\\infty, x]$. Thus, by Proposition 2.3, $F_\\\\mu (y_n) = \\\\mu(A_n) \\\\downarrow \\\\mu(A) = F_\\\\mu (x)$. We often write $F_\\\\mu (\u2212\\\\infty) = 0$ and $F_\\\\mu (\\\\infty) = 1$ as shorthand for the second property.\\n\\nAny function $F : R \\\\to R$ which satisfies the same three properties as $F_\\\\mu$ above will be called a distribution function on $R$. Using the Carath\\\\'eodory Extension Theorem, we can construct all Borel probability measures on $R$ (i.e., probability measures on $(R, B(R))$): there is one for each distribution function. Since finite measures can all be obtained from probability measures (by multiplying by a constant), this characterizes all finite measures on $B(R)$."
    },
    {
        "type": "theorem",
        "id": "Theorem 2.16",
        "name": "Lebesgue",
        "topic": "Measures on (R, B(R))",
        "previous_results": [
            "Theorem 2.10",
            "Theorem 2.11",
            "Lemma 2.4",
            "Remark 2.12"
        ],
        "statement": "Let $F : R \\\\to R$ be a distribution function, i.e., $F$ is an increasing, right continuous function with $F(\u2212\\\\infty) = 0$ and $F(\\\\infty) = 1$. Then there is a unique Borel probability measure $\\\\mu = \\\\mu_F$ on $R$ such that $\\\\mu((\u2212\\\\infty, x]) = F(x)$ for every $x$. Every Borel probability measure $\\\\mu$ on $R$ arises in this way.",
        "proof": "Suppose for the moment that the existence statement holds. Since $\\\\pi(R) = \\\\{(\u2212\\\\infty, x] : x \\\\in R\\\\}$ is a $\\\\pi$-system which generates the $\\\\sigma$-algebra $B(R)$, uniqueness follows by Theorem 2.10. Also, to see the final part, let $\\\\mu$ be any Borel probability measure on $R$, and let $F$ be its distribution function. Then $F$ has the properties required for the first part of the theorem, and we obtain a measure $\\\\mu_F$ which by uniqueness is the measure $\\\\mu$ we started with.\\n\\nFor existence we shall apply Theorem 2.11, so first we need a suitable algebra. For $\u2212\\\\infty \\\\leq a \\\\leq b < \\\\infty$, let $I_{a,b} = (a, b]$, and set $I_{a,\\\\infty} = (a, \\\\infty)$. Let $I = \\\\{I_{a,b} : \u2212\\\\infty \\\\leq a \\\\leq b \\\\leq \\\\infty\\\\}$ be the collection of intervals that are open on the left and closed on the right. Let $A$ be the set of finite disjoint unions of elements of $I$; then $A$ is an algebra, and $\\\\sigma(A) = \\\\sigma(I) = B(R)$.\\n\\nWe can define a set function $\\\\mu_0$ on $A$ by setting\\n\\n$\\\\mu_0(I_{a,b}) = F(b) \u2212 F(a)$\\n\\nfor intervals and then extending it to $A$ by defining it as the sum for disjoint unions from $I$. It is an easy exercise to show that $\\\\mu_0$ is well defined and finitely additive. Carath\\\\'eodory's Extension Theorem tells us that $\\\\mu_0$ extends to a probability measure on $B(R)$ provided that $\\\\mu_0$ is countably additive on $A$. Proving this is slightly tricky. Note that we will have to use right continuity at some point.\\n\\nFirst note that by Lemma 2.4, since $\\\\mu_0$ is finite and additive on $A$, it is countably additive if and only if, for any sequence $(A_n)$ of sets from $A$ with $A_n \\\\downarrow \\\\emptyset$, $\\\\mu_0(A_n) \\\\downarrow 0$.\\n\\nSuppose that $F$ has the stated properties but, for a contradiction, that there exist $A_1, A_2, \\\\ldots \\\\in A$ with $A_n \\\\downarrow \\\\emptyset$ but $\\\\mu_0(A_n) \\\\not\\\\to 0$. Since $\\\\mu_0(A_n)$ is a decreasing sequence, there is some $\\\\delta > 0$ (namely, $\\\\lim \\\\mu_0(A_n)$) such that $\\\\mu_0(A_n) \\\\geq \\\\delta$ for all $n$. We look for a descending sequence of compact sets; since if all the sets in such a sequence are non-empty, so is their intersection.\\n\\nStep 1: Replace $A_n$ by $B_n = A_n \\\\cap (\u2212l, l]$. Since\\n\\n$\\\\mu_0(A_n \\\\setminus B_n) \\\\leq \\\\mu_0\\\\left((\u2212\\\\infty, l] \\\\cup (l, \\\\infty)\\\\right) = F(\u2212l) + 1 \u2212 F(l)$,\\n\\nif we take $l$ large enough then we have $\\\\mu_0(B_n) \\\\geq \\\\delta/2$ for all $n$.\\n\\nStep 2: Suppose that $B_n = \\\\bigcup_{i=1}^{k_n} I_{a_{n,i},b_{n,i}}$. Let $C_n = \\\\bigcup_{i=1}^{k_n} I_{\\\\tilde{a}_{n,i},b_{n,i}}$ where $a_{n,i} < \\\\tilde{a}_{n,i} < b_{n,i}$ and we use right continuity of $F$ to do this in such a way that\\n\\n$\\\\mu_0(B_n\\\\setminus C_n) < \\\\frac{\\\\delta}{2^{n+2}}$ for each $n$.\\n\\nLet $\\\\overline{C}_n$ be the closure of $C_n$ (obtained by adding the points $\\\\tilde{a}_{n,i}$ to $C_n$).\\n\\nStep 3: The sequence $(\\\\overline{C}_n)$ need not be decreasing, so set $D_n = \\\\bigcap_{i=1}^n C_i$, and $E_n = \\\\bigcap_{i=1}^n \\\\overline{C}_i$. Since\\n\\n$\\\\mu_0(D_n) \\\\geq \\\\mu_0(B_n) - \\\\sum_{i=1}^n \\\\mu_0(B_i\\\\setminus C_i) \\\\geq \\\\frac{\\\\delta}{2} - \\\\sum_{i=1}^n \\\\frac{\\\\delta}{2^{i+2}} \\\\geq \\\\frac{\\\\delta}{4}$,\\n\\n$D_n$ is non-empty. Thus $E_n \\\\supseteq D_n$ is non-empty.\\n\\nEach $E_n$ is closed and bounded, and so compact. Also, each $E_n$ is non-empty, and $E_n \\\\supseteq E_{n+1}$. Hence, by a basic result from topology, there is some $x$ such that $x \\\\in E_n$ for all $n$. Since $E_n \\\\subseteq \\\\overline{C}_n \\\\subseteq B_n \\\\subseteq A_n$, we have $x \\\\in A_n$ for all $n$, contradicting $A_n \\\\downarrow \\\\emptyset$."
    },
    {
        "type": "corollary",
        "id": "Corollary 2.17",
        "name": "Lebesgue Measure",
        "topic": "Measures on (R, B(R))",
        "previous_results": [
            "Theorem 2.16",
            "Remark 2.12"
        ],
        "statement": "There exists a unique Borel measure Leb on $R$ such that for all $a, b \\\\in R$ with $a < b$, $\\\\text{Leb} ((a, b]) = b \u2212 a$. The measure Leb is the Lebesgue measure on $B(R)$.",
        "proof": "The statement with $R$ replaced by $(0, 1]$ follows from Theorem 2.16 with $F(x) = 0$ on $(\u2212\\\\infty, 0]$, $F(x) = x$ on $[0, 1]$ and $F(x) = 1$ on $[1, \\\\infty)$. This gives us the Lebesgue measure $\\\\text{Leb}_k$ on any $(k, k + 1]$. We set $\\\\text{Leb}(A) = \\\\sum_{k\\\\in Z} \\\\text{Leb}_k(A \\\\cap (k, k + 1])$ and easily check it defines a measure on $B(R)$ with the right properties. Uniqueness follows from Remark 2.12."
    },
    {
        "type": "definition",
        "id": "Definition 2.18",
        "name": "Pushforward Measure",
        "topic": "Pushforward (image) measure",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and let $X$ be a random variable from $(\\\\Omega, F)$ to $(E, E)$. Then\\n\\n$Q(A) = P(X^{\u22121}(A)), A \\\\in E$,\\n\\ndefines a measure on $(E, E)$, the image measure of $P$ via $X$, or the pushforward measure. We write $Q = P \\\\circ X^{\u22121}$ and also call it the law or the distribution of $X$.",
        "proof": "Put differently, to measure a set in $E$, we transport it back into $\\\\Omega$ via $X^{\u22121}$ and then measure it there using $P$. It is a matter of a simple exercise to verify that $Q$ is a measure. This follows since $X^{\u22121}$ preserves set operations."
    },
    {
        "type": "example",
        "id": "Example 2.19",
        "name": "Image Measure Distribution",
        "topic": "Measures",
        "previous_results": [
            "Theorem 2.16"
        ],
        "statement": "Let $X$ be a real-valued random variable on a probability space $(\\\\Omega, \\\\mathcal{F}, P)$. Then $P \\\\circ X^{-1}$ is a probability measure on $\\\\mathbb{R}$, the distribution or the law of the variable $X$, and we often denote it by $\\\\mu_X$. We have $\\\\mu_X((\u2212\\\\infty, a]) = P(X \\\\leq a) =: F_X(a)$ is the distribution function of $X$, or of the measure $P \\\\circ X^{\u22121}$. Note that $\\\\mu_X$ is the Lebesgue-Stieltjes measure associated to $F_X$ through Theorem 2.16.\\n\\nLet $F$ be a distribution function on $\\\\mathbb{R}$ and $\\\\mu_F$ the Lebesgue-Stieltjes measure associated to $F$ through Theorem 2.16. Then the identity mapping on $(\\\\mathbb{R}, \\\\mathcal{B}(\\\\mathbb{R}), \\\\mu_F)$, i.e., $X(\\\\omega) = \\\\omega$, is a random variable distributed according to $\\\\mu_F$. The following example gives another, more canonical, way for such a construction.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 2.20",
        "name": "Quantile Function Construction",
        "topic": "Measures",
        "previous_results": [
            "Theorem 2.16"
        ],
        "statement": "Let $F$ be a distribution function on $\\\\mathbb{R}$. Define its right-continuous inverse $F^{-1}(z) = \\\\inf\\\\{y : F(y) > z\\\\}$, which is also known as the quantile function. Then a random variable $X$ on $([0, 1], \\\\mathcal{B}([0, 1]), \\\\text{Leb})$, given by $X(\\\\omega) = F^{-1}(\\\\omega)$ is distributed according to $\\\\mu_F$, $\\\\mu_X = \\\\mu_F$.",
        "proof": "To show this, first note that $F^{-1}$ is increasing and hence measurable. Then note that\\n\\n$\\\\{\\\\omega : \\\\omega < F(x)\\\\} \\\\subseteq \\\\{\\\\omega : F^{-1}(\\\\omega) \\\\leq x\\\\} \\\\subseteq \\\\{\\\\omega : \\\\omega \\\\leq F(x)\\\\}$\\n\\nand the outer sets both have the same Leb measure $F(x)$. It thus follows that\\n\\n$F_X(x) = \\\\text{Leb}(X \\\\leq x) = \\\\text{Leb}(F^{-1} \\\\leq x) = \\\\text{Leb}(\\\\{\\\\omega : F^{-1}(\\\\omega) \\\\leq x\\\\}) = \\\\text{Leb}(\\\\{\\\\omega : \\\\omega < F(x)\\\\}) = F(x).$\\n\\nThis tells us that we can always construct random variables with a given distribution. For two random variables $X,Y$, defined possibly on different probability spaces, we shall often write $X \\\\sim Y$ to denote $\\\\mu_X = \\\\mu_Y$, i.e., that $X$ and $Y$ have the same distribution. A lot of properties of random variables will in fact just functions of their distribution and not their particular definition."
    },
    {
        "type": "example",
        "id": "Example 2.21",
        "name": "Marginal Measure",
        "topic": "Measures",
        "previous_results": [
            "Definition 1.7",
            "Example 1.20"
        ],
        "statement": "Consider a probability measure $P$ on a product space from Definition 1.7, $(\\\\Omega, \\\\mathcal{F}) = (\\\\prod_{i\\\\in I} \\\\Omega_i, \\\\times_{i\\\\in I}\\\\mathcal{F}_i)$. Let $X_i(\\\\omega) = \\\\omega_i$, $1 \\\\leq i \\\\leq d$, be random variables given by coordinate projections, see Example 1.20. Then $\\\\mu_i := \\\\mu_{X_i}$ is called the $i$th marginal measure of $\\\\mu$. Note that $\\\\mu_i$ is a probability measure on $(\\\\Omega_i, \\\\mathcal{F}_i)$ and\\n\\n$\\\\mu_i(A) = \\\\mu (\\\\Omega_1 \\\\times ... \\\\Omega_{i-1} \\\\times A \\\\times \\\\Omega_{i+1} \\\\times ... \\\\Omega_n) , A \\\\in \\\\mathcal{F}_i$.\\n\\nNote that $\\\\mu$ determines its marginals but that the marginal distributions do not determine $\\\\mu$. Indeed, it is easy to construct examples of $\\\\mu \\\\neq \\\\nu$ with the same marginals. One way to do this is to use the method of the next example.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 2.22",
        "name": "Joint Distribution",
        "topic": "Measures",
        "previous_results": [
            "Example 1.20",
            "Lemma 2.23"
        ],
        "statement": "Let $X,Y$ be real-valued random variables on a probability space $(\\\\Omega, \\\\mathcal{F}, P)$. Then, by Example 1.20, $(X,Y)$ is an $\\\\mathbb{R}^2$-valued random variable. Its distribution, $\\\\mu_{(X,Y)}$ is called the the joint law of $X$ and $Y$. It is easy to verify (and follows instantly from Lemma 2.23 below) that its marginals are given by $\\\\mu_X$ and $\\\\mu_Y$, the distributions of $X$ and $Y$ respectively. However the joint law encodes also how the two variables behave jointly, i.e., their (in)dependence.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 2.23",
        "name": "Image Measure Transitivity",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, P)$ be a probability space, $(E, \\\\mathcal{E})$ and $(G, \\\\mathcal{G})$ two measurable spaces and $X : \\\\Omega \\\\to E$, $Y : E \\\\to G$ random variables. Then the image measure of $\\\\mu_X$ via $Y$ is the image measure of $\\\\mu$ via $Y \\\\circ X$.",
        "proof": "This is instantly seen with a simple drawing. More formally, we have\\n\\n$\\\\mu_X \\\\circ Y^{-1}(A) = \\\\mu_X (Y^{-1}(A)) = \\\\mu_X (\\\\{e \\\\in E : Y(e) \\\\in A\\\\}) = \\\\mu(X^{-1}(\\\\{e \\\\in E : Y(e) \\\\in A\\\\}))$\\n$= \\\\mu(\\\\{\\\\omega \\\\in \\\\Omega : Y(X(\\\\omega)) \\\\in A\\\\}) = \\\\mu((Y \\\\circ X)^{-1}(A)) = \\\\mu_{Y \\\\circ X}(A)$\\n\\nas required."
    },
    {
        "type": "example",
        "id": "Example 2.22",
        "name": "Joint Distribution",
        "topic": "Measures",
        "previous_results": [],
        "statement": "Let $X,Y$ be real-valued random variables on a probability space $(\\\\Omega, \\\\mathcal{F}, P)$. Then, by Example 1.20, $(X,Y)$ is an $\\\\mathbb{R}^2$-valued random variable. Its distribution, $\\\\mu_{(X,Y)}$ is called the the joint law of $X$ and $Y$. It is easy to verify (and follows instantly from Lemma 2.23 below) that its marginals are given by $\\\\mu_X$ and $\\\\mu_Y$, the distributions of $X$ and $Y$ respectively. However the joint law encodes also how the two variables behave jointly, i.e., their (in)dependence.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 2.24",
        "name": "Product Measure",
        "topic": "Product measure",
        "previous_results": [
            "Theorem 2.11",
            "Lemma 2.4",
            "Definition 1.7",
            "Exercise 1.13"
        ],
        "statement": "Let $(\\\\Omega_i, \\\\mathcal{F}_i, P_i)$, $i = 1, \\\\ldots, N$, be probability measures. Then there exists a unique measure $P$ on the product space $(\\\\Omega, \\\\mathcal{F}) = (\\\\prod_{i=1}^N \\\\Omega_i, \\\\times_{i=1}^N\\\\mathcal{F}_i)$ such that\\n\\n$P(E_1 \\\\times \\\\ldots \\\\times E_N) = P_1(E_1) \\\\cdot \\\\ldots \\\\cdot P_N(E_N)$, $E_i \\\\in \\\\mathcal{F}_i$, $1 \\\\leq i \\\\leq N$.\\n\\n$P$ is called the product measure and is also denoted $\\\\otimes_{i\\\\leq N} P_i$ or $P_1 \\\\otimes \\\\ldots \\\\otimes P_N$.",
        "proof": "We show the statement for $N = 2$. The general case then follows by induction since a general $N$ product can be see as product of two spaces: $\\\\Omega_1$ and $\\\\Omega_2 \\\\times \\\\ldots \\\\times \\\\Omega_N$.\\n\\nSuppose $N = 2$. A set in $\\\\mathcal{F}$ of the form $A \\\\times B$ for $A \\\\in \\\\mathcal{F}_1$, $B \\\\in \\\\mathcal{F}_2$ is called a measurable rectangle. These sets form a $\\\\pi$-system which, by Definition 1.7, generates $\\\\mathcal{F}$. Let $\\\\mathcal{A}$ denote the collection of finite unions of mutually disjoint measurable rectangles. Then $\\\\mathcal{A}$ is an algebra and we can define a set function $P$ on $\\\\mathcal{A}$ by\\n\\n$P(A_1 \\\\times B_1 \\\\cup \\\\ldots \\\\cup A_n \\\\times B_n) := \\\\sum_{i=1}^n P_1(A_i)P_2(B_i)$, $A_i \\\\in \\\\mathcal{F}_1$, $B_i \\\\in \\\\mathcal{F}_2$, $A_i \\\\times B_i \\\\cap A_j \\\\times B_j = \\\\emptyset$,\\n\\n$1 \\\\leq i, j \\\\leq n$, $i \\\\neq j$,\\n\\nfor $n \\\\geq 1$. Clearly $P(\\\\emptyset) = 0$ and, by Theorem 2.11, it remains to check that $P$ is countably additive on $\\\\mathcal{A}$. Let $(D_n)_{n\\\\geq 1}$ be a sequence of sets in $\\\\mathcal{A}$ with $D_n \\\\downarrow \\\\emptyset$. By Lemma 2.4, it suffices to show that $\\\\lim_{n\\\\to\\\\infty} P(D_n) = 0$.\\n\\nEach $D_n$ is a finite union of measurable rectangles $A_{n,k} \\\\times B_{n,k}$, $1 \\\\leq k \\\\leq m_n$. If $A_{n,i} \\\\cap A_{n,j} \\\\neq \\\\emptyset$, we may replace these two rectangles by three other rectangles with disjoint first sets, so that with no loss of generality we assume $A_{n\\\\cdot}$ are mutually disjoint. For $\\\\omega_1 \\\\in \\\\Omega_1$, let $D_n(\\\\omega_1) = \\\\{\\\\omega_2 \\\\in \\\\Omega_2 : (\\\\omega_1, \\\\omega_2) \\\\in D_n\\\\}$ so that $D_n(\\\\omega_1) = B_{n,k}$ if $\\\\omega_1 \\\\in A_{n,k}$, for some (and hence only one) $1 \\\\leq k \\\\leq m_n$ and $D_n(\\\\omega_2) = \\\\emptyset$ otherwise. In particular, $D_n(\\\\omega_1) \\\\in \\\\mathcal{F}_2$ (this also follows more generally, see Exercise 1.13). Properties of $(D_n)_{n\\\\geq 1}$ imply that $D_n(\\\\omega_1) \\\\downarrow \\\\emptyset$ for all $\\\\omega_1 \\\\in \\\\Omega_1$. Since $P_2$ is a probability measure, it follows that if we define a sequence of functions on $\\\\Omega_1$ by $X_n(\\\\omega_1) = P_2(D_n(\\\\omega_1))$, $n \\\\geq 1$, then $X_n \\\\downarrow 0$ pointwise on $\\\\Omega_1$. Note also that $X_n$ is a simple function, constant on any of the sets $A_{n,k}$, and zero otherwise. In particular, for $\\\\varepsilon > 0$,\\n\\n$X_n^{-1}((\\\\varepsilon, \\\\infty)) = \\\\{\\\\omega_1 : X_n(\\\\omega_1) > \\\\varepsilon\\\\} = \\\\bigcup_{k\\\\in I_n} A_{n,i}$,\\n\\nfor some subset $I_n \\\\subseteq \\\\{1, \\\\ldots, m_n\\\\}$. Again, by properties of $(D_n)_{n\\\\geq 1}$, we have $X_n^{-1}((\\\\varepsilon, \\\\infty)) \\\\downarrow \\\\emptyset$ and hence the $P_1$-probability of these sets decreases to zero. This yields\\n\\n$P(D_n) = \\\\sum_{k=1}^{m_n} P_1(A_{n,k})P_2(B_{n,k}) \\\\leq P_1(X_n > \\\\varepsilon)P_2(\\\\Omega_2) + \\\\varepsilon P_1(\\\\Omega_1)$,\\n\\nwhere we kept $P_i(\\\\Omega_i) = 1$ terms to make it clear how the inequalities were obtained. Taking limit as $n \\\\to \\\\infty$, gives $\\\\lim_{n\\\\to\\\\infty} P(D_n) \\\\leq \\\\varepsilon$ for any $\\\\varepsilon > 0$ and hence $\\\\lim_{n\\\\to\\\\infty} P(D_n) = 0$ as required."
    },
    {
        "type": "definition",
        "id": "Definition 3.1",
        "name": "Independent \u03c3-algebras",
        "topic": "Independence",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, P)$ be a probability space and $(\\\\mathcal{G}_i)_{i\\\\leq n}$ a finite collection of $\\\\sigma$-algebras, $\\\\mathcal{G}_i \\\\subseteq \\\\mathcal{F}$ for $i \\\\leq n$. We say that the $\\\\sigma$-algebras $(\\\\mathcal{G}_i)_{i\\\\leq n}$ are independent if and only if\\n\\n$P(A_1 \\\\cap \\\\ldots \\\\cap A_n) = P(A_1) \\\\cdot \\\\ldots \\\\cdot P(A_n)$,\\n\\nfor any $A_i \\\\in \\\\mathcal{G}_i$, $i \\\\leq n$.\\n\\nFor an arbitrary collection $(\\\\mathcal{G}_i)_{i\\\\in I}$ of sub-$\\\\sigma$-algebras of $\\\\mathcal{F}$, we say that these $\\\\sigma$-algebras are independent if any finite sub-collection of them is. Note that in (13) we could have $A_i = \\\\Omega$ for some indices $i$ and, in particular, a sub-collection of independent $\\\\sigma$-algebras are also independent. Equally clearly, if $(\\\\mathcal{G}_i)_{i\\\\in I}$ are independent and $\\\\mathcal{H}_i \\\\subseteq \\\\mathcal{G}_i$, $i \\\\in I$, are further sub-$\\\\sigma$-algebras, then $(\\\\mathcal{H}_i)_{i\\\\in I}$ are also independent.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 3.2",
        "name": "Trivial \u03c3-algebra Independence",
        "topic": "Independence",
        "previous_results": [],
        "statement": "The trivial $\\\\sigma$-algebra $\\\\{\\\\Omega, \\\\emptyset\\\\}$ is independent of any other $\\\\sigma$-algebra. Its information content is null. More generally, observe that $\\\\mathcal{G}$ is independent of itself if and only if $P(A) \\\\in \\\\{0, 1\\\\}$ for all $A \\\\in \\\\mathcal{G}$.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 3.4",
        "name": "Independent Generated \u03c3-algebras",
        "topic": "Independence",
        "previous_results": [
            "Theorem 3.5"
        ],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and $A_1, \\\\ldots, A_n$ some events in $F$. Then, their generated $\\\\sigma$-algebras are independent if and only if\\n\\n$P\\\\left(\\\\bigcap_{i\\\\in J} A_i\\\\right) = \\\\prod_{i\\\\in J} P(A_i)$,\\n\\nfor any $J \\\\subseteq \\\\{1, \\\\ldots, n\\\\}$.",
        "proof": "The results follows from Theorem 3.5 below since $\\\\mathcal{A}_i = \\\\{A_i\\\\}$ is a $\\\\pi$-system. But it can also be shown by hand, which we do for $n = 2$. One direction is obvious. For the other recall that $\\\\sigma(A) = \\\\{\\\\Omega, \\\\emptyset, A, A^c\\\\}$ and note that if $P(A \\\\cap B) = P(A)P(B)$ then\\n\\n$P(A \\\\cap B^c) = P(A) - P(A \\\\cap B) = P(A)(1 - P(B)) = P(A)P(B^c)$\\n\\nand the result follows by symmetry."
    },
    {
        "type": "theorem",
        "id": "Theorem 3.5",
        "name": "\u03c0-system Independence",
        "topic": "Independence",
        "previous_results": [
            "Lemma 1.12"
        ],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space, $(\\\\mathcal{G}_i)_{i\\\\in I}$ an arbitrary collection of $\\\\sigma$-algebras, each generated by a $\\\\pi$-system $\\\\mathcal{A}_i \\\\subseteq F$, i.e., $\\\\mathcal{G}_i = \\\\sigma(\\\\mathcal{A}_i)$, $i \\\\in I$. Then $(\\\\mathcal{G}_i)_{i\\\\in I}$ are independent if and only if\\n\\n$P\\\\left(\\\\bigcap_{i\\\\in J} A_i\\\\right) = \\\\prod_{i\\\\in J} P(A_i)$\\n\\nfor any $A_i \\\\in \\\\mathcal{A}_i$, $i \\\\in J$, for any finite subset $J \\\\subseteq I$.",
        "proof": "If $(\\\\mathcal{G}_i)_{i\\\\in I}$ are independent then, by definition, (14) holds. The reverse implication is a simple application of Lemma 1.12 but we give the details nevertheless. Fix a finite subset $J \\\\subset I$ and number its elements $J = \\\\{i_1, \\\\ldots, i_n\\\\}$. Let $\\\\mathcal{M}_1$ be the set of $A \\\\in F$ for which\\n\\n$P(A \\\\cap A_2 \\\\cap \\\\ldots \\\\cap A_n) = P(A) \\\\cdot P(A_2) \\\\cdot \\\\ldots \\\\cdot P(A_n)$\\n\\nfor any $A_l \\\\in \\\\mathcal{A}_{i_l}$, $l = 2, \\\\ldots, n$.\\n\\nBy assumption, $\\\\mathcal{A}_{i_1} \\\\subseteq \\\\mathcal{M}_1$ and also $\\\\Omega \\\\in \\\\mathcal{M}_1$ by the assumption applied to $J_1 = J \\\\setminus \\\\{i_1\\\\}$. For $A \\\\subseteq B$ both in $\\\\mathcal{M}_1$, we have\\n\\n$P((B \\\\setminus A) \\\\cap A_2 \\\\cap \\\\ldots \\\\cap A_n) = P(B \\\\cap A_2 \\\\cap \\\\ldots \\\\cap A_n) - P(A \\\\cap A_2 \\\\cap \\\\ldots \\\\cap A_n)$\\n\\n$= (P(B) - P(A))P(A_2) \\\\ldots P(A_n) = P(B \\\\setminus A)P(A_2) \\\\ldots P(A_n)$\\n\\nso that $B \\\\setminus A \\\\in \\\\mathcal{M}_1$. Finally, for an increasing sequence $B_k \\\\in \\\\mathcal{M}_1$, $B_k \\\\uparrow B$, continuity from below of $P$ implies that $B \\\\in \\\\mathcal{M}_1$. We conclude that $\\\\mathcal{M}_1$ is a $\\\\lambda$-system and hence, by the $\\\\pi$-$\\\\lambda$ systems Lemma (Lemma 1.12), $\\\\mathcal{G}_{i_1} = \\\\sigma(\\\\mathcal{A}_{i_1}) \\\\subseteq \\\\mathcal{M}_1$. We then proceed by induction. We let $\\\\mathcal{M}_k$ be the $A \\\\in F$ for which\\n\\n$P(A_1 \\\\cap \\\\ldots A_{k-1} \\\\cap A \\\\cap A_{k+1} \\\\ldots \\\\cap A_n) = P(A_1) \\\\cdot \\\\ldots \\\\cdot P(A_{k-1}) \\\\cdot P(A) \\\\cdot P(A_{k+1}) \\\\cdot \\\\ldots \\\\cdot P(A_n)$,\\n\\nfor any $A_l \\\\in \\\\mathcal{G}_{i_l}$, $1 \\\\leq l < k$ and $A_l \\\\in \\\\mathcal{A}_{i_l}$, $k < l \\\\leq n$. Then, by induction step, $\\\\mathcal{A}_{i_k} \\\\subseteq \\\\mathcal{M}_k$ and, as above, $\\\\pi$-$\\\\lambda$ systems lemma gives $\\\\mathcal{G}_{i_k} \\\\subseteq \\\\mathcal{M}_k$."
    },
    {
        "type": "corollary",
        "id": "Corollary 3.6",
        "name": "Merged Independent \u03c3-algebras",
        "topic": "Independence",
        "previous_results": [
            "Theorem 3.5"
        ],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and $(\\\\mathcal{G}_i)_{i\\\\in I}$ a collection of independent $\\\\sigma$-algebras. Suppose that $I_j \\\\subseteq I$, $j \\\\in J$ are all pairwise disjoint. Then $(\\\\sigma(\\\\mathcal{G}_i : i \\\\in I_j) : j \\\\in J)$ are independent $\\\\sigma$-algebras.",
        "proof": "Let $\\\\mathcal{H}_j = \\\\sigma(\\\\mathcal{G}_i : i \\\\in I_j)$ and\\n\\n$\\\\mathcal{A}_j = \\\\left\\\\{\\\\bigcap_{i\\\\in K} A_i : A_i \\\\in \\\\mathcal{G}_i, i \\\\in K, \\\\text{ for some } K \\\\text{ a finite subset of } I_j\\\\right\\\\}$\\n\\nbe the collection of finite intersection of sets from $\\\\mathcal{G}_i$, $i \\\\in I_j$. Note that $\\\\mathcal{A}_j$ is a $\\\\pi$-system and $\\\\sigma(\\\\mathcal{A}_j) = \\\\mathcal{H}_j$. Note also that a finite subset of $I_j$ is, in particular, a finite subset of $I$ and hence, by the assumed independence, probability of a set in $\\\\mathcal{A}_j$ factorises as in (13). Now let $L$ be a finite subset of $J$ and consider sets $B_l \\\\in \\\\mathcal{A}_l$, $l \\\\in L$. Each of these sets is itself an intersection of sets, i.e., $B_l = \\\\bigcap_{i\\\\in K_l} A^l_i$, where $K_l$ is a finite subset of $I_l$ and $A^l_i \\\\in \\\\mathcal{G}_i$. As all $K_l$, $l \\\\in L$, are disjoint, we obtain\\n\\n$P\\\\left(\\\\bigcap_{l\\\\in L} B_l\\\\right) = P\\\\left(\\\\bigcap_{l\\\\in L,i\\\\in K_l} A^l_i\\\\right) = \\\\prod_{l\\\\in L,i\\\\in K_l} P(A^l_i) = \\\\prod_{l\\\\in L} \\\\prod_{i\\\\in K_l} P(A^l_i) = \\\\prod_{l\\\\in L} P\\\\left(\\\\bigcap_{i\\\\in K_l} A^l_i\\\\right) = \\\\prod_{l\\\\in L} P(B_l)$,\\n\\nwhere we used independence of $\\\\mathcal{G}_i$, $i \\\\in I$ and (13) for the second equality, and we used independence of $\\\\mathcal{G}_i$, $i \\\\in I_l$ separately for each $l \\\\in L$ to obtain the fourth equality. The claim now follows by Theorem 3.5."
    },
    {
        "type": "definition",
        "id": "Definition 3.7",
        "name": "Independent Random Variables",
        "topic": "Independence",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and $(X_i)_{i\\\\in I}$ a family of random variables with values in some measurable spaces $(E_i, \\\\mathcal{E}_i)_{i\\\\in I}$. We say that these random variables are independent if their generated $\\\\sigma$-algebras $(\\\\sigma(X_i))_{i\\\\in I}$ are.\\n\\nIt follows by the definition that $(X_i)_{i\\\\in I}$ are independent if and only if for any finite subset $J \\\\subseteq I$\\n\\n$P(X_i \\\\in A_i \\\\text{ for all } i \\\\in J) = \\\\prod_{i\\\\in J} P(X_i \\\\in A_i)$,\\n\\nfor any $A_i \\\\in \\\\mathcal{E}_i$, $i \\\\in J$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 3.8",
        "name": "Independence Characterization",
        "topic": "Independence",
        "previous_results": [
            "Definition 3.7",
            "Theorem 3.5",
            "Theorem 2.24"
        ],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and $(X_i)_{i\\\\leq n}$ a finite family of random variables with values in some measurable spaces $(E_i, \\\\mathcal{E}_i)_{i\\\\leq n}$. These random variables are independent if and only if their joint distribution $\\\\mu_{(X_1,...,X_n)}$ on the product space $(\\\\prod_{i\\\\leq n} E_i, \\\\times_{i\\\\leq n}\\\\mathcal{E}_i)$ is the product measure of the marginal distributions $\\\\mu_{X_i}$.",
        "proof": "This follows directly from the definitions. By Definition 3.7, the random variables $(X_i)_{i\\\\leq n}$ are independent if and only if their generated $\\\\sigma$-algebras $(\\\\sigma(X_i))_{i\\\\leq n}$ are independent. By Theorem 3.5, this is equivalent to\\n\\n$P\\\\left(\\\\bigcap_{i\\\\in J} X_i^{-1}(A_i)\\\\right) = \\\\prod_{i\\\\in J} P(X_i^{-1}(A_i))$\\n\\nfor any finite subset $J \\\\subseteq \\\\{1,\\\\ldots,n\\\\}$ and any $A_i \\\\in \\\\mathcal{E}_i$. This can be rewritten as\\n\\n$P(X_1 \\\\in A_1, \\\\ldots, X_n \\\\in A_n) = P(X_1 \\\\in A_1) \\\\cdots P(X_n \\\\in A_n)$\\n\\nwhich is precisely the statement that the joint distribution $\\\\mu_{(X_1,\\\\ldots,X_n)}$ is the product measure of the marginal distributions $\\\\mu_{X_i}$, as constructed in Theorem 2.24."
    },
    {
        "type": "corollary",
        "id": "Corollary 3.9",
        "name": "Independence Criterion",
        "topic": "Independence",
        "previous_results": [
            "Theorem 3.8",
            "Theorem 3.5"
        ],
        "statement": "A sequence $(X_n)_{n\\\\geq 1}$ of real-valued random variables on $(\\\\Omega, F, P)$ is independent if and only if for all $n \\\\geq 1$ and all $x_1, \\\\ldots x_n \\\\in \\\\mathbb{R}$ (or $\\\\overline{\\\\mathbb{R}}$),\\n\\n$P(X_1 \\\\leq x_1, \\\\ldots, X_n \\\\leq x_n) = P(X_1 \\\\leq x_1) \\\\ldots P(X_n \\\\leq x_n)$.",
        "proof": "This follows directly from Theorem 3.8 and Theorem 3.5. For real-valued random variables, the sets of the form $(-\\\\infty, x]$ for $x \\\\in \\\\mathbb{R}$ form a $\\\\pi$-system that generates the Borel $\\\\sigma$-algebra $\\\\mathcal{B}(\\\\mathbb{R})$. By Theorem 3.5, it suffices to check independence on these generating sets.\\n\\nFor any $n \\\\geq 1$ and any $x_1, \\\\ldots, x_n \\\\in \\\\mathbb{R}$, the event $\\\\{X_1 \\\\leq x_1, \\\\ldots, X_n \\\\leq x_n\\\\}$ can be written as $\\\\bigcap_{i=1}^n X_i^{-1}((-\\\\infty, x_i])$. By Theorem 3.8, the random variables are independent if and only if\\n\\n$P\\\\left(\\\\bigcap_{i=1}^n X_i^{-1}((-\\\\infty, x_i])\\\\right) = \\\\prod_{i=1}^n P(X_i^{-1}((-\\\\infty, x_i]))$\\n\\nwhich is equivalent to\\n\\n$P(X_1 \\\\leq x_1, \\\\ldots, X_n \\\\leq x_n) = P(X_1 \\\\leq x_1) \\\\cdots P(X_n \\\\leq x_n)$\\n\\nas required."
    },
    {
        "type": "example",
        "id": "Example 3.10",
        "name": "Coin Tossing Model",
        "topic": "Independence",
        "previous_results": [
            "Example 1.22"
        ],
        "statement": "Recall our coin tossing representation in Example 1.22, namely on $([0, 1], \\\\mathcal{B}([0, 1]))$ we let $X_n(\\\\omega) = 1_{\\\\lfloor 2^n\\\\omega\\\\rfloor \\\\text{ is even}}$, $n \\\\geq 1$, where $0$ is even. We can now check that $(X_n)_{n\\\\geq 1}$ are independent (exercise!). This shows that we built a good model, as different coin tosses ought to be independent, and also that the notion of independence is interesting (and non-vacuous as already observed).",
        "proof": "To verify independence, we need to check that for any finite subset $\\\\{n_1, n_2, \\\\ldots, n_k\\\\}$ and any choice of values $a_1, a_2, \\\\ldots, a_k \\\\in \\\\{0, 1\\\\}$, we have\\n\\n$P(X_{n_1} = a_1, X_{n_2} = a_2, \\\\ldots, X_{n_k} = a_k) = P(X_{n_1} = a_1) \\\\cdot P(X_{n_2} = a_2) \\\\cdots P(X_{n_k} = a_k)$\\n\\nWithout loss of generality, assume $n_1 < n_2 < \\\\ldots < n_k$. For each $n$, the function $X_n$ divides $[0,1]$ into $2^n$ equal intervals, and $X_n = 1$ on exactly half of these intervals (where $\\\\lfloor 2^n\\\\omega\\\\rfloor$ is even).\\n\\nThe key observation is that for $m > n$, each interval where $X_n$ is constant contains exactly $2^{m-n}$ intervals where $X_m$ is constant, and exactly half of these have $X_m = 0$ and half have $X_m = 1$, regardless of the value of $X_n$.\\n\\nThis means that the events $\\\\{X_{n_i} = a_i\\\\}$ partition $[0,1]$ into a collection of intervals, and the probability of their intersection equals the product of their individual probabilities, which is $2^{-k}$. This confirms independence."
    },
    {
        "type": "proposition",
        "id": "Proposition 3.11",
        "name": "Functions of Independent RVs",
        "topic": "Independence",
        "previous_results": [
            "Definition 3.7"
        ],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and $(X_i)_{i\\\\in I}$ a family of independent random variables with values in some measurable spaces $(E_i, \\\\mathcal{E}_i)_{i\\\\in I}$ and $f_i: E_i \\\\to \\\\mathbb{R}$ be measurable, $i \\\\in I$. Then $(Y_i := f_i(X_i))_{i\\\\in I}$ are independent random variables.",
        "proof": "By Definition 3.7, the random variables $(X_i)_{i\\\\in I}$ are independent if and only if their generated $\\\\sigma$-algebras $(\\\\sigma(X_i))_{i\\\\in I}$ are independent.\\n\\nFor each $i \\\\in I$, since $Y_i = f_i(X_i)$, we have $\\\\sigma(Y_i) \\\\subseteq \\\\sigma(X_i)$. This follows because any event in $\\\\sigma(Y_i)$ can be written as $Y_i^{-1}(B) = X_i^{-1}(f_i^{-1}(B))$ for some Borel set $B$, and thus belongs to $\\\\sigma(X_i)$.\\n\\nNow, if $(\\\\sigma(X_i))_{i\\\\in I}$ are independent $\\\\sigma$-algebras, then any collection of sub-$\\\\sigma$-algebras $(\\\\mathcal{G}_i)_{i\\\\in I}$ with $\\\\mathcal{G}_i \\\\subseteq \\\\sigma(X_i)$ are also independent. This is because for any finite subset $J \\\\subset I$ and any choice of sets $A_i \\\\in \\\\mathcal{G}_i \\\\subseteq \\\\sigma(X_i)$ for $i \\\\in J$, we have\\n\\n$P\\\\left(\\\\bigcap_{i\\\\in J} A_i\\\\right) = \\\\prod_{i\\\\in J} P(A_i)$\\n\\nby the independence of $(\\\\sigma(X_i))_{i\\\\in I}$.\\n\\nTherefore, the $\\\\sigma$-algebras $(\\\\sigma(Y_i))_{i\\\\in I}$ are independent, which means the random variables $(Y_i)_{i\\\\in I}$ are independent."
    },
    {
        "type": "definition",
        "id": "Definition 3.12",
        "name": "Tail \u03c3-algebra",
        "topic": "Independence",
        "previous_results": [],
        "statement": "For a sequence of random variables $(X_n)_{n\\\\geq 1}$ define\\n\\n$T_n = \\\\sigma (X_{n+1}, X_{n+2} \\\\ldots)$\\n\\nand\\n\\n$T = \\\\bigcap_{n=1}^{\\\\infty} T_n$.\\n\\nThen $T$ is called the tail $\\\\sigma$-algebra of the sequence $(X_n)_{n\\\\geq 1}$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 3.14",
        "name": "Kolmogorov's 0-1 Law",
        "topic": "Independence",
        "previous_results": [
            "Theorem 3.5",
            "Corollary 3.6"
        ],
        "statement": "Let $(X_n)_{n\\\\geq1}$ be a sequence of independent random variables. Then the tail $\\\\sigma$-algebra $T$ of $(X_n)_{n\\\\geq1}$ contains only events of probability $0$ or $1$. Moreover, any $T$-measurable random variable is almost surely constant.",
        "proof": "Fix $n \\\\geq 1$ and let $F_n = \\\\sigma (X_1, \\\\ldots, X_n)$. Note that $F_n$ is generated by the $\\\\pi$-system of events\\n\\n$A = \\\\{\\\\{X_1 \\\\leq x_1, \\\\ldots, X_n \\\\leq x_n\\\\} : x_1, \\\\ldots, x_n \\\\in \\\\mathbb{R}\\\\}$\\n\\nand $T_n$ is generated by the $\\\\pi$-system of events\\n\\n$B = \\\\{\\\\{X_{n+1} \\\\leq x_{n+1}, \\\\ldots, X_{n+k} \\\\leq x_{n+k}\\\\} : k \\\\geq 1, x_{n+1}, \\\\ldots, x_{n+k} \\\\in \\\\mathbb{R}\\\\}$.\\n\\nFor any $A \\\\in A$, $B \\\\in B$, by the independence of the random variables $(X_n)$, we have\\n\\n$P(A \\\\cap B) = P(A)P(B)$\\n\\nand so by Theorem 3.5 the $\\\\sigma$-algebras $\\\\sigma(A) = F_n$ and $\\\\sigma(B) = T_n$ are also independent. Note also that this statement follows directly from Corollary 3.6. Since $T \\\\subseteq T_n$ we conclude that $F_n$ and $T$ are also independent.\\n\\nThe above was true for all $n \\\\geq 1$ and hence $\\\\bigcup_{n\\\\geq1} F_n$ and $T$ are also independent. Now $\\\\bigcup_{n\\\\geq1} F_n$ is a $\\\\pi$-system (although not in general a $\\\\sigma$-algebra) generating the $\\\\sigma$-algebra $F_\\\\infty = \\\\sigma ((X_n)_{n\\\\geq1})$. So applying Theorem 3.5 again we see that $F_\\\\infty$ and $T$ are independent. But $T \\\\subseteq F_\\\\infty$ so that if $A \\\\in T$\\n\\n$P(A) = P(A \\\\cap A) = P(A)^2$\\n\\nand so $P(A) = 0$ or $P(A) = 1$.\\n\\nNow suppose that $Y$ is any (real-valued) $T$-measurable random variable. Then its distribution function $F_Y(y) = P(Y \\\\leq y)$ is increasing, right continuous and takes only values in $\\\\{0, 1\\\\}$ since $\\\\{Y \\\\leq y\\\\} \\\\in T$. So $P(Y = c) = 1$ where $c = \\\\inf\\\\{y : F_Y(y) = 1\\\\}$. This extends easily to the extended-real-valued case."
    },
    {
        "type": "example",
        "id": "Example 3.15",
        "name": "Tail Random Variables",
        "topic": "Independence",
        "previous_results": [],
        "statement": "Let $(X_n)_{n\\\\geq1}$ be a sequence of independent, identically distributed (i.i.d.) random variables and let $S_n = \\\\sum_{k=1}^n X_k$. Consider $U = \\\\limsup_{n\\\\to\\\\infty} S_n/n$ and $L = \\\\liminf_{n\\\\to\\\\infty} S_n/n$. Then $U$ and $L$ are tail random variables and so almost surely constant. We'll prove later in the course that, $L = U$ is the expectation of $X_1$, a result known as the Strong Law of Large Numbers.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 3.16",
        "name": "Limsup and Liminf of Sets",
        "topic": "The Borel-Cantelli Lemmas",
        "previous_results": [],
        "statement": "Let $(A_n)_{n\\\\geq1}$ be a sequence of sets from $F$. We define\\n\\n$\\\\limsup_{n\\\\to\\\\infty} A_n = \\\\bigcap_{n=1}^\\\\infty \\\\bigcup_{m\\\\geq n} A_m$\\n\\n$= \\\\{\\\\omega \\\\in \\\\Omega : \\\\omega \\\\in A_m \\\\text{ for infinitely many } m\\\\}$\\n$= \\\\{A_n \\\\text{ occurs infinitely often}\\\\}$\\n$= \\\\{A_n \\\\text{ i.o.}\\\\}$\\n\\nand\\n\\n$\\\\liminf_{n\\\\to\\\\infty} A_n = \\\\bigcup_{n=1}^\\\\infty \\\\bigcap_{m\\\\geq n} A_m$\\n\\n$= \\\\{\\\\omega \\\\in \\\\Omega : \\\\exists m_0(\\\\omega) \\\\text{ such that } \\\\omega \\\\in A_m \\\\text{ for all } m \\\\geq m_0(\\\\omega)\\\\}$\\n$= \\\\{A_n \\\\text{ eventually}\\\\}$\\n$= \\\\{A_n^c \\\\text{ infinitely often}\\\\}^c$.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 3.17",
        "name": "Indicator Functions of Limsup/Liminf",
        "topic": "The Borel-Cantelli Lemmas",
        "previous_results": [],
        "statement": "$1_{\\\\limsup_{n\\\\to\\\\infty} A_n} = \\\\limsup_{n\\\\to\\\\infty} 1_{A_n}$,\\n\\n$1_{\\\\liminf_{n\\\\to\\\\infty} A_n} = \\\\liminf_{n\\\\to\\\\infty} 1_{A_n}$.",
        "proof": "Note that $1_{\\\\bigcup_n A_n} = \\\\sup_n 1_{A_n}$ and $1_{\\\\bigcap_n A_n} = \\\\inf_n 1_{A_n}$, and apply these twice."
    },
    {
        "type": "lemma",
        "id": "Lemma 3.18",
        "name": "Fatou and Reverse Fatou for sets",
        "topic": "The Borel-Cantelli Lemmas",
        "previous_results": [
            "Proposition 2.3"
        ],
        "statement": "Let $(A_n)_{n\\\\geq1}$ be a sequence of sets from $F$. Then\\n\\n$P(\\\\liminf_{n\\\\to\\\\infty} A_n) \\\\leq \\\\liminf_{n\\\\to\\\\infty} P(A_n)$ and $P(\\\\limsup_{n\\\\to\\\\infty} A_n) \\\\geq \\\\limsup_{n\\\\to\\\\infty} P(A_n)$.",
        "proof": "Using continuity of $P$ from above and below, see Proposition 2.3, we have\\n\\n$P(A_n \\\\text{ eventually}) = \\\\lim_{n\\\\to\\\\infty} P\\\\left(\\\\bigcap_{m\\\\geq n} A_m\\\\right) \\\\leq \\\\lim_{n\\\\to\\\\infty} \\\\inf_{m\\\\geq n} P(A_m) = \\\\liminf_{n\\\\to\\\\infty} P(A_n)$\\n\\nand hence (taking complements)\\n\\n$P(A_n \\\\text{ i.o.}) \\\\geq \\\\limsup_{n\\\\to\\\\infty} P(A_n)$."
    },
    {
        "type": "lemma",
        "id": "Lemma 3.19",
        "name": "First Borel\u2013Cantelli Lemma",
        "topic": "Independence",
        "previous_results": [
            "Proposition 2.3"
        ],
        "statement": "If $\\\\sum_{n=1}^{\\\\infty} P(A_n) < \\\\infty$ then $P(A_n \\\\text{ i.o.}) = 0$.",
        "proof": "Let $G_n = \\\\bigcup_{m\\\\geq n} A_m$. Then\\n\\n$P(G_n) \\\\leq \\\\sum_{m=n}^{\\\\infty} P(A_m)$\\n\\nand $G_n \\\\downarrow G = \\\\limsup_{n\\\\to\\\\infty} A_n$, so by Proposition 2.3, $P(G_n) \\\\downarrow P(G)$.\\n\\nSince $\\\\sum_{n=1}^{\\\\infty} P(A_n) < \\\\infty$, we have that\\n\\n$\\\\sum_{m=n}^{\\\\infty} P(A_m) \\\\to 0 \\\\quad \\\\text{as } n \\\\to \\\\infty,$\\n\\nand so\\n\\n$P\\\\left(\\\\limsup_{n\\\\to\\\\infty} A_n\\\\right) = \\\\lim_{n\\\\to\\\\infty} P(G_n) = 0$\\n\\nas required."
    },
    {
        "type": "lemma",
        "id": "Lemma 3.20",
        "name": "Second Borel\u2013Cantelli Lemma",
        "topic": "Independence",
        "previous_results": [
            "Exercise 3.3"
        ],
        "statement": "Let $(A_n)$ be a sequence of independent events. If $\\\\sum_{n=1}^{\\\\infty} P(A_n) = \\\\infty$ then $P(A_n \\\\text{ i.o.}) = 1$.",
        "proof": "Set $a_m = P(A_m)$ and note that $1 - a \\\\leq e^{-a}$. We consider the complementary event $\\\\{A_n^c \\\\text{ eventually}\\\\}$.\\n\\n$P\\\\left(\\\\bigcap_{m\\\\geq n} A_m^c\\\\right) = \\\\prod_{m\\\\geq n} (1 - a_m)$ (by independence, recall Exercise 3.3)\\n\\n$\\\\leq \\\\exp\\\\left(-\\\\sum_{m\\\\geq n} a_m\\\\right) = 0.$\\n\\nHence\\n\\n$P(A_n^c \\\\text{ eventually}) = P\\\\left(\\\\bigcup_{n\\\\geq 1} \\\\bigcap_{m\\\\geq n} A_m^c\\\\right) = \\\\lim_{n\\\\to\\\\infty} P\\\\left(\\\\bigcap_{m\\\\geq n} A_m^c\\\\right) = 0,$\\n\\nand\\n\\n$P(A_n \\\\text{ i.o.}) = 1 - P(A_n^c \\\\text{ eventually}) = 1.$"
    },
    {
        "type": "definition",
        "id": "Definition 4.1",
        "name": "Simple Function Integral",
        "topic": "Integration",
        "previous_results": [],
        "statement": "If $f$ is a non-negative simple function with canonical form (6), then we define the integral of $f$ with respect to $\\\\mu$ as\\n\\n$\\\\int f d\\\\mu = \\\\sum_{k=1}^n a_k\\\\mu(E_k)$.\\n\\nThis formula then also applies (exercise) whenever $f$ is as in (6), even if this is not the canonical form, as long as we avoid $\\\\infty - \\\\infty$ (for example by taking $a_k \\\\geq 0$).",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 4.2",
        "name": "General Integral Definition",
        "topic": "Integration",
        "previous_results": [
            "Lemma 1.26"
        ],
        "statement": "For a non-negative measurable function $f$ on $(\\\\Omega, \\\\mathcal{F}, \\\\mu)$ we define the integral\\n\\n$\\\\int f d\\\\mu = \\\\sup\\\\left\\\\{\\\\int g d\\\\mu : g \\\\text{ simple}, 0 \\\\leq g \\\\leq f\\\\right\\\\}$.\\n\\nNote that the supremum may be equal to $+\\\\infty$. Recall from Lemma 1.26 that measurability of $f$ is equivalent with $f$ being an increasing limit of simple function. The above definition and this notion of integral can not be extended to non-measurable functions in any meaningful way. Indeed, we know well by now that we can not measure - that is integrate the indicator function - some non-measurable sets! We recall also that one can use a canonical construction to approximate $f$, see the proof of Lemma 1.26, and the above supremum may be replaced with a limit along such an approximating sequence of simple functions \u2013 this is easy to check directly (exercise!) but it will also follow from the more general Theorem 4.6.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 4.3",
        "name": "Integrable Function",
        "topic": "Integration",
        "previous_results": [],
        "statement": "We say that a function $f$ on $(\\\\Omega, \\\\mathcal{F}, \\\\mu)$ is integrable, and write $f \\\\in L^1(\\\\Omega, \\\\mathcal{F}, \\\\mu)$, if $f$ is measurable and $\\\\int |f|d\\\\mu < \\\\infty$. If $f$ is integrable, its integral is defined to be\\n\\n$\\\\int f d\\\\mu = \\\\int f^+ d\\\\mu - \\\\int f^- d\\\\mu$,\\n\\nwhere $f^+ = \\\\max(f, 0)$ and $f^- = \\\\max(-f, 0)$ are the positive and negative parts of $f$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 4.4",
        "name": "Lebesgue Integral",
        "topic": "Integration",
        "previous_results": [],
        "statement": "If $\\\\mu$ is the Lebesgue measure on $(\\\\mathbb{R}, \\\\mathcal{B}(\\\\mathbb{R}))$, then we have just redefined the Lebesgue integral as in Part A.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 4.5",
        "name": "Discrete Measure Integration",
        "topic": "Integration",
        "previous_results": [],
        "statement": "Suppose that $\\\\mu$ is a discrete measure with mass $p_i$ at point $x_i \\\\in \\\\mathbb{R}$, for a (finite or countably infinite) sequence $x_1, x_2, \\\\ldots$. Then you can check that\\n\\n$\\\\int f d\\\\mu = \\\\sum_{i} f(x_i)p_i$,\\n\\nwhenever $f \\\\geq 0$ (where $+\\\\infty$ is allowed as the answer) or the sum converges absolutely. This example is very different in nature to the Lebesgue integral above \u2013 here integrals are just sums. It is rather pleasing to see that the toolbox we developed covers both cases with a unified language.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 4.6",
        "name": "Monotone Convergence Theorem",
        "topic": "Integration",
        "previous_results": [
            "Proposition 2.3"
        ],
        "statement": "Let $(f_n)$ be a sequence of non-negative measurable functions on $(\\\\Omega, \\\\mathcal{F}, \\\\mu)$. Then\\n\\n$f_n \\\\uparrow f \\\\Rightarrow \\\\int f_n d\\\\mu \\\\uparrow \\\\int f d\\\\mu$.\\n\\nNote that we are not excluding $\\\\int f d\\\\mu = \\\\infty$ here. Also, it is easy to see that it is enough to suppose that $f_n \\\\uparrow f$ $\\\\mu$-almost everywhere. An equivalent formulation of the Monotone Convergence Theorem (MCT) considers partial sums: if $(f_n)$ is a sequence of non-negative measurable functions, then\\n\\n$\\\\int \\\\sum_{n=1}^{\\\\infty} f_n d\\\\mu = \\\\sum_{n=1}^{\\\\infty} \\\\int f_n d\\\\mu$.",
        "proof": "Note that the MCT for $f_n = 1_{A_n}$ is simply the continuity of $\\\\mu$ from below, Proposition 2.3 (iv). The general case is deduced from this, see Part A Integration."
    },
    {
        "type": "exercise",
        "id": "Exercise 4.7",
        "name": "Linearity of Integration",
        "topic": "Integration",
        "previous_results": [],
        "statement": "As a simple warmup exercise, show that if $f$ and $g$ are measurable functions on $(\\\\Omega, \\\\mathcal{F}, \\\\mu)$ that are either both non-negative or both integrable, and $c \\\\in \\\\mathbb{R}$, then\\n\\n$\\\\int (f + g)d\\\\mu = \\\\int f d\\\\mu + \\\\int g d\\\\mu$,\\n\\n$\\\\int c f d\\\\mu = c \\\\int f d\\\\mu$.",
        "proof": ""
    },
    {
        "type": "exercise",
        "id": "Exercise 4.8",
        "name": "MCT Application",
        "topic": "Integration",
        "previous_results": [
            "Lemma 3.19",
            "Theorem 4.6"
        ],
        "statement": "Use MCT to prove Lemma 3.19.",
        "proof": "Consider $N_n := \\\\sum_{k=1}^n 1_{A_k}$, the (random) number of events $A_k$ that hold for $k \\\\leq n$. Then $\\\\int N_n dP = \\\\sum_{k=1}^n P(A_k)$. Since $N_n \\\\uparrow N = N_{\\\\infty}$, by MCT, we have $\\\\int N dP = \\\\sum_{k\\\\geq 1} P[A_k] < \\\\infty$. But $\\\\int N dP < \\\\infty$ implies $P(N = \\\\infty) = 0$, as required."
    },
    {
        "type": "theorem",
        "id": "Theorem 4.9",
        "name": "Radon-Nikodym Theorem",
        "topic": "Integration",
        "previous_results": [
            "Definition 2.7",
            "Theorem 2.16",
            "Theorem 2.10"
        ],
        "statement": "Let $\\\\mu, \\\\nu$ be two probability measures on a measurable space $(\\\\Omega, \\\\mathcal{F})$. Then $\\\\nu \\\\ll \\\\mu$ if and only if there exists a non-negative random variable $f$ such that\\n\\n$\\\\nu(A) = \\\\int_A f d\\\\mu, A \\\\in \\\\mathcal{F}$.\\n\\nThe function $f$ is often denoted $\\\\frac{d\\\\nu}{d\\\\mu}$ and is called the Radon-Nikodym derivative of $\\\\nu$ w.r.t. $\\\\mu$. Further, $\\\\nu \\\\sim \\\\mu$ if and only if $f > 0$ $\\\\mu$-a.s. (and then also $\\\\nu$-a.s.) in which case $\\\\frac{d\\\\mu}{d\\\\nu} = \\\\frac{1}{f}$.",
        "proof": ""
    },
    {
        "type": "exercise",
        "id": "Exercise 4.10",
        "name": "Discrete RN Theorem",
        "topic": "Integration",
        "previous_results": [
            "Example 2.9",
            "Theorem 4.9"
        ],
        "statement": "Recall discrete measure theory on a countable $\\\\Omega$ as presented in Example 2.9. Prove Theorem 4.9 in this setting.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 4.11",
        "name": "Fatou's Lemma",
        "topic": "Integration",
        "previous_results": [
            "Lemma 3.18",
            "Theorem 4.6"
        ],
        "statement": "Let $(f_n)$ be a sequence of non-negative measurable functions on $(\\\\Omega, \\\\mathcal{F}, \\\\mu)$. Then\\n\\n$\\\\int \\\\liminf_{n\\\\to\\\\infty} f_n d\\\\mu \\\\leq \\\\liminf_{n\\\\to\\\\infty} \\\\int f_n d\\\\mu$.",
        "proof": "We write $\\\\liminf f_n$ for $\\\\liminf_{n\\\\to\\\\infty} f_n$. Recall that\\n\\n$\\\\liminf f_n = \\\\lim_{k\\\\to\\\\infty} g_k$, $g_k = \\\\inf_{n\\\\geq k} f_n$.\\n\\nIn particular, for $n \\\\geq k$, $f_n \\\\geq g_k$ and hence also $\\\\int f_n d\\\\mu \\\\geq \\\\int g_k d\\\\mu$. As this holds for all $n \\\\geq k$, we have\\n\\n$\\\\int g_k d\\\\mu \\\\leq \\\\inf_{n\\\\geq k} \\\\int f_n d\\\\mu$.\\n\\nSince $g_k \\\\uparrow \\\\liminf f_n$, as $k \\\\to \\\\infty$, we apply MCT to obtain the desired inequality:\\n\\n$\\\\int \\\\liminf f_n d\\\\mu = \\\\lim_{k\\\\to\\\\infty} \\\\int g_k d\\\\mu \\\\leq \\\\lim_{k\\\\to\\\\infty} \\\\inf_{n\\\\geq k} \\\\int f_n d\\\\mu = \\\\liminf_{n\\\\to\\\\infty} \\\\int f_n d\\\\mu$."
    },
    {
        "type": "lemma",
        "id": "Lemma 4.12",
        "name": "Reverse Fatou's Lemma",
        "topic": "Integration",
        "previous_results": [],
        "statement": "Let $(f_n)$ be a sequence of non-negative measurable functions on $(\\\\Omega, F, \\\\mu)$. Assume that there exists a function $g \\\\in L^1(\\\\Omega, F, \\\\mu)$ such that $f_n \\\\leq g$ for all $n$. Then $\\\\int \\\\lim\\\\sup_{n\\\\to\\\\infty} f_n d\\\\mu \\\\geq \\\\lim\\\\sup_{n\\\\to\\\\infty} \\\\int f_n d\\\\mu$.",
        "proof": "Apply Fatou to $h_n = g - f_n$. (Note that $\\\\int g d\\\\mu < \\\\infty$ is needed.)"
    },
    {
        "type": "theorem",
        "id": "Theorem 4.13",
        "name": "Dominated Convergence Theorem",
        "topic": "Integration",
        "previous_results": [
            "Lemma 4.12"
        ],
        "statement": "Let $(f_n)$ be a sequence of measurable functions on $(\\\\Omega, F, \\\\mu)$ with $f_n \\\\to f$ pointwise. Suppose that for some integrable function $g$, $|f_n| \\\\leq g$ for all $n$. Then $f$ is integrable and $\\\\int f_n d\\\\mu \\\\to \\\\int f d\\\\mu$ as $n \\\\to \\\\infty$.",
        "proof": "Taking limits we have $0 \\\\leq |f| \\\\leq g$ so that $f \\\\in L^1(\\\\Omega, F, \\\\mu)$ by comparison. Using $(15)$ and applying Lemma 4.12 to $h_n = |f_n - f| \\\\leq 2g$, we obtain\\n\\n$0 \\\\leq \\\\lim\\\\sup_{n\\\\to\\\\infty} \\\\left|\\\\int f_n d\\\\mu - \\\\int f d\\\\mu\\\\right| \\\\leq \\\\lim\\\\sup_{n\\\\to\\\\infty} \\\\int |f_n - f|d\\\\mu \\\\leq \\\\int \\\\lim\\\\sup_{n\\\\to\\\\infty} |f_n - f|d\\\\mu = \\\\int 0d\\\\mu = 0.$"
    },
    {
        "type": "lemma",
        "id": "Lemma 4.14",
        "name": "Scheff\u00e9's Lemma",
        "topic": "Integration",
        "previous_results": [
            "Theorem 4.13"
        ],
        "statement": "Suppose that $f_n, f \\\\in L^1(\\\\Omega, F, \\\\mu)$ converge pointwise, $f_n \\\\to f$ as $n \\\\to \\\\infty$. Then $\\\\int |f_n - f|d\\\\mu \\\\to 0 \\\\Leftrightarrow \\\\int |f_n|d\\\\mu \\\\to \\\\int |f|d\\\\mu$.",
        "proof": "The \\",
        "Rightarrow$\\": "mplication is trivial since $-|f_n - f| \\\\leq |f_n|-|f| \\\\leq |f_n - f|$ so we show the reverse. Suppose first that $f_n, f$ are positive and $\\\\int f_n d\\\\mu \\\\to \\\\int f d\\\\mu$. Since $(f_n - f)^- \\\\leq f$, DCT gives $\\\\int (f_n - f)^- d\\\\mu \\\\to 0$. For the positive part, we have\\n\\n$\\\\int (f_n - f)^+ d\\\\mu = \\\\int_{f_n\\\\geq f} (f_n - f)d\\\\mu = \\\\int f_n d\\\\mu - \\\\int f d\\\\mu - \\\\int_{f_n< f} (f_n - f)d\\\\mu$.\\n\\nThe first term converges to the second by assumption and the last one converges to zero by the previous argument. Together, we obtain the desired convergence $\\\\int |f_n - f|d\\\\mu \\\\to 0$.\\n\\nIn the general case, we have $\\\\int f^\\\\pm d\\\\mu \\\\leq \\\\lim\\\\inf \\\\int f^\\\\pm_n d\\\\mu$ by Fatou. By assumption,\\n\\n$\\\\int (f^+ + f^-)d\\\\mu = \\\\lim \\\\int (f^+_n + f^-_n)d\\\\mu$\\n\\nso that necessarily the sequences $f^+_n \\\\to f^+$ and $f^-_n \\\\to f^-$ satisfy the assumption of the Lemma and are positive, so the proof above applies and we conclude using $|f_n - f| \\\\leq |f^+_n - f^+| + |f^-_n - f^-|$."
    },
    {
        "type": "theorem",
        "id": "Theorem 4.15",
        "name": "Image Measure Integration",
        "topic": "Integration",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability measure, $X$ a random variable with values in a measurable space $(E, E)$ and $g$ a real-valued random variable on $(E, E)$. Let $Q = P \\\\circ X^{-1}$ be the image of $P$ via $X$. Then $g$ is $Q$-integrable if and only if $g \\\\circ X$ is $P$-integrable and then $\\\\int_E g(x)Q(dx) = \\\\int_\\\\Omega g(X(\\\\omega))P(d\\\\omega)$.",
        "proof": "$(17)$ holds by definition for $g = 1_A$ an indicator of an event $A \\\\in E$. By linearity it then holds for any simple function $g$. For a measurable $g \\\\geq 0$, let $g_n \\\\uparrow g$ be a sequence of simple functions increasing to $g$, say $g_n = \\\\sum_{k\\\\leq m_n} a_k1_{A_k}$ and note that\\n\\n$g_n(X(\\\\omega)) = \\\\sum_{k\\\\leq m_n} a_k1_{X(\\\\omega)\\\\in A_k} = \\\\sum_{k\\\\leq m_n} a_k1_{X^{-1}(A_k)}(\\\\omega)$\\n\\nare simple functions on $\\\\Omega$, $g_n \\\\circ X \\\\uparrow g \\\\circ X$. MCT then gives the required equality for $g$ with one integral being finite if and only if the other is. The general case follows with $g = g^+ - g^-$ and in particular $g$ is $Q$-integrable if and only if $g \\\\circ X$ is $P$-integrable."
    },
    {
        "type": "definition",
        "id": "Definition 4.16",
        "name": "Expectation",
        "topic": "Integration",
        "previous_results": [],
        "statement": "We say that $X$ admits a first moment, if $X$ is integrable, i.e., $X \\\\in L^1(\\\\Omega, \\\\mathcal{F}, P)$ or\\n\\n$E[|X|] = \\\\int_{\\\\Omega} |X(\\\\omega)|P(d\\\\omega) < \\\\infty$.\\n\\nThe expectation of a random variable $X$ defined on a probability space $(\\\\Omega, \\\\mathcal{F}, P)$ is\\n\\n$E[X] = \\\\int X dP = \\\\int_{\\\\Omega} X(\\\\omega)P(d\\\\omega)$.\\n\\nNote that this is well defined and finite if $E[|X|] < \\\\infty$ but otherwise may be either $+\\\\infty$ or undefined.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 4.18",
        "name": "Variance",
        "topic": "Integration",
        "previous_results": [],
        "statement": "Suppose $X$ admits a second moment, i.e., $E[X^2] < \\\\infty$. Then, the variance of $X$ is given by\\n\\n$\\\\operatorname{Var}(X) := E[(X - E[X])^2] = E[X^2] - (E[X])^2$\\n\\nand is also called the the second centred moment. The square root of the variance, $\\\\sqrt{\\\\operatorname{Var}(X)}$, is called the standard deviation of $X$.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 4.19",
        "name": "Standardized Moments",
        "topic": "Integration",
        "previous_results": [
            "Theorem 4.15"
        ],
        "statement": "The $n$th standardised moment of $X$, if well defined, is given by\\n\\n$E[Y^n] = E\\\\left[\\\\left(\\\\frac{X - E[X]}{\\\\sqrt{\\\\operatorname{Var}(X)}}\\\\right)^n\\\\right]$.\\n\\nThe third standardised moment is known as skewness of $X$ and the fourth one as kurtosis.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 4.20",
        "name": "Fubini/Tonelli",
        "topic": "Integration on a product space",
        "previous_results": [
            "Lemma 1.29",
            "Theorem 1.28"
        ],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, P)$ be the product of probability spaces $(\\\\Omega_i, \\\\mathcal{F}_i, P_i)$, $i = 1, 2$, and let $f = f(x, y)$ be a bounded measurable function on $(\\\\Omega, \\\\mathcal{F})$. The functions\\n\\n$x \\\\mapsto \\\\int_{\\\\Omega_2} f(x, y)P_2(dy), \\\\quad y \\\\mapsto \\\\int_{\\\\Omega_1} f(x, y)dP_1(dx)$\\n\\nare $\\\\mathcal{F}_1$- and $\\\\mathcal{F}_2$-measurable respectively.\\n\\nSuppose either (i) that $f$ is $P$-integrable on $\\\\Omega$ or (ii) that $f \\\\geq 0$. Then\\n\\n$\\\\int_{\\\\Omega} f dP = \\\\int_{\\\\Omega_2}\\\\left(\\\\int_{\\\\Omega_1} f(x, y)P_1(dx)\\\\right)P_2(dy) = \\\\int_{\\\\Omega_1}\\\\left(\\\\int_{\\\\Omega_2} f(x, y)P_2(dy)\\\\right)P_1(dx)$,\\n\\nwhere in case (ii) the common value may be $\\\\infty$.",
        "proof": "Both statements follow as immediate applications of the Monotone Class Theorem (Theorem 1.28) and we only outline the proof. First we check that the class $\\\\mathcal{H}$ of bounded functions which satisfy the statements satisfies the assumptions in Theorem 1.28. Then we observe that $f = 1_{A_1 \\\\times A_2} \\\\in \\\\mathcal{H}$ for all $A_1 \\\\in \\\\mathcal{F}_1$, $A_2 \\\\in \\\\mathcal{F}_2$. The statements then hold for all $\\\\mathcal{F}$ measurable bounded functions, including simple functions. The general case follows via the MCT."
    },
    {
        "type": "example",
        "id": "Example 4.21",
        "name": "Area Under Graph",
        "topic": "Integration on a product space",
        "previous_results": [
            "Theorem 4.20"
        ],
        "statement": "Let us consider an important example. Let $X$ be a positive random variable on a generic probability space $(\\\\Omega, \\\\mathcal{F}, P)$. We consider the product space $([0, \\\\infty) \\\\times \\\\Omega, \\\\mathcal{B}([0, \\\\infty)) \\\\times \\\\mathcal{F}, \\\\text{Leb} \\\\otimes P)$. Consider the area under the graph of $\\\\omega \\\\mapsto X(\\\\omega)$, namely\\n\\n$A := \\\\{(x, \\\\omega) : 0 \\\\leq x \\\\leq X(\\\\omega)\\\\}; \\\\quad f = 1_A$.\\n\\nThe partial integrals are given by\\n\\n$\\\\int_{\\\\Omega} f(x, \\\\omega)P(d\\\\omega) = P(X \\\\geq x)$\\n\\nand\\n\\n$\\\\int_{[0,\\\\infty)} f(x, \\\\omega)dx = X(\\\\omega)$,\\n\\nwhere $dx$ denotes $\\\\text{Leb}(dx)$ in the usual fashion. Fubini gives us\\n\\n$(P \\\\times \\\\text{Leb})(A) = \\\\int_{[0,\\\\infty)} P(X \\\\geq x)dx = E[X]$.",
        "proof": ""
    },
    {
        "type": "corollary",
        "id": "Corollary 4.22",
        "name": "Independence Characterization",
        "topic": "Integration",
        "previous_results": [
            "Corollary 3.9",
            "Theorem 3.8",
            "Theorem 4.15"
        ],
        "statement": "Let $X,Y$ be random variables on some probability space $(\\\\Omega, F, P)$. Then $X$ and $Y$ are independent if and only if for any positive measurable functions $f$, $g$ $$E[f(X)g(Y)] = E[f(X)]E[g(Y)].$$",
        "proof": "For the \\",
        "if\\": "irection, take $f = 1_{(-\\\\infty,r]}$, $g = 1_{(-\\\\infty,s]}$, $r, s \\\\in \\\\mathbb{R}$, and use Corollary 3.9. For the \\"
    },
    {
        "if\\": "irection, by Theorem 3.8, the joint distribution of $(X,Y)$ is the product measure, $\\\\mu_{(X,Y)} = \\\\mu_X \\\\otimes \\\\mu_Y$. The result then follows from Fubini's theorem since, by Theorem 4.15, $E[f(X)g(Y)] = \\\\int_{\\\\mathbb{R}^2} f(x)g(y)\\\\mu_{(X,Y)}(d(x, y))$."
    },
    {
        "type": "definition",
        "id": "Definition 4.23",
        "name": "Probability Kernel",
        "topic": "Integration",
        "previous_results": [],
        "statement": "A probability kernel on the product space $(\\\\mathbb{R} \\\\times \\\\Omega, \\\\mathcal{B}(\\\\mathbb{R}) \\\\times \\\\mathcal{F})$ is a family of probability measures $(P_x)_{x\\\\in\\\\mathbb{R}}$ on $\\\\mathcal{F}$ such that $\\\\mathbb{R} \\\\ni x \\\\mapsto P_x(A)$ is measurable for any $A \\\\in \\\\mathcal{F}$.\\n\\nIn words, a probability kernel is a measurable function in one argument and a probability measure in the other. A very special case is given by $P_x = P$ is independent of $x$. This is the case when constructing product measures.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 4.24",
        "name": "Generalised Fubini",
        "topic": "Integration",
        "previous_results": [],
        "statement": "Let $(P_x)_{x\\\\in\\\\mathbb{R}}$ be a probability kernel on $(\\\\mathbb{R} \\\\times \\\\Omega, \\\\mathcal{B}(\\\\mathbb{R}) \\\\times \\\\mathcal{F})$ and let $\\\\mu$ be a probability measure on $\\\\mathbb{R}$. Then there exists a unique probability measure $Q$ on $\\\\mathcal{B}(\\\\mathbb{R}) \\\\times \\\\mathcal{F}$ such that\\n\\n$$Q(E \\\\times A) = \\\\int_E P_x(A)\\\\mu(dx), \\\\quad E \\\\in \\\\mathcal{B}(\\\\mathbb{R}), A \\\\in \\\\mathcal{F}.$$\\n\\nFor a positive measurable function $f$ on $\\\\mathbb{R} \\\\times \\\\Omega$, the function $x \\\\mapsto \\\\int_{\\\\Omega} f(x, \\\\omega)P_x(d\\\\omega)$ is measurable and\\n\\n$$\\\\int_{\\\\mathbb{R}\\\\times\\\\Omega} f \\\\, dQ = \\\\int_{\\\\mathbb{R}} \\\\mu(dx) \\\\int_{\\\\Omega} f(x, \\\\omega)P_x(d\\\\omega).$$\\n\\nThe above equation remains true if $f$ is assumed $Q$-integrable on $\\\\mathbb{R} \\\\times \\\\Omega$ and then the function $\\\\omega \\\\mapsto f(x, \\\\omega)$ is $P_x$-integrable $\\\\mu$-a.s.",
        "proof": ""
    },
    {
        "type": "corollary",
        "id": "Corollary 4.25",
        "name": "Marginal Integration",
        "topic": "Integration",
        "previous_results": [
            "Theorem 4.24",
            "Theorem 4.15"
        ],
        "statement": "In the setup of Theorem 4.24, let $P$ be the marginal of $Q$ on $\\\\Omega$ and $X$ be a positive variable on $(\\\\Omega, \\\\mathcal{F})$. Then $x \\\\mapsto \\\\int_{\\\\Omega} X(\\\\omega)P_x(d\\\\omega)$ is measurable and\\n\\n$$\\\\int_{\\\\Omega} X(\\\\omega)P(d\\\\omega) = \\\\int_{\\\\mathbb{R}} \\\\mu(dx) \\\\int_{\\\\Omega} X(\\\\omega)P_x(d\\\\omega).$$",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 5.1",
        "name": "L^p Spaces",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "Let $p \\\\geq 0$. The space of all random variables $X$ such that $E[|X|^p] < \\\\infty$ is denoted $L^p$. In particular, $L^0$ is the space of all random variables. We also denote $L^\\\\infty$ the set of all random variables that are bounded.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 5.2",
        "name": "Modes of Convergence",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "Let $X_1, X_2, \\\\ldots$ and $X$ be random variables. We say that $X_n$ converges to $X$\\n\\n$\\\\bullet$ almost surely (written $X_n \\\\stackrel{a.s.}{\\\\rightarrow} X$ or $X_n \\\\rightarrow X$ a.s.) if\\n\\n$P[X_n \\\\rightarrow X] = P\\\\left[\\\\{\\\\omega : \\\\lim_{n\\\\rightarrow\\\\infty} X_n(\\\\omega) = X(\\\\omega)\\\\}\\\\right] = 1$.\\n\\n$\\\\bullet$ in probability (written $X_n \\\\stackrel{P}{\\\\rightarrow} X$) if, for every $\\\\varepsilon > 0$,\\n\\n$\\\\lim_{n\\\\rightarrow\\\\infty} P(|X_n - X| > \\\\varepsilon) = \\\\lim_{n\\\\rightarrow\\\\infty} P\\\\left[\\\\{\\\\omega : |X_n(\\\\omega) - X(\\\\omega)| > \\\\varepsilon\\\\}\\\\right] = 0$.\\n\\n$\\\\bullet$ in $L^p$ (or in $L_p$, or in $p$th moment), written $X_n \\\\stackrel{L_p}{\\\\rightarrow} X$, if all $X, X_n \\\\in L^p$, $n \\\\geq 1$ and $\\\\lim_{n\\\\rightarrow\\\\infty} E[|X_n - X|^p] = 0$.\\n\\n$\\\\bullet$ weakly in $L^1$ (or in the $\\\\sigma(L_1, L_\\\\infty)$ topology) if $X_n, X \\\\in L^1$, $n \\\\geq 1$ and\\n\\n$\\\\lim_{n\\\\rightarrow\\\\infty} E[X_nY] = E[XY], \\\\forall$ bounded r.v. $Y$.\\n\\n$\\\\bullet$ in distribution (or weakly) (written $X_n \\\\stackrel{d}{\\\\rightarrow} X$ or $X_n \\\\Rightarrow X$) if $\\\\lim_{n\\\\rightarrow\\\\infty} F_{X_n}(x) = F_X(x)$ for every $x \\\\in \\\\mathbb{R}$ at which $F_X$ is continuous and where $F_Y$ denotes the distribution function of $Y$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 5.3",
        "name": "Convergence a.s. not L1",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "On the probability space $\\\\Omega = [0, 1]$ with the Borel $\\\\sigma$-algebra and Lebesgue measure, consider the sequence of functions $f_n$ given by\\n\\n$f_n(x) = \\\\begin{cases} n(1 - nx) & 0 \\\\leq x \\\\leq 1/n, \\\\\\\\ 0 & \\\\text{otherwise}. \\\\end{cases}$\\n\\nThen $f_n \\\\rightarrow 0$ almost everywhere on $[0, 1]$ but $f_n \\\\not\\\\rightarrow 0$ in $L^1$. Thinking of each $f_n$ as a random variable, we have $f_n \\\\rightarrow 0$ almost surely but $f_n \\\\not\\\\rightarrow 0$ in $L^1$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 5.4",
        "name": "Convergence in Probability",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "To understand what's going on in (21) and (20), let's stick with $[0, 1]$ with the Borel sets and Lebesgue measure as our probability space. We define $(X_n)_{n\\\\geq1}$ as follows:\\n\\nfor each $n$ there is a unique pair of integers $(m, k)$ such that $n = 2^m + k$ and $0 \\\\leq k < 2^m$. We set\\n\\n$X_n(\\\\omega) = 1_{[k/2^m,(k+1)/2^m)}(\\\\omega)$.\\n\\nPictorially we have a 'moving blip' which travels repeatedly across $[0, 1]$ getting narrower at each pass.\\n\\nFor fixed $\\\\omega \\\\in (0, 1)$, $X_n(\\\\omega) = 1$ i.o., so $X_n \\\\not\\\\rightarrow 0$ a.s., but\\n\\n$P[X_n \\\\neq 0] = \\\\frac{1}{2^m} \\\\rightarrow 0$ as $n \\\\rightarrow \\\\infty$,\\n\\nso $X_n \\\\stackrel{P}{\\\\rightarrow} 0$. (Also, $E[|X_n - 0|] = 1/2^m \\\\rightarrow 0$, so $X_n \\\\stackrel{L^1}{\\\\rightarrow} 0$.) On the other hand, if we look at the $(X_{2^n})_{n\\\\geq1}$, we have\\n\\nand we see that $X_{2^n} \\\\stackrel{a.s.}{\\\\rightarrow} 0$.\\n\\nIt turns out that this is a general phenomenon.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 5.5",
        "name": "Convergence in Probability and a.s. Convergence",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Lemma 3.18"
        ],
        "statement": "Let $X_1, X_2, \\\\ldots$ and $X$ be random variables.\\n\\n(i) If $X_n \\\\stackrel{a.s.}{\\\\rightarrow} X$ then $X_n \\\\stackrel{P}{\\\\rightarrow} X$.\\n\\n(ii) If $X_n \\\\stackrel{P}{\\\\rightarrow} X$, then there exists a subsequence $(X_{n_k})_{k\\\\geq1}$ such that $X_{n_k} \\\\stackrel{a.s.}{\\\\rightarrow} X$ as $k \\\\rightarrow \\\\infty$.",
        "proof": "For $\\\\varepsilon > 0$ and $n \\\\in \\\\mathbb{N}$ let\\n\\n$A_{n,\\\\varepsilon} = \\\\{|X_n - X| > \\\\varepsilon\\\\}$.\\n\\n(i) Suppose $X_n \\\\stackrel{a.s.}{\\\\rightarrow} X$. Then for any $\\\\varepsilon > 0$ we have $P[A_{n,\\\\varepsilon} \\\\text{ i.o.}] = 0$. By Lemma 3.18 (Fatou's Lemma for sets), we have\\n\\n$0 = P[A_{n,\\\\varepsilon} \\\\text{ i.o.}] = P[\\\\limsup_{n\\\\rightarrow\\\\infty} A_{n,\\\\varepsilon}] \\\\geq \\\\limsup_{n\\\\rightarrow\\\\infty} P[A_{n,\\\\varepsilon}]$\\n\\nand in particular $P[A_{n,\\\\varepsilon}] \\\\rightarrow 0$, so $X_n \\\\stackrel{P}{\\\\rightarrow} X$.\\n\\n(ii) This is the more interesting direction. Suppose that $X_n \\\\stackrel{P}{\\\\rightarrow} X$. Then for each $k \\\\geq 1$ we have $P[A_{n,1/k}] \\\\rightarrow 0$, so there is some $n_k$ such that $P[A_{n_k,1/k}] < 1/k^2$ and $n_k > n_{k-1}$ for $k \\\\geq 2$. Setting $B_k = A_{n_k,1/k}$, we have\\n\\n$\\\\sum_{k=1}^{\\\\infty} P[B_k] \\\\leq \\\\sum_{k=1}^{\\\\infty} k^{-2} < \\\\infty$.\\n\\nHence, by BC1, $P[B_k \\\\text{ i.o.}] = 0$. But if only finitely many $B_k$ hold, then certainly $X_{n_k} \\\\rightarrow X$, so $X_{n_k} \\\\stackrel{a.s.}{\\\\rightarrow} X$."
    },
    {
        "type": "lemma",
        "id": "Lemma 5.6",
        "name": "Markov's inequality",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, P)$ be a probability space and $X$ a non-negative random variable. Then, for each $\\\\lambda > 0$\\n\\n$P[X \\\\geq \\\\lambda] \\\\leq \\\\frac{1}{\\\\lambda} E[X]$.",
        "proof": "Let $\\\\lambda > 0$. Then, for each $\\\\omega \\\\in \\\\Omega$ we have $X(\\\\omega) \\\\geq \\\\lambda 1_{\\\\{X\\\\geq\\\\lambda\\\\}}(\\\\omega)$. Hence,\\n\\n$E[X] \\\\geq E[\\\\lambda 1_{\\\\{X\\\\geq\\\\lambda\\\\}}] = \\\\lambda P[X \\\\geq \\\\lambda]$."
    },
    {
        "type": "corollary",
        "id": "Corollary 5.7",
        "name": "General Chebyshev's Inequality",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Lemma 5.6"
        ],
        "statement": "Let $X$ be a random variable taking values in a (measurable) set $A \\\\subseteq \\\\mathbb{R}$, and let $\\\\phi : A \\\\to [0, \\\\infty]$ be an increasing, measurable function. Then for any $\\\\lambda \\\\in A$ with $\\\\phi (\\\\lambda ) < \\\\infty$ we have\\n\\n$P[X \\\\geq \\\\lambda ] \\\\leq \\\\frac{E[\\\\phi (X)]}{\\\\phi (\\\\lambda )}$.",
        "proof": "We have\\n\\n$P[X \\\\geq \\\\lambda ] \\\\leq P[\\\\phi (X) \\\\geq \\\\phi (\\\\lambda )]$\\n\\n$\\\\leq \\\\frac{1}{\\\\phi (\\\\lambda )} E[\\\\phi (X)]$,\\n\\nby Markov's inequality."
    },
    {
        "type": "corollary",
        "id": "Corollary 5.8",
        "name": "Lp Convergence Implication",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Corollary 5.7"
        ],
        "statement": "For $p > 0$, convergence in $L^p$ implies convergence in probability.",
        "proof": "Recall that $X_n \\\\to X$ in $L^p$ if $E[|X_n - X|^p] \\\\to 0$ as $n \\\\to \\\\infty$. Now\\n\\n$P[|X_n - X| > \\\\varepsilon] = P[|X_n - X|^p > \\\\varepsilon^p] \\\\leq \\\\frac{1}{\\\\varepsilon^p} E[|X_n - X|^p] \\\\to 0$."
    },
    {
        "type": "corollary",
        "id": "Corollary 5.9",
        "name": "Weak Law of Large Numbers",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Corollary 5.7"
        ],
        "statement": "Let $(X_n)_{n\\\\geq1}$ be i.i.d. random variables with mean $m$ and variance $\\\\sigma^2 < \\\\infty$. Set\\n\\n$X^{(n)} = \\\\frac{1}{n}\\\\sum_{i=1}^{n} X_i$.\\n\\nThen $X^{(n)} \\\\to m$ in probability as $n \\\\to \\\\infty$.",
        "proof": "We have $E[X^{(n)}] = n^{-1} \\\\sum_{i=1}^{n} E[X_i] = m$ and, since the $X_n$ are independent,\\n\\n$\\\\text{Var}[X^{(n)}] = n^{-2}\\\\text{Var}\\\\left(\\\\sum_{i=1}^{n} X_i\\\\right) = n^{-2}\\\\sum_{i=1}^{n} \\\\text{Var}[X_i] = \\\\sigma^2/n$.\\n\\nHence, by Chebyshev's inequality,\\n\\n$P[|X^{(n)} - m| > \\\\varepsilon] \\\\leq \\\\frac{\\\\text{Var}[X^{(n)}]}{\\\\varepsilon^2} = \\\\frac{\\\\sigma^2}{\\\\varepsilon^2 n} \\\\to 0$."
    },
    {
        "type": "definition",
        "id": "Definition 5.10",
        "name": "Convex Function",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "Let $I \\\\subseteq \\\\mathbb{R}$ be a (bounded or unbouded) interval. A function $f : I \\\\to \\\\mathbb{R}$ is convex if for all $x, y \\\\in I$ and $t \\\\in [0, 1]$,\\n\\n$f (tx + (1 - t)y) \\\\leq t f (x) + (1 - t) f (y)$.\\n\\nImportant examples of convex functions include $x^2$, $e^x$, $e^{-x}$ and $|x|$ on $\\\\mathbb{R}$, and $1/x$ on $(0, \\\\infty)$. Note that a twice differentiable function $f$ is convex if and only if $f''(x) \\\\geq 0$ for all $x$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 5.11",
        "name": "Jensen's inequality",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "Let $f : I \\\\to \\\\mathbb{R}$ be a convex function on an interval $I \\\\subseteq \\\\mathbb{R}$. If $X$ is an integrable random variable taking values in $I$ then $E[ f (X)] \\\\geq f (E[X])$.",
        "proof": "If $E[X]$ is not an interior point of $I$ then it is an endpoint, and $X$ must be almost surely constant, so the inequality is trivial. Otherwise, setting $m = E[X]$ in the previous lemma we have\\n\\n$f (X) \\\\geq f (E[X]) + a(X - E[X])$.\\n\\nNow take expectations to recover\\n\\n$E[ f (X)] \\\\geq f (E[X])$\\n\\nas required.\\n\\nAs a byproduct of the proof, since a convex function is bounded from below by an affine function, $E[ f (X)]$ is well defined, possibly infinite."
    },
    {
        "type": "lemma",
        "id": "Lemma 5.12",
        "name": "Supporting Line",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "Suppose that $f : I \\\\to \\\\mathbb{R}$ is convex and let $m$ be an interior point of $I$. Then there exists $a \\\\in \\\\mathbb{R}$ such that $f (x) \\\\geq f (m) + a(x - m)$ for all $x \\\\in I$.",
        "proof": "Let $m$ be an interior point of $I$. For any $x < m$ and $y > m$ with $x, y \\\\in I$, by convexity we have\\n\\n$f (m) \\\\leq \\\\frac{y - m}{y - x} f (x) + \\\\frac{m - x}{y - x} f (y)$.\\n\\nRearranging (or, better, drawing a picture), this is equivalent to\\n\\n$\\\\frac{f (m) - f (x)}{m - x} \\\\leq \\\\frac{f (y) - f (m)}{y - m}$.\\n\\nIt follows that\\n\\n$\\\\sup_{x<m} \\\\frac{f (m) - f (x)}{m - x} \\\\leq \\\\inf_{y>m} \\\\frac{f (y) - f (m)}{y - m}$,\\n\\nso choosing $a$ so that\\n\\n$\\\\sup_{x<m} \\\\frac{f (m) - f (x)}{m - x} \\\\leq a \\\\leq \\\\inf_{y>m} \\\\frac{f (y) - f (m)}{y - m}$\\n\\n(if $f$ is differentiable at $m$ we can choose $a = f'(m)$) we have that $f (x) \\\\geq f (m) + a(x - m)$ for all $x \\\\in I$."
    },
    {
        "type": "lemma",
        "id": "Lemma 5.13",
        "name": "Lp Inclusion",
        "topic": "Lp spaces",
        "previous_results": [
            "Theorem 5.11"
        ],
        "statement": "Let $0 \\\\leq r \\\\leq p$. Suppose $X \\\\in L^p$. Then $X \\\\in L^r$ and $\\\\|X\\\\|_r \\\\leq \\\\|X\\\\|_p$. In particular, convergence in $L^p$ implies convergence in $L^r$.",
        "proof": "Let $X_k = |X| \\\\wedge k$ which is positive and bounded (and in particular integrable). Applying Jensen's inequality with the convex function $f(x) = x^{p/r}$ on $[0, \\\\infty)$ we get\\n\\n$\\\\|X_k\\\\|_p^r = (E[|X_k|^r])^{p/r} \\\\leq E[|X_k|^p] \\\\leq E[|X|^p] = \\\\|X\\\\|_p^p$.\\n\\nTaking limits and invoking the MCT gives the desired inequality. The implications for convergence in $L^p$ and $L^r$ is immediate."
    },
    {
        "type": "theorem",
        "id": "Theorem 5.14",
        "name": "H\u00f6lder and Minkowski",
        "topic": "Lp spaces",
        "previous_results": [
            "Theorem 5.11"
        ],
        "statement": "Let $p, q > 1$ be such that $\\\\frac{1}{p} + \\\\frac{1}{q} = 1$. Suppose $X,Y \\\\in L^p$ and $Z \\\\in L^q$. Then\\n\\n(H\u00f6lder's inequality) $E[|XZ|] \\\\leq \\\\|X\\\\|_p\\\\|Z\\\\|_q$,\\n\\n(Minkowski's inequality) $\\\\|X+Y\\\\|_p \\\\leq \\\\|X\\\\|_p + \\\\|Y\\\\|_p$.",
        "proof": "Proofs of these inequalities on $(\\\\mathbb{R}, \\\\mathcal{B}(\\\\mathbb{R}), \\\\text{Leb})$ were given in Part A Integration. Here we follow Williams and derive these from Jensen's inequality.\\n\\nIf $X = 0$ a.s. then there is nothing to show. Otherwise, define a new probability measure on $(\\\\Omega, \\\\mathcal{F})$ by $Q(A) = E[|X|^p1_A]/\\\\|X\\\\|_p^p$, as we did in \u00a74.2, and a random variable $Z := |Y|/|X|^{p-1}1_{|X|>0}$. Applying Jensen's inequality with $f(x) = x^q$, we have\\n\\n$(E[|XY|])^q = (E [Z|X|^p])^q = \\\\left(\\\\int Z dQ \\\\cdot \\\\|X\\\\|_p^p\\\\right)^q \\\\leq \\\\int Z^q dQ \\\\cdot \\\\|X\\\\|_p^{pq} = E[|Y|^q]\\\\|X\\\\|_p^q$,\\n\\nwhere we used $p + q = pq$. H\u00f6lder's inequality follows raising the sides to $1/q$.\\n\\nFor Minkowski's inequality note that $X + Y \\\\in L^p$ since it is a vector space and let $c = E[|X + Y|^p]^{1/q} = \\\\||X + Y|^{p-1}\\\\|_q$. Using first the triangular inequality on $\\\\mathbb{R}$, $|x + y| \\\\leq |x| + |y|$ and then H\u00f6lder's inequality we obtain\\n\\n$E[|X +Y|^p] \\\\leq E[|X| \\\\cdot |X +Y|^{p-1}] + E[|Y| \\\\cdot |X +Y|^{p-1}] \\\\leq \\\\|X\\\\|_p \\\\cdot c + \\\\|Y\\\\|_p \\\\cdot c$.\\n\\nDividing by $c$ gives the desired result since $1 - 1/q = 1/p$."
    },
    {
        "type": "lemma",
        "id": "Lemma 5.15",
        "name": "Tail Bound",
        "topic": "Lp spaces",
        "previous_results": [
            "Theorem 5.14"
        ],
        "statement": "Let $X,Y$ be two positive random variables such that $xP(X \\\\geq x) \\\\leq E[Y 1_{\\\\{X\\\\geq x\\\\}}], \\\\forall x > 0$. Then for $p > 1$ and $q = p/(p - 1)$, we have $\\\\|X\\\\|_p \\\\leq q\\\\|Y\\\\|_p$.",
        "proof": "This is only non-trivial if $Y \\\\in L^p$ so we suppose $E[Y^p] < \\\\infty$. First use Fubini, in analogy to Example 4.21, and the assumption, to show $E[X^p] \\\\leq qE[X^{p-1}Y]$. Then use H\u00f6lder's inequality assuming $X \\\\in L^p$. In general, use for $X_n = X \\\\wedge n$ and invoke MCT. The details are left as an exercise."
    },
    {
        "type": "theorem",
        "id": "Theorem 5.16",
        "name": "Completeness of Lp",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Theorem 5.5",
            "Monotone Convergence Theorem"
        ],
        "statement": "Let $p \\\\geq 1$. The vector space $L^p$ is complete, i.e., for any sequence $(X_n)_{n\\\\geq1} \\\\subseteq L^p$ such that\\n\\n$\\\\sup_{r,s\\\\geq n} \\\\|X_s - X_r\\\\|_p \\\\xrightarrow{n\\\\to\\\\infty} 0$\\n\\nthere exists $X \\\\in L^p$ such that $X_n \\\\to X$ in $L^p$.",
        "proof": "We proceed in analogy to the proof of (ii) in Theorem 5.5 above. Pick $k_n$ such that\\n\\n$\\\\sup_{r,s\\\\geq k_n} \\\\|X_s - X_r\\\\|_p \\\\leq 2^{-n}$,\\n\\nand in particular $E[|X_{k_n} - X_{k_{n+1}}|] \\\\leq \\\\|X_{k_n} - X_{k_{n+1}}\\\\|_p \\\\leq 2^{-n}$.\\n\\nPut $Y = \\\\sum_{n\\\\geq1} |X_{k_n} - X_{k_{n+1}}|$. By MCT we have $E[Y] < \\\\infty$ and in particular $Y < \\\\infty$ a.s. The series being absolutely convergent implies that $\\\\lim_{n\\\\to\\\\infty} X_{k_n}$ exists a.s. We define\\n\\n$X(\\\\omega) := \\\\limsup_{n\\\\to\\\\infty} X_{k_n}(\\\\omega), \\\\omega \\\\in \\\\Omega$\\n\\nso that $X$ is a random variable and $X_{k_n} \\\\to X$ a.s. For $n \\\\geq 1$ and $r > k_n$\\n\\n$E[|X_r - X_{k_m}|^p] = \\\\|X_r - X_{k_m}\\\\|_p^p \\\\leq 2^{-np}, m \\\\geq n.$\\n\\nTaking $m \\\\uparrow \\\\infty$ and using Fatou's lemma gives\\n\\n$E[|X_r - X|^p] \\\\leq 2^{-np}.$\\n\\nIt follows that $X \\\\in L^p$ and also $X_r \\\\to X$ in $L^p$, as required."
    },
    {
        "type": "definition",
        "id": "Definition 5.17",
        "name": "Uniform Integrability",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "A collection $C$ of random variables is called uniformly integrable (UI) if\\n\\n$\\\\lim_{K\\\\to\\\\infty} \\\\sup_{X\\\\in C} E[|X|1_{\\\\{|X|>K\\\\}}] = 0$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 5.18",
        "name": "Singleton UI",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Dominated Convergence Theorem"
        ],
        "statement": "For $X \\\\in L^1$ the decreasing function $E[|X|1_{\\\\{|X|>K\\\\}}]$ tends to $0$ as $K \\\\to \\\\infty$. Indeed, setting $f_n = |X|1_{\\\\{|X|>n\\\\}}$, the functions $f_n$ converge to $0$ a.s., and are dominated by the integrable function $|X|$. So by the DCT, $E[f_n] \\\\to 0$. It follows that the singleton family $\\\\{X\\\\}$ is uniformly integrable if and only if $X$ is integrable.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 5.19",
        "name": "Dominated UI",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Example 5.18"
        ],
        "statement": "If $C$ is a family of random variables with $|X| \\\\leq Y$ for all $X \\\\in C$ and $Y \\\\in L^1$ then $C$ is uniformly integrable (this is clear by the previous example). In particular, if we are in the setting of the DCT then UI holds. From the definition, clearly if $C$ contains a non-integrable random variable then $C$ is not UI. But UI of $C$ is strictly more than just all $X \\\\in C$ being integrable: we require the convergence $E[|X|1_{\\\\{|X|>K\\\\}}] \\\\to 0$, $K \\\\to \\\\infty$, to hold uniformly across $X \\\\in C$. As easy but very important example is provided by a sequence converging in $L^1$.",
        "proof": ""
    },
    {
        "type": "proposition",
        "id": "Proposition 5.22",
        "name": "UI Characterization",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Markov's inequality"
        ],
        "statement": "Let $C$ be a family of random variables. Then $C$ is UI if and only if\\n\\n(i) $\\\\sup_{X\\\\in C} E[|X|] < \\\\infty$\\n\\nand\\n\\n(ii) $\\\\sup_{X\\\\in C} \\\\sup_{A\\\\in F :P(A)\\\\leq\\\\delta} E[|X|1_A] \\\\xrightarrow{\\\\delta \\\\to 0} 0$.\\n\\n$0 \\\\leq (|X| - 2K)^+ \\\\leq |X|1_{\\\\{|X|>2K\\\\}} \\\\leq 2(|X| - K)^+$.",
        "proof": "Suppose $C$ is UI. By definition, there exists $K$ such that $E[|X|1_{\\\\{|X|>K\\\\}}] \\\\leq 1$, for all $X \\\\in C$. Thus (i) holds:\\n\\n$E[|X|] = E\\\\left[|X|1_{\\\\{|X|\\\\leq K\\\\}} + |X|1_{\\\\{|X|>K\\\\}}\\\\right] \\\\leq K + E\\\\left[|X|1_{\\\\{|X|>K\\\\}}\\\\right] \\\\leq K + 1, \\\\forall X \\\\in C$.\\n\\nFix $\\\\varepsilon > 0$ and choose $K$ such that\\n\\n$E[|X|1_{\\\\{|X|>K\\\\}}] < \\\\frac{1}{2}\\\\varepsilon, \\\\forall X \\\\in C$.\\n\\nSet $\\\\delta = \\\\varepsilon/(2K)$ and suppose that $P(A) < \\\\delta$. Then for any $X \\\\in C$,\\n\\n$E[|X|1_A] = E[|X|1_A1_{\\\\{|X|>K\\\\}}] + E[|X|1_A1_{\\\\{|X|\\\\leq K\\\\}}] \\\\leq E[|X|1_{\\\\{|X|>K\\\\}}] + E[K1_A] \\\\leq \\\\frac{1}{2}\\\\varepsilon + KP(A) < \\\\varepsilon$,\\n\\nso that (ii) holds.\\n\\nFor the converse, suppose (i) and (ii) hold. Let $\\\\varepsilon > 0$ be given. By (ii) there exists $\\\\delta > 0$ such that $P(A) < \\\\delta$ implies $E[|X|1_A] < \\\\varepsilon$ for all $X \\\\in C$. Let $M$ denote the value of the finite supremum in (i). For $K$ large enough, namely for $K > M/\\\\delta$, by Markov's inequality we have\\n\\n$P(|X| > K) \\\\leq \\\\frac{E[|X|]}{K} \\\\leq \\\\frac{M}{K} < \\\\delta, \\\\forall X \\\\in C$.\\n\\nPutting the two together we get the desired result:\\n$E\\\\left[|X|1_{\\\\{|X|>K\\\\}}\\\\right] < \\\\varepsilon$ for all $X \\\\in C$."
    },
    {
        "type": "lemma",
        "id": "Lemma 5.23",
        "name": "Bounded Convergence",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "Let $(X_n)$ be a sequence of random variables with $X_n \\\\to X$ in probability, and suppose that $|X|$ and all $|X_n|$ are bounded by the same real number $K$. Then $X_n \\\\to X$ in $L^1$.",
        "proof": "We use an idea which recurs again and again in this context: split by whether the relevant quantity is 'small' or 'large'. Specifically, fix $\\\\varepsilon > 0$. Let $A_n$ be the event $\\\\{|X_n - X| > \\\\varepsilon\\\\}$. Then\\n\\n$E[|X_n - X|] = E[|X_n - X|1_{A_n} + |X_n - X|1_{A_n^c}] \\\\leq E[|X_n|1_{A_n}] + E[|X|1_{A_n}] + \\\\varepsilon \\\\leq 2E[K1_{A_n}] + \\\\varepsilon = 2KP[A_n] + \\\\varepsilon.$\\n\\nSince $X_n$ converges to $X$ in probability, $P[A_n] \\\\to 0$, so the bound above is at most $2\\\\varepsilon$ if $n$ is large enough, and $E[|X_n - X|] \\\\to 0$ as required."
    },
    {
        "type": "theorem",
        "id": "Theorem 5.24",
        "name": "Vitali's Convergence Theorem",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Lemma 5.23",
            "Theorem 5.5",
            "Proposition 5.22",
            "Lemma 4.14"
        ],
        "statement": "Let $(X_n)$ be a sequence of integrable random variables which converges in probability to a random variable $X$. TFAE (The Following Are Equivalent):\\n\\n(i) the family $\\\\{X_n : n \\\\geq 1\\\\}$ is uniformly integrable,\\n\\n(ii) $X \\\\in L^1$ and $E[|X_n - X|] \\\\to 0$ as $n \\\\to \\\\infty$,\\n\\n(iii) $X \\\\in L^1$ and $E[|X_n|] \\\\to E[|X|] < \\\\infty$ as $n \\\\to \\\\infty$.",
        "proof": "Suppose $C = \\\\{X_n : n \\\\geq 1\\\\}$ is UI. We try to repeat the proof of Lemma 5.23, using the bound (22). Since $|X_n| \\\\to |X|$ in probability, by Theorem 5.5 there exists a subsequence $(X_{n_k})_{k\\\\geq 1}$ that converges to $X$ a.s. Fatou's Lemma gives\\n\\n$E[|X|] \\\\leq \\\\liminf_{k\\\\to\\\\infty} E[|X_{n_k}|] \\\\leq \\\\sup_n E[|X_n|],$\\n\\nwhich is finite by Proposition 5.22, i.e., $X$ is integrable. Now fix $\\\\varepsilon > 0$, and let $A_n = \\\\{|X_n - X| > \\\\varepsilon\\\\}$. As before,\\n\\n$E[|X_n - X|] = E[|X_n - X|1_{A_n}] + E[|X_n - X|1_{A_n^c}] \\\\leq E[|X_n|1_{A_n}] + E[|X|1_{A_n}] + \\\\varepsilon.$\\n\\nSince $X_n \\\\to X$ in probability we have $P[A_n] \\\\to 0$ as $n \\\\to \\\\infty$, so by Proposition 5.22 (ii)\\n\\n$E[|X_n|1_{A_n}] \\\\to 0 \\\\text{ as } n \\\\to \\\\infty.$\\n\\nSimilarly, since $\\\\{X\\\\}$ is uniformly integrable,\\n\\n$E[|X|1_{A_n}] \\\\to 0 \\\\text{ as } n \\\\to \\\\infty.$\\n\\nHence $E[|X_n - X|] \\\\leq 2\\\\varepsilon$ for $n$ large enough. Since $\\\\varepsilon > 0$ was arbitrary this proves (ii).\\n\\n(ii) $\\\\Rightarrow$ (iii) follows by $-|X_n - X| \\\\leq |X| - |X_n| \\\\leq |X - X_n|$ as in the proof of Lemma 4.14.\\n\\nIt remains to show (iii) $\\\\Rightarrow$ (i). Note that we can not repeat the arguments in the proof of Lemma 4.14 which relied on a.s. convergence to use the DCT. Instead, we use the bounded convergence result Lemma 5.23. To avoid clutter, let $Y_n = |X_n|$ and $Y = |X|$. Note that $Y_n, Y \\\\geq 0$, $Y_n \\\\stackrel{P}{\\\\to} Y$. We use Remark 5.21 to establish UI of $C$.\\n\\nSince $|(Y_n \\\\wedge K) - (Y \\\\wedge K)| \\\\leq |Y_n - Y|$, we have $Y_n \\\\wedge K \\\\stackrel{P}{\\\\to} Y \\\\wedge K$ and, by Lemma 5.23, $E[Y_n \\\\wedge K] \\\\to E[Y \\\\wedge K]$.\\n\\nRecalling that, by assumption, $E[Y_n] \\\\to E[Y]$ this gives\\n\\n$E[(Y_n - K)^+] = E[Y_n] - E[Y_n \\\\wedge K] \\\\stackrel{n\\\\to\\\\infty}{\\\\to} E[Y] - E[Y \\\\wedge K] = E[(Y - K)^+] < \\\\varepsilon,$\\n\\nwhere the last inequality holds for all $K$ large enough since $Y \\\\in L^1$. Hence there is an $n_0$ such that for $n \\\\geq n_0$,\\n\\n$E[(|X_n| - K)^+] = E[(Y_n - K)^+] < 2\\\\varepsilon.$\\n\\nThere are only finitely many $n < n_0$, so there exists $K' \\\\geq K$ such that such that\\n\\n$E[(|X_n| - K')^+] < 2\\\\varepsilon$\\n\\nfor all $n$, as required."
    },
    {
        "type": "theorem",
        "id": "Theorem 5.25",
        "name": "La Vall\u00e9e Poussin",
        "topic": "Complements and further results on integration",
        "previous_results": [],
        "statement": "Let $C \\\\subseteq L^1$. Then $C$ is UI if and only if there exists a positive increasing and convex $g : \\\\mathbb{R}^+ \\\\to \\\\mathbb{R}$ such that\\n\\n$\\\\lim_{x\\\\to\\\\infty} \\\\frac{g(x)}{x} = \\\\infty$\\n\\nand\\n\\n$\\\\sup_{X\\\\in C} E[g(|X|)] < \\\\infty$.\\n\\nOne example of $g$ which we shall meet later on is given by $g(x) = x \\\\log x$.",
        "proof": "TBC"
    },
    {
        "type": "theorem",
        "id": "Theorem 5.26",
        "name": "Dunford-Pettis",
        "topic": "Complements and further results on integration",
        "previous_results": [
            "Proposition 5.22",
            "Lemma 2.4",
            "Theorem 4.9",
            "Lemma 1.26"
        ],
        "statement": "Let $C \\\\subseteq L^1$. TFAE\\n\\n(i) $C$ is UI\\n\\n(ii) $C$ is relatively weakly compact (i.e., in the $\\\\sigma (L^1, L^\\\\infty)$ topology the closure is compact)\\n\\n(iii) every sequence of elements in $C$ contains a subsequence converging in $\\\\sigma (L^1, L^\\\\infty)$.",
        "proof": "Sketchy sketch of (i) $\\\\Rightarrow$ (ii). From (i) to (ii): consider $Q(A) := \\\\lim_U E[X1_A]$, where $U$ is an ultrafilter on $C$ and $A \\\\in \\\\mathcal{F}$. Part (i) in Proposition 5.22 shows the limit is well defined, while part (ii), together with Lemma 2.4, shows it is a measure. Using Theorem 4.9 we get $\\\\xi = \\\\frac{dQ}{dP}$, in particular $\\\\xi \\\\in L^1$, and show that $\\\\lim_U E[XY] = E[\\\\xi Y]$ for any $Y \\\\in L^\\\\infty$. This is easy for a simple $Y$ and then follows with a universal approximation argument in Lemma 1.26.\\n\\nThe reverse, from (ii) to (i), is more difficult. Equivalence between (ii) and (iii) follows from Eberlein-Smulian theorem, a difficult result which asserts that different types of compactness are equivalent for the weak topology on a Banach space."
    },
    {
        "type": "definition",
        "id": "Definition 6.2",
        "name": "Conditional Expectation",
        "topic": "Conditional Expectation",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, P)$ be a probability space and $X$ an integrable random variable. Let $\\\\mathcal{G} \\\\subseteq \\\\mathcal{F}$ be a $\\\\sigma$-algebra. We say that a random variable $Y$ is (a version of) the conditional expectation of $X$ given $\\\\mathcal{G}$ if $Y$ is integrable, $\\\\mathcal{G}$-measurable and $E[Y 1_G] = E[X1_G]$ for all $G \\\\in \\\\mathcal{G}$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 6.3",
        "name": "Existence and uniqueness of conditional expectation",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Definition 6.2"
        ],
        "statement": "Let $X$ be an integrable random variable on a probability space $(\\\\Omega, \\\\mathcal{F}, P)$ and $\\\\mathcal{G} \\\\subseteq \\\\mathcal{F}$ a $\\\\sigma$-algebra. The conditional expectation of $X$ given $\\\\mathcal{G}$ exists and is denoted $E[X | \\\\mathcal{G}]$. It is a.s. unique in the sense that if $Z$ is also the conditional expectation of $X$ given $\\\\mathcal{G}$ then $Z = E[X | \\\\mathcal{G}]$ a.s.",
        "proof": "Let $Y, Z$ be two conditional expectations of $X$ given $\\\\mathcal{G}$. Let $G := \\\\{Y > Z\\\\}$ and note that $G \\\\in \\\\mathcal{G}$ as $Y, Z$ are $\\\\mathcal{G}$-measurable. By definition, $E[Y 1_G] = E[X1_G] = E[Z1_G]$ so that $E[(Y - Z)1_G] = 0$. But $(Y - Z)1_G \\\\geq 0$ a.s. and hence $(Y - Z)1_G = 0$ a.s., i.e., $P(G) = 0$ since $Y - Z > 0$ on $G$. Swapping $Y$ and $Z$, we also have $P(Z > Y) = 0$ and hence $Y = Z$ a.s."
    },
    {
        "type": "proposition",
        "id": "Proposition 6.5",
        "name": "Conditional Expectation Properties",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Definition 6.2",
            "Theorem 6.3"
        ],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, P)$ be a probability space, $X$ and $Y$ integrable random variables, $\\\\mathcal{G} \\\\subseteq \\\\mathcal{F}$ a $\\\\sigma$-algebra and $a, b, c$ real numbers. Then\\n\\n(i) $E[E[X | \\\\mathcal{G}]] = E[X]$.\\n\\n(ii) $E[aX + bY + c | \\\\mathcal{G}] \\\\stackrel{a.s.}{=} aE[X | \\\\mathcal{G}] + bE[Y | \\\\mathcal{G}] + c$.\\n\\n(iii) If $X$ is $\\\\mathcal{G}$-measurable, then $E[X | \\\\mathcal{G}] \\\\stackrel{a.s.}{=} X$.\\n\\n(iv) $E[c | \\\\mathcal{G}] \\\\stackrel{a.s.}{=} c$.\\n\\n(v) $E[X | \\\\{ \\\\emptyset, \\\\Omega\\\\}] = E[X]$.\\n\\n(vi) If $\\\\sigma(X)$ and $\\\\mathcal{G}$ are independent then $E[X | \\\\mathcal{G}] = E[X]$ a.s.\\n\\n(vii) If $X \\\\leq Y$ a.s. then $E[X | \\\\mathcal{G}] \\\\leq E[Y | \\\\mathcal{G}]$ a.s. In particular, if $X \\\\geq 0$ a.s. then $E[X | \\\\mathcal{G}] \\\\geq 0$ a.s.\\n\\n(viii) $|E[X | \\\\mathcal{G}]| \\\\leq E[|X| | \\\\mathcal{G}]$ a.s.",
        "proof": "The proofs all follow from the requirement that $E[X | \\\\mathcal{G}]$ be $\\\\mathcal{G}$-measurable and the defining relation (24). We just do some examples.\\n\\n(i) Set $G = \\\\Omega$ in the defining relation.\\n\\n(ii) Clearly $Z = aE[X | \\\\mathcal{G}] + bE[Y | \\\\mathcal{G}]$ is $\\\\mathcal{G}$-measurable, so we just have to check the defining relation. But for $G \\\\in \\\\mathcal{G}$,\\n\\\\begin{align}\\n\\\\int_G Z \\\\, dP &= \\\\int_G (aE[X | \\\\mathcal{G}] + bE[Y | \\\\mathcal{G}]) \\\\, dP \\\\\\\\\\n&= a\\\\int_G E[X | \\\\mathcal{G}] \\\\, dP + b\\\\int_G E[Y | \\\\mathcal{G}] \\\\, dP \\\\\\\\\\n&= a\\\\int_G X \\\\, dP + b\\\\int_G Y \\\\, dP \\\\\\\\\\n&= \\\\int_G (aX + bY) \\\\, dP\\n\\\\end{align}\\nSo $Z$ is a version of $E[aX + bY | \\\\mathcal{G}]$, and equality a.s. follows from uniqueness.\\n\\n(v) The sub $\\\\sigma$-algebra is just $\\\\{\\\\emptyset, \\\\Omega\\\\}$ and so $E[X | \\\\{\\\\emptyset, \\\\Omega\\\\}]$ (in order to be measurable with respect to $\\\\{\\\\emptyset, \\\\Omega\\\\}$) must be constant. Now integrate over $\\\\Omega$ to identify that constant.\\n\\n(vi) Note that $E[X]$ is $\\\\mathcal{G}$-measurable and for $G \\\\in \\\\mathcal{G}$\\n\\\\begin{align}\\nE[E[X]1_G] = E[X]P[G] = E[X]E[1_G] = E[X1_G],\\n\\\\end{align}\\nso the defining relation holds, where in the last equality we used independence and Proposition 3.11.\\n\\n(vii) By linearity it is enough to show the 'in particular' part. Suppose $X \\\\geq 0$. If $P(E[X | \\\\mathcal{G}] < 0) > 0$ then $P(A) > 0$, where $A = \\\\{E[X | \\\\mathcal{G}] \\\\leq -1/n\\\\}$ for some $n > 0$. Since $A \\\\in \\\\mathcal{G}$, by (24), we have\\n\\\\begin{align}\\n0 \\\\leq E[X1_A] = E[E[X|\\\\mathcal{G}]1_A] \\\\leq -\\\\frac{P(A)}{n} < 0\\n\\\\end{align}\\na contradiction."
    },
    {
        "type": "proposition",
        "id": "Proposition 6.6",
        "name": "Conditional Convergence Theorems",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Definition 6.2",
            "Theorem 6.3",
            "Proposition 6.5"
        ],
        "statement": "Let $X_1, X_2, \\\\ldots$ and $X$ be integrable random variables on a probability space $(\\\\Omega, \\\\mathcal{F}, P)$, and let $\\\\mathcal{G} \\\\subseteq \\\\mathcal{F}$ be a $\\\\sigma$-algebra.\\n\\n1. cMCT: If $X_n \\\\geq 0$ for all $n$ and $X_n \\\\uparrow X$ as $n \\\\to \\\\infty$, then $E[X_n | \\\\mathcal{G}] \\\\uparrow E[X | \\\\mathcal{G}]$ a.s. as $n \\\\to \\\\infty$.\\n\\n2. cFatou: If $X_n \\\\geq 0$ for all $n$ then $E[\\\\liminf_{n\\\\to\\\\infty} X_n | \\\\mathcal{G}] \\\\leq \\\\liminf_{n\\\\to\\\\infty} E[X_n | \\\\mathcal{G}]$ a.s.\\n\\n3. cDCT: If $Y$ is an integrable random variable, $|X_n| \\\\leq Y$ for all $n$ and $X_n \\\\stackrel{a.s.}{\\\\to} X$, then $E[X_n | \\\\mathcal{G}] \\\\stackrel{a.s.}{\\\\to} E[X | \\\\mathcal{G}]$ as $n \\\\to \\\\infty$.",
        "proof": "The proofs all use the defining relation (24) to transfer statements about convergence of the conditional probabilities to our usual convergence theorems. We give details for cMCT and leave the rest as an exercise.\\n\\nLet $Y_n = E[X_n | \\\\mathcal{G}]$. By Proposition 6.5 (vii) we know that $Y_n \\\\geq 0$ a.s. and $A_n = \\\\{Y_n < Y_{n-1}\\\\} \\\\in \\\\mathcal{G}$ and is null, $P(A_n) = 0$. Let $Y := \\\\limsup_{n\\\\to\\\\infty} Y_n$ and $A = \\\\bigcup_{n\\\\geq 2} A_n$. Then $A \\\\in \\\\mathcal{G}$ is a null set, $P(A) = 0$, $Y$ is $\\\\mathcal{G}$-measurable and outside of $A$ it is an increasing limit of $Y_n$'s. For any $G \\\\in \\\\mathcal{G}$ we have\\n\\\\begin{align}\\nE[Y 1_G] &= E[Y 1_{G\\\\cap A^c}] \\\\stackrel{\\\\text{MCT}}{=} \\\\lim_{n\\\\to\\\\infty} E[Y_n1_{G\\\\cap A^c}] \\\\\\\\\\n&\\\\stackrel{(24)}{=} \\\\lim_{n\\\\to\\\\infty} E[X_n1_{G\\\\cap A^c}] \\\\stackrel{\\\\text{MCT}}{=} E[X1_{G\\\\cap A^c}] = E[X1_G].\\n\\\\end{align}\\nTaking $G = \\\\Omega$, $E[Y] = E[X] < \\\\infty$ and it follows that $Y$ is a version of $E[X | \\\\mathcal{G}]$, as required."
    },
    {
        "type": "lemma",
        "id": "Lemma 6.7",
        "name": "Taking Out Known",
        "topic": "Conditional Expectation",
        "previous_results": [],
        "statement": "Let $X$ and $Y$ be random variables on $(\\\\Omega, \\\\mathcal{F}, P)$ with $X$, $Y$ and $XY$ integrable. Let $\\\\mathcal{G} \\\\subseteq \\\\mathcal{F}$ be a $\\\\sigma$-algebra and suppose that $Y$ is $\\\\mathcal{G}$-measurable. Then $E[XY | \\\\mathcal{G}] \\\\stackrel{a.s.}{=} Y E[X | \\\\mathcal{G}]$.",
        "proof": "The function $Y E[X | \\\\mathcal{G}]$ is clearly $\\\\mathcal{G}$-measurable, so we must check that it satisfies the defining relation for $E[XY | \\\\mathcal{G}]$. We do this by a standard sequence of steps.\\n\\nFirst suppose that $X$ and $Y$ are non-negative.\\n\\nIf $Y = 1_A$ for some $A \\\\in \\\\mathcal{G}$, then for any $G \\\\in \\\\mathcal{G}$ we have $G \\\\cap A \\\\in \\\\mathcal{G}$ and so by the defining relation (24) for $E[X | \\\\mathcal{G}]$\\n\\n\\\\begin{align}\\n\\\\int_G Y E[X | \\\\mathcal{G}]dP &= \\\\int_{G\\\\cap A} E[X | \\\\mathcal{G}]dP \\\\\\\\\\n&= \\\\int_{G\\\\cap A} X dP \\\\\\\\\\n&= \\\\int_G Y X dP.\\n\\\\end{align}\\n\\nNow extend by linearity to simple positive $Y$s. Now suppose that $Y \\\\geq 0$ is $\\\\mathcal{G}$-measurable. Then there is a sequence $(Y_n)_{n\\\\geq 1}$ of simple $\\\\mathcal{G}$-measurable random variables with $Y_n \\\\uparrow Y$ as $n \\\\to \\\\infty$, it follows that $Y_n X \\\\uparrow Y X$ and we conclude by cMCT and a.s. uniqueness of the conditional expectation. Finally, for $X$, $Y$ not necessarily non-negative, write $XY = (X^+ - X^-)(Y^+ -Y^-)$ and use linearity of the integral."
    },
    {
        "type": "proposition",
        "id": "Proposition 6.8",
        "name": "Tower Property",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Proposition 6.5"
        ],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, P)$ be a probability space, $X$ an integrable random variable and $\\\\mathcal{F}_1, \\\\mathcal{F}_2$ $\\\\sigma$-algebras with $\\\\mathcal{F}_1 \\\\subseteq \\\\mathcal{F}_2 \\\\subseteq \\\\mathcal{F}$. Then $E\\\\left[E[X | \\\\mathcal{F}_2] \\\\mid \\\\mathcal{F}_1\\\\right] = E[X | \\\\mathcal{F}_1]$ a.s.\\n\\nIn other words, writing $X_i = E[X | \\\\mathcal{F}_i]$, $E[X_2 | \\\\mathcal{F}_1] = X_1$ a.s.",
        "proof": "The left-hand side is certainly $\\\\mathcal{F}_1$-measurable, so we need to check the defining relation for $E[X | \\\\mathcal{F}_1]$. Let $G \\\\in \\\\mathcal{F}_1$, noting that $G \\\\in \\\\mathcal{F}_2$. Applying the defining relation twice\\n\\n\\\\begin{align}\\n\\\\int_G E\\\\left[E[X | \\\\mathcal{F}_2] \\\\mid \\\\mathcal{F}_1\\\\right] dP &= \\\\int_G E[X | \\\\mathcal{F}_2]dP \\\\\\\\\\n&= \\\\int_G X dP.\\n\\\\end{align}\\n\\nThis extends (i) of Proposition 6.5 which (in the light of (v)) is just the case $\\\\mathcal{F}_1 = \\\\{\\\\emptyset, \\\\Omega\\\\}$."
    },
    {
        "type": "proposition",
        "id": "Proposition 6.9",
        "name": "Conditional Jensen's Inequality",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Lemma 5.12"
        ],
        "statement": "Suppose that $(\\\\Omega, \\\\mathcal{F}, P)$ is a probability space and that $X$ is an integrable random variable taking values in an open interval $I \\\\subseteq \\\\mathbb{R}$. Let $f: I \\\\to \\\\mathbb{R}$ be convex and let $\\\\mathcal{G}$ be a sub $\\\\sigma$-algebra of $\\\\mathcal{F}$. If $E[|f(X)|] < \\\\infty$ then $E[f(X) | \\\\mathcal{G}] \\\\geq f(E[X | \\\\mathcal{G}])$ a.s.",
        "proof": "A convex function $f$ on $I$ is continuous and can be represented as the supremum over a countable family of affine functions $\\\\{l_n : n \\\\geq 1\\\\}$ on $I$. Indeed, we may simply take $l_n$ to be supporting tangents from Lemma 5.12 over a dense sets of $m_n$ in $I$. We have\\n\\n$$l_n(E[X | \\\\mathcal{G}]) = E[l_n(X) | \\\\mathcal{G}] \\\\leq E[f(X)|\\\\mathcal{G}] \\\\quad \\\\text{a.s.}$$\\n\\nand since a countable union of null sets is null, we may assume that the above holds a.s. for all $n \\\\geq 1$ simultaneously. The result follows by taking the supremum in $n$."
    },
    {
        "type": "example",
        "id": "Example 6.10",
        "name": "Paley-Zygmund Inequality",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Proposition 6.5",
            "Proposition 6.9"
        ],
        "statement": "Suppose that $X$ is a non-trivial non-negative random variable: $X \\\\geq 0$ and $P(X > 0) > 0$. Then $P[X > 0] \\\\geq \\\\frac{E[X]^2}{E[X^2]}$.",
        "proof": "Let $A = \\\\{X > 0\\\\}$ and note that $E[X1_{A^c}] = 0$ and $E[X] = E[X1_A]$. In particular\\n\\n$$E[X | \\\\sigma(A)] = \\\\frac{E[X]}{P(A)}1_A.$$\\n\\nUsing Proposition 6.5 (i) and Proposition 6.9,\\n\\n\\\\begin{align}\\nE[X^2] &= E\\\\left(E[X^2 | \\\\sigma(A)]\\\\right) \\\\\\\\\\n&\\\\geq E\\\\left(E[X | \\\\sigma(A)]^2\\\\right) \\\\\\\\\\n&= \\\\frac{E[X]^2}{P(A)}.\\n\\\\end{align}\\n\\nRearranging gives the result."
    },
    {
        "type": "theorem",
        "id": "Theorem 6.11",
        "name": "Uniform Integrability",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Proposition 6.9",
            "Proposition 6.5"
        ],
        "statement": "Let $X$ be an integrable random variable on $(\\\\Omega, \\\\mathcal{F}, P)$ and $\\\\{\\\\mathcal{F}_\\\\alpha : \\\\alpha \\\\in I\\\\}$ a family of $\\\\sigma$-algebras with each $\\\\mathcal{F}_\\\\alpha \\\\subseteq \\\\mathcal{F}$. Then the family $\\\\{X_\\\\alpha : \\\\alpha \\\\in I\\\\}$ with $X_\\\\alpha = E[X | \\\\mathcal{F}_\\\\alpha]$ a.s. is uniformly integrable.",
        "proof": "Since $f(x) = |x|$ is convex, by the conditional form of Jensen's inequality (Proposition 6.9),\\n\\n$$|X_\\\\alpha| = |E[X | \\\\mathcal{F}_\\\\alpha]| \\\\leq E\\\\left(|X| | \\\\mathcal{F}_\\\\alpha\\\\right) \\\\text{ a.s.}$$\\n\\nand in particular $E[|X_\\\\alpha|] \\\\leq E[|X|]$. Using (25) and monotonicity of the conditional expectation (property (vii) in Proposition 6.5), we have\\n\\n$$E[|X_\\\\alpha|1_{\\\\{|X_\\\\alpha|>K\\\\}}] \\\\leq E\\\\left(E[|X| | \\\\mathcal{F}_\\\\alpha]1_{\\\\{|X_\\\\alpha|>K\\\\}}\\\\right) = E[|X|1_{\\\\{|X_\\\\alpha|>K\\\\}}],$$\\n\\nwhere for the equality we moved the indicator function inside the conditional expectation by Lemma 6.7 and then used property (i) in Proposition 6.5. Since $\\\\{X\\\\}$ is UI, applying Proposition 5.22, for a given $\\\\varepsilon > 0$ we can find $\\\\delta > 0$ such that $P(A) < \\\\delta$ implies $E[|X|1_A] < \\\\varepsilon$. Since\\n\\n$$P[|X_\\\\alpha| \\\\geq K] \\\\leq \\\\frac{E[|X_\\\\alpha|]}{K} \\\\leq \\\\frac{E[|X|]}{K},$$\\n\\nsetting $K = 2E[|X|]/\\\\delta < \\\\infty$, it follows that $E[|X_\\\\alpha|1_{\\\\{|X_\\\\alpha|>K\\\\}}] < \\\\varepsilon$ for every $\\\\alpha$."
    },
    {
        "type": "proposition",
        "id": "Proposition 6.5",
        "name": "Conditional Expectation Properties",
        "topic": "Conditional Expectation",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, P)$ be a probability space, $X$ and $Y$ integrable random variables, $\\\\mathcal{G} \\\\subseteq \\\\mathcal{F}$ a $\\\\sigma$-algebra and $a, b, c$ real numbers. Then\\n\\n(i) $E[E[X | \\\\mathcal{G}]] = E[X]$.\\n\\n(ii) $E[aX + bY + c | \\\\mathcal{G}] \\\\stackrel{a.s.}{=} aE[X | \\\\mathcal{G}] + bE[Y | \\\\mathcal{G}] + c$.\\n\\n(iii) If $X$ is $\\\\mathcal{G}$-measurable, then $E[X | \\\\mathcal{G}] \\\\stackrel{a.s.}{=} X$.\\n\\n(iv) $E[c | \\\\mathcal{G}] \\\\stackrel{a.s.}{=} c$.\\n\\n(v) $E[X | \\\\{ \\\\emptyset, \\\\Omega\\\\}] = E[X]$.\\n\\n(vi) If $\\\\sigma(X)$ and $\\\\mathcal{G}$ are independent then $E[X | \\\\mathcal{G}] = E[X]$ a.s.\\n\\n(vii) If $X \\\\geq Y$ a.s. then $E[X | \\\\mathcal{G}] \\\\geq E[Y | \\\\mathcal{G}]$ a.s. In particular, if $X \\\\geq 0$ a.s. then $E[X | \\\\mathcal{G}] \\\\geq 0$ a.s.\\n\\n(viii) $|E[X | \\\\mathcal{G}]| \\\\leq E[|X| | \\\\mathcal{G}]$ a.s.",
        "proof": "The proofs all follow from the requirement that $E[X | \\\\mathcal{G}]$ be $\\\\mathcal{G}$-measurable and the defining relation (24).\\nWe just do some examples.\\n\\n(i) Set $G = \\\\Omega$ in the defining relation.\\n(ii) Clearly $Z = aE[X | \\\\mathcal{G}] + bE[Y | \\\\mathcal{G}]$ is $\\\\mathcal{G}$-measurable, so we just have to check the defining relation. But\\n\\nfor $G \\\\in \\\\mathcal{G}$,\\n\\n$\\\\int_{G} Z dP = \\\\int_{G} (aE[X | \\\\mathcal{G}] + bE[Y | \\\\mathcal{G}])dP = a \\\\int_{G} E[X | \\\\mathcal{G}]dP + b \\\\int_{G} E[Y | \\\\mathcal{G}]dP$\\n\\n$= a \\\\int_{G} X dP + b \\\\int_{G} Y dP$\\n\\n$= \\\\int_{G} (aX + bY)dP$.\\n\\nSo $Z$ is a version of $E[aX + bY | \\\\mathcal{G}]$, and equality a.s. follows from uniqueness.\\n\\n(v) The sub $\\\\sigma$-algebra is just $\\\\{\\\\emptyset, \\\\Omega\\\\}$ and so $E[X | \\\\{\\\\emptyset, \\\\Omega\\\\}]$ (in order to be measurable with respect to $\\\\{\\\\emptyset, \\\\Omega\\\\}$)\\n\\nmust be constant. Now integrate over $\\\\Omega$ to identify that constant.\\n\\n(vi) Note that $E[X]$ is $\\\\mathcal{G}$-measurable and for $G \\\\in \\\\mathcal{G}$\\n\\n$E[E[X]1_G] = E[X]P[G] = E[X]E[1_G] = E[X1_G]$,\\n\\nso the defining relation holds, where in the last equality we used independence and Proposition 3.11.\\n\\n(vii) By linearity it is enough to show the 'in particular' part. Suppose $X \\\\geq 0$. If $P(E[X | \\\\mathcal{G}] < 0) > 0$ then\\n\\n$P(A) > 0$, where $A = \\\\{E[X | \\\\mathcal{G}] \\\\leq -1/n\\\\}$ for some $n > 0$. Since $A \\\\in \\\\mathcal{G}$, by (24), we have\\n\\n$0 \\\\leq E[X1_A] = E[E[X|\\\\mathcal{G}]1_A] \\\\leq -\\\\frac{P(A)}{n} < 0$\\n\\na contradiction."
    },
    {
        "type": "lemma",
        "id": "Lemma 6.7",
        "name": "Taking Out Known",
        "topic": "Conditional Expectation",
        "previous_results": [],
        "statement": "Let $X$ and $Y$ be random variables on $(\\\\Omega, \\\\mathcal{F}, P)$ with $X$, $Y$ and $XY$ integrable. Let $\\\\mathcal{G} \\\\subseteq \\\\mathcal{F}$ be a\\n$\\\\sigma$-algebra and suppose that $Y$ is $\\\\mathcal{G}$-measurable. Then\\n\\n$E[XY | \\\\mathcal{G}] \\\\stackrel{a.s.}{=} Y E[X | \\\\mathcal{G}]$.",
        "proof": "The function $Y E[X | \\\\mathcal{G}]$ is clearly $\\\\mathcal{G}$-measurable, so we must check that it satisfies the defining relation\\nfor $E[XY | \\\\mathcal{G}]$. We do this by a standard sequence of steps.\\n\\nFirst suppose that $X$ and $Y$ are non-negative.\\n\\nIf $Y = 1_A$ for some $A \\\\in \\\\mathcal{G}$, then for any $G \\\\in \\\\mathcal{G}$ we have\\n$G \\\\cap A \\\\in \\\\mathcal{G}$ and so by the defining relation (24) for $E[X | \\\\mathcal{G}]$\\n\\n$\\\\int_{G} Y E[X | \\\\mathcal{G}]dP = \\\\int_{G \\\\cap A} E[X | \\\\mathcal{G}]dP = \\\\int_{G \\\\cap A} X dP = \\\\int_{G} Y X dP$.\\n\\nNow extend by linearity to simple positive $Y$s. Now suppose that $Y \\\\geq 0$ is $\\\\mathcal{G}$-measurable. Then there is a\\nsequence $(Y_n)_{n \\\\geq 1}$ of simple $\\\\mathcal{G}$-measurable random variables with $Y_n \\\\uparrow Y$ as $n \\\\to \\\\infty$, it follows that $Y_n X \\\\uparrow Y X$\\nand we conclude by cMCT and a.s. uniqueness of the conditional expectation. Finally, for $X, Y$ not necessarily\\nnon-negative, write $XY = (X^+ - X^-)(Y^+ - Y^-)$ and use linearity of the integral."
    },
    {
        "type": "proposition",
        "id": "Proposition 6.6",
        "name": "Conditional Convergence Theorems",
        "topic": "Conditional Expectation",
        "previous_results": [],
        "statement": "Let $X_1, X_2, \\\\ldots$ and $X$ be integrable random variables on a probability space $(\\\\Omega, \\\\mathcal{F}, P)$, and let $\\\\mathcal{G} \\\\subseteq \\\\mathcal{F}$ be a $\\\\sigma$-algebra.\\n\\n1. cMCT: If $X_n \\\\geq 0$ for all $n$ and $X_n \\\\uparrow X$ as $n \\\\to \\\\infty$, then $E[X_n | \\\\mathcal{G}] \\\\uparrow E[X | \\\\mathcal{G}]$ a.s. as $n \\\\to \\\\infty$.\\n\\n2. cFatou: If $X_n \\\\geq 0$ for all $n$ then\\n\\n$E[\\\\liminf_{n\\\\to\\\\infty} X_n | \\\\mathcal{G}] \\\\geq \\\\liminf_{n\\\\to\\\\infty} E[X_n | \\\\mathcal{G}]$ a.s.\\n\\n3. cDCT: If $Y$ is an integrable random variable, $|X_n| \\\\leq Y$ for all $n$ and $X_n \\\\stackrel{a.s.}{\\\\to} X$, then\\n\\n$E[X_n | \\\\mathcal{G}] \\\\stackrel{a.s.}{\\\\to} E[X | \\\\mathcal{G}]$ as $n \\\\to \\\\infty$.",
        "proof": "The proofs all use the defining relation (24) to transfer statements about convergence of the conditional\\nprobabilities to our usual convergence theorems. We give details for cMCT and leave the rest as an exercise.\\n\\nLet $Y_n = E[X_n | \\\\mathcal{G}]$. By Proposition 6.5 (vii) we know that $Y_n \\\\geq 0$ a.s. and $A_n = \\\\{Y_n < Y_{n-1}\\\\} \\\\in \\\\mathcal{G}$ and is null,\\n$P(A_n) = 0$. Let $Y := \\\\limsup_{n\\\\to\\\\infty} Y_n$ and $A = \\\\cup_{n\\\\geq 2} A_n$. Then $A \\\\in \\\\mathcal{G}$ is a null set, $P(A) = 0$, $Y$ is $\\\\mathcal{G}$-measurable and\\noutside of $A$ it is an increasing limit of $Y_n$'s. For any $G \\\\in \\\\mathcal{G}$ we have\\n\\n$E[Y 1_G] = E[Y 1_{G\\\\cap A^c}] \\\\stackrel{\\\\text{MCT}}{=} \\\\lim_{n\\\\to\\\\infty} E[Y_n 1_{G\\\\cap A^c}]$\\n\\n$\\\\stackrel{(24)}{=} \\\\lim_{n\\\\to\\\\infty} E[X_n 1_{G\\\\cap A^c}] \\\\stackrel{\\\\text{MCT}}{=} E[X 1_{G\\\\cap A^c}] = E[X 1_G]$.\\n\\nTaking $G = \\\\Omega$, $E[Y] = E[X] < \\\\infty$ and it follows that $Y$ is a version of $E[X | \\\\mathcal{G}]$, as required."
    },
    {
        "type": "theorem",
        "id": "Theorem 6.15",
        "name": "Orthogonal Projection",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Theorem 5.14",
            "Theorem 5.16"
        ],
        "statement": "Let $K$ be a complete vector subspace of $L^2$. For any $X \\\\in L^2$ the infimum\\n\\n$\\\\inf_{Z\\\\in K} \\\\|X - Z\\\\|_2$\\n\\nis attained by some $Y \\\\in K$ and $(X-Y)$ is orthogonal to $Z$ for all $Z \\\\in K$ with $\\\\|Z\\\\|_2 > 0$.",
        "proof": "Let $(Y_n)_{n\\\\geq 1}$ be a sequence which attains the desired infimum, $\\\\|X-Y_n\\\\|_2 \\\\to \\\\Delta$. We argue\\nthat the sequence is Cauchy. Using (27), we have\\n\\n$\\\\|X-Y_r\\\\|_2^2 + \\\\|X-Y_s\\\\|_2^2 = 2\\\\|X - \\\\frac{1}{2}(Y_r+Y_s)\\\\|_2^2 + 2\\\\|\\\\frac{1}{2}(Y_r-Y_s)\\\\|_2^2$.\\n\\nSince $K$ is a vector space, $\\\\frac{1}{2}(Y_r \\\\pm Y_s) \\\\in K$ and in particular $\\\\|X - \\\\frac{1}{2}(Y_r + Y_s)\\\\|_2^2\\n\\\\geq \\\\Delta^2$. Optimality of $(Y_n)_{n\\\\geq 1}$ readily implies that\\n\\n$\\\\sup_{r,s\\\\geq n} \\\\|Y_r-Y_s\\\\|_2 \\\\stackrel{n\\\\to\\\\infty}{\\\\longrightarrow} 0$,\\n\\ni.e., $(Y_n)_{n\\\\geq 1}$ is Cauchy. Since $K$ is complete, there exists $Y \\\\in K$ with $\\\\|Y_n-Y\\\\|_2 \\\\to 0$ as $n \\\\to \\\\infty$. Minkowski's\\ninequality, see Theorem 5.14, then gives $\\\\|X - Y\\\\|_2 \\\\leq \\\\|X - Y_n\\\\|_2 + \\\\|Y - Y_n\\\\|_2$ and taking limits we see that\\n$\\\\|X-Y\\\\|_2 = \\\\Delta$ as required.\\n\\nNow, let $Z \\\\in K$ with $\\\\|Z\\\\|_2 > 0$ and note that $(Y + tZ) \\\\in K$ for all $t \\\\in \\\\mathbb{R}$. Using optimality of $Y$ we have\\n\\n$0 \\\\leq \\\\|X - (Y + tZ)\\\\|_2^2 - \\\\|X-Y\\\\|_2^2 = t^2\\\\|Z\\\\|_2^2 - 2tE[Z(X-Y)]$.\\n\\nTaking $t = E[Z(X-Y)]/\\\\|Z\\\\|_2^2$, yields\\n\\n$0 \\\\leq -\\\\frac{E[Z(X-Y)]^2}{\\\\|Z\\\\|_2^2}$\\n\\nwhich implies that $E[Z(X-Y)] = 0$ as desired."
    },
    {
        "type": "lemma",
        "id": "Lemma 6.13",
        "name": "Pythagoras' theorem",
        "topic": "Conditional Expectation",
        "previous_results": [],
        "statement": "If $X,Y \\\\in L^2$ are orthogonal then\\n\\n$\\\\|X+Y\\\\|_2^2 = \\\\|X\\\\|_2^2 + \\\\|Y\\\\|_2^2$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 6.16",
        "name": "Constant Projection",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Exercise 4.17"
        ],
        "statement": "Let $K$ be the vector space of random variables which are a.s. constant. Exercise 4.17 shows\\nthat the projection of $X$ on $K$ is given by $E[X]$.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 6.17",
        "name": "Conditional Independence",
        "topic": "Conditional Expectation",
        "previous_results": [],
        "statement": "Let $\\\\mathcal{F}_1, \\\\mathcal{F}_2, \\\\mathcal{F}_3$ be three sub-$\\\\sigma$-algebras of $\\\\mathcal{F}$. We say that $\\\\mathcal{F}_1$ and $\\\\mathcal{F}_3$ are conditionally\\nindependent given $\\\\mathcal{F}_2$ if\\n\\n$P(A_1 \\\\cap A_3 | \\\\mathcal{F}_2) = P(A_1 | \\\\mathcal{F}_2)P(A_3 | \\\\mathcal{F}_2)$, a.s.\\n\\nfor all $A_1 \\\\in \\\\mathcal{F}_1, A_3 \\\\in \\\\mathcal{F}_3$.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 6.17",
        "name": "Conditional Independence",
        "topic": "Conditional Expectation",
        "previous_results": [],
        "statement": "Let $F_1, F_2, F_3$ be three sub-$\\\\sigma$-algebras of $F$. We say that $F_1$ and $F_3$ are conditionally independent given $F_2$ if\\n\\n$P(A_1 \\\\cap A_3 | F_2) = P(A_1 | F_2)P(A_3 | F_2)$,\\n\\na.s.\\n\\nfor all $A_1 \\\\in F_1$, $A_3 \\\\in F_3$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 6.18",
        "name": "Conditional Independence Characterization",
        "topic": "Conditional Expectation",
        "previous_results": [
            "Proposition 6.5",
            "Proposition 6.8",
            "Lemma 6.7",
            "Lemma 1.12"
        ],
        "statement": "Let $F_1, F_2, F_3$ be three sub-$\\\\sigma$-algebras of $F$, and set $F_{12} = \\\\sigma(F_1, F_2)$. Then $F_1$ and $F_3$ are conditionally independent given $F_2$ if and only if\\n\\n$E[X_3 | F_{12}] = E[X_3 | F_2]$ a.s.\\n\\nfor all $F_3$-measurable integrable random variable $X_3$.",
        "proof": "To make various equalities clearer, we will refer to different properties in Proposition 6.5 simply by their list numbers (i), (ii) etc. We will refer to the tower property of conditional expectation, Proposition 6.8, as (t) and to the property of \\",
        "known\\": "Lemma 6.7, as (k).\\n\\n$(\\\\Rightarrow)$ We suppose $F_1$ and $F_3$ are conditionally independent given $F_2$. By definition, $E[X_3 | F_2]$ is $F_2$-measurable and hence also $F_{12}$-measurable. To establish the desired equality, we thus need to verify that\\n\\n$E[E[X_3 | F_2]1_A] = E[X_31_A]$\\n\\nfor all $A \\\\in F_{12}$. This holds for $A = \\\\Omega$ by (i). It is then easy to see that the family of sets $A$ for which the above holds is a $\\\\lambda$-system and thus it is enough, by Lemma 1.12, to verify it for the $\\\\pi$-system of sets $A = A_1 \\\\cap A_2$, $A_1 \\\\in F_1$, $A_1 \\\\in F_2$. We have\\n\\n$E[E[X_3 | F_2]1_{A_1}1_{A_2}] \\\\stackrel{(t)}{=} E[E[E[X_3 | F_2]1_{A_1}1_{A_2} | F_2]] \\\\stackrel{(k)}{=} E[E[1_{A_2}X_31_{A_1} | F_2]] = E[1_{A_2}E[X_31_{A_1} | F_2]] = E[1_{A_2}E[X_3 | F_2]E[1_{A_2} | F_2]] \\\\stackrel{(i)}{=} E[X_31_{A_1}1_{A_2}]$,\\n\\nas required, and where the third equality followed by the assumed conditional independence.\\n\\n$(\\\\Leftarrow)$ Suppose now that\\n\\n$E[X_3 | F_{12}] = E[X_3 | F_2]$ a.s.\\n\\nfor all $F_3$-measurable integrable random variable $X_3$. Then\\n\\n$E[X_1X_3 | F_2] \\\\stackrel{(t)}{=} E[E[X_1X_3 | F_{12}] | F_2] \\\\stackrel{(k)}{=} E[X_1E[X_3 | F_{12}] | F_2] = E[X_1E[X_3 | F_2] | F_2] \\\\stackrel{(k)}{=} E[X_1 | F_2]E[X_3 | F_2]$,\\n\\nas required and where the third equality followed by assumption."
    },
    {
        "type": "definition",
        "id": "Definition 7.1",
        "name": "Filtration",
        "topic": "Filtrations and stopping times",
        "previous_results": [],
        "statement": "A filtration on the probability space $(\\\\Omega, F, P)$ is a sequence $(F_n)_{n\\\\geq 0}$ of $\\\\sigma$-algebras $F_n \\\\subseteq F$ such that for all $n$, $F_n \\\\subseteq F_{n+1}$.\\n\\nWe then call $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ a filtered probability space.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 7.2",
        "name": "Adapted Process",
        "topic": "Filtrations and stopping times",
        "previous_results": [],
        "statement": "A stochastic process $(X_n)_{n\\\\geq 0}$ is a sequence of random variables defined on $(\\\\Omega, F, P)$. The process is integrable if each $X_n$ is integrable.\\n\\nWe say that $(X_n)_{n\\\\geq 0}$ is adapted to the filtration $(F_n)_{n\\\\geq 0}$ if, for each $n$, $X_n$ is $F_n$-measurable.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 7.3",
        "name": "Natural Filtration",
        "topic": "Filtrations and stopping times",
        "previous_results": [],
        "statement": "The natural filtration $(F^X_n)_{n\\\\geq 0}$ associated with a stochastic process $(X_n)_{n\\\\geq 0}$ on the probability space $(\\\\Omega, F, P)$ is defined by\\n\\n$F^X_n = \\\\sigma(X_0, X_1, \\\\ldots, X_n)$,\\n\\n$n \\\\geq 0$.\\n\\nA stochastic process $X$ is automatically adapted to the natural filtration it generates. It is also, by definition, the smallest filtration to which $X$ is adapted.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 7.4",
        "name": "Stopping Time",
        "topic": "Filtrations and stopping times",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and $(F_n)_{n\\\\geq 0}$ a filtration. A random variable $\\\\tau$ taking values in $\\\\mathbb{N} \\\\cup \\\\{\\\\infty\\\\} = \\\\{0, 1, 2, \\\\ldots, \\\\infty\\\\}$ is called a stopping time with respect to $(F_n)_{n\\\\geq 0}$ if $\\\\{\\\\tau = n\\\\} \\\\in F_n$ for all $n$.",
        "proof": ""
    },
    {
        "type": "proposition",
        "id": "Proposition 7.5",
        "name": "Properties of Stopping Times",
        "topic": "Filtrations and stopping times",
        "previous_results": [
            "Definition 7.4"
        ],
        "statement": "Let $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ be a filtered probability space and $\\\\tau, \\\\rho$ stopping times. Then\\n\\n(i) A deterministic time $t$, $t(\\\\omega) = n$ for all $\\\\omega \\\\in \\\\Omega$ is a stopping time;\\n\\n(ii) $\\\\tau \\\\wedge \\\\rho$ and $\\\\tau \\\\vee \\\\rho$ are stopping times.",
        "proof": "Exercise"
    },
    {
        "type": "proposition",
        "id": "Proposition 7.6",
        "name": "First Hitting Time",
        "topic": "Filtrations and stopping times",
        "previous_results": [
            "Definition 7.4"
        ],
        "statement": "Let $X = (X_n)_{n\\\\geq 0}$ be an adapted process on $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ and $B \\\\in \\\\mathcal{B}(\\\\mathbb{R})$. Then\\n\\n$h_B = \\\\inf\\\\{n \\\\geq 0 : X_n \\\\in B\\\\}$,\\n\\nthe first hitting time of $B$, is a stopping time.",
        "proof": "$\\\\{h_B \\\\leq n\\\\} = \\\\bigcup_{k=0}^n X_k^{-1}(B) \\\\in F_n$."
    },
    {
        "type": "definition",
        "id": "Definition 7.7",
        "name": "Information at Stopping Time",
        "topic": "Filtrations and stopping times",
        "previous_results": [
            "Definition 7.4"
        ],
        "statement": "Let $\\\\tau$ be a stopping time on $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$. The $\\\\sigma$-algebra of information at time $\\\\tau$ is defined as\\n\\n$F_\\\\tau = \\\\{A \\\\in F_\\\\infty : A \\\\cap \\\\{\\\\tau = n\\\\} \\\\in F_n \\\\forall n \\\\geq 0\\\\}$.",
        "proof": ""
    },
    {
        "type": "proposition",
        "id": "Proposition 7.8",
        "name": "Properties of Information",
        "topic": "Filtrations and stopping times",
        "previous_results": [
            "Definition 7.7"
        ],
        "statement": "Let $\\\\tau, \\\\rho$ be stopping times on $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$. Then\\n\\n(i) $F_\\\\tau$ defined in (31) is a $\\\\sigma$-algebra;\\n\\n(ii) if $\\\\tau \\\\leq \\\\rho$ then $F_\\\\tau \\\\subseteq F_\\\\rho$.",
        "proof": "Exercise."
    },
    {
        "type": "proposition",
        "id": "Proposition 7.9",
        "name": "Stopped process",
        "topic": "Filtrations and stopping times",
        "previous_results": [
            "Corollary 1.19"
        ],
        "statement": "Let $X = (X_n)_{n\\\\geq 0}$ be an adapted process on $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ and $\\\\tau$ a stopping time. Then $X^{\\\\tau} = (X_{\\\\tau\\\\wedge n})_{n\\\\geq 0}$ is a stochastic process, called the stopped process. $X^{\\\\tau}$ is adapted to the filtration $(F_{\\\\tau\\\\wedge n})_{n\\\\geq 0}$ and hence also to the filtration $(F_n)_{n\\\\geq 0}$.",
        "proof": "It suffices to show that if $\\\\rho$ is a finite stopping time then $X_\\\\rho$ is $F_\\\\rho$-measurable which follows from Corollary 1.19 and (31) since\\n\\n$$\\\\{X_\\\\rho \\\\leq x\\\\} \\\\cap \\\\{\\\\rho = n\\\\} = \\\\{X_n \\\\leq x\\\\} \\\\cap \\\\{\\\\rho = n\\\\} \\\\in F_n,$$\\n\\nfor all $n \\\\geq 0$."
    },
    {
        "type": "definition",
        "id": "Definition 8.1",
        "name": "Martingale, submartingales, supermartingale",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ be a filtered probability space. An integrable, $F_n$-adapted stochastic process $(X_n)_{n\\\\geq 0}$ is called\\n\\n(i) a martingale if for every $n \\\\geq 0$, $E[X_{n+1} | F_n] = X_n$ a.s.,\\n\\n(ii) a submartingale if for every $n \\\\geq 0$, $E[X_{n+1} | F_n] \\\\geq X_n$ a.s.,\\n\\n(iii) a supermartingale if for every $n \\\\geq 0$, $E[X_{n+1} | F_n] \\\\leq X_n$ a.s.",
        "proof": ""
    },
    {
        "type": "proposition",
        "id": "Proposition 8.2",
        "name": "Martingale properties",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space.\\n\\n(i) A stochastic process $(X_n)_{n\\\\geq 0}$ on $(\\\\Omega, F, P)$ is a submartingale w.r.t. the filtration $(F_n)_{n\\\\geq 0}$ if and only if $(-X_n)_{n\\\\geq 0}$ is a supermartingale. It is a martingale if and only if it is both a supermartingale and a submartingale.\\n\\n(ii) If $(X_n)_{n\\\\geq 0}$ is a submartingale w.r.t. some filtration $(F_n)_{n\\\\geq 0}$ and is adapted to another smaller filtration $(G_n)_{n\\\\geq 0}$, $G_n \\\\subseteq F_n$, $n \\\\geq 0$, then it is also a submartingale with respect to $(G_n)_{n\\\\geq 0}$. In particular, $X$ is a submartingale with respect to its natural filtration $(F^X_n)_{n\\\\geq 0}$.\\n\\n(iii) If $(X_n)_{n\\\\geq 0}$ is a submartingale and $n \\\\geq m$ then\\n\\n$E[X_n | F_m] \\\\geq X_m$ a.s.",
        "proof": "(i) is obvious.\\n\\nFor (ii) note that integrability is not affected by a change of filtration. Thus, by the tower property,\\n\\n$$E[X_{n+1} | G_n] = E\\\\left(E[X_{n+1} | F_n] | G_n\\\\right) \\\\geq E[X_n | G_n] = X_n \\\\text{ a.s.}$$\\n\\nBy definition, $X$ is adapted to its own natural filtration and it is the smallest such filtration so $F^X_n \\\\subseteq F_n$ and the above applies.\\n\\n(iii). We fix $m$ and prove the result by induction on $n$. The base case $n = m$ is obvious. For $n > m$ we have $F_m \\\\subseteq F_n$ and using the submartingale property\\n\\n$$E[X_{n+1} | F_m] = E\\\\left(E[X_{n+1} | F_n] | F_m\\\\right) \\\\geq E[X_n | F_m] \\\\text{ a.s.},$$\\n\\nso $E[X_n | F_m] \\\\geq X_m$ a.s. follows by induction."
    },
    {
        "type": "example",
        "id": "Example 8.3",
        "name": "Sums of independent variables",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Proposition 6.5"
        ],
        "statement": "Suppose that $Y_1,Y_2, \\\\ldots$ are independent integrable random variables on the probability space $(\\\\Omega, F, P)$ and that $E[Y_n] = 0$ for each $n$. Let $X_0 = 0$ and\\n\\n$X_n = \\\\sum_{k=1}^n Y_k, \\\\quad n \\\\geq 1$.\\n\\nThen $(X_n)_{n\\\\geq 0}$ is a martingale with respect to the natural filtration given by\\n\\n$F_n = \\\\sigma (X_0, X_1, \\\\ldots, X_n) = \\\\sigma (Y_1, \\\\ldots,Y_n)$.",
        "proof": "Indeed, $X$ is adapted and integrable and\\n\\n$$E[X_{n+1} | F_n] = E[X_n + Y_{n+1} | F_n] = E[X_n | F_n] + E[Y_{n+1} | F_n] = X_n + E[Y_{n+1}] = X_n, \\\\text{ a.s.}$$\\n\\nNote that we used basic properties of the conditional expectations, notably (iii) and (vi) in Proposition 6.5. These are very useful when dealing with martingales!"
    },
    {
        "type": "definition",
        "id": "Definition 8.4",
        "name": "Martingale Differences",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and $(F_n)_{n\\\\geq 0}$ a filtration. A sequence $(Y_n)_{n\\\\geq 1}$ of integrable random variables, adapted to the filtration $(F_n)_{n\\\\geq 1}$, is called a martingale difference sequence w.r.t. $(F_n)$ if $E[Y_{n+1} | F_n] = 0$ a.s. for all $n \\\\geq 0$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 8.5",
        "name": "Product Martingale",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and let $(Z_n)_{n\\\\geq 1}$ be a sequence of independent integrable random variables with $E[Z_n] = 1$ for all $n$. Define $X_n = \\\\prod_{i=1}^{n} Z_i$ for $n \\\\geq 0$, so $X_0 = 1$. Then $(X_n)_{n\\\\geq 0}$ is a martingale w.r.t. its natural filtration.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 8.6",
        "name": "Exponential Martingale",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "Suppose that $Y_1, Y_2, \\\\ldots$ are i.i.d. random variables on $(\\\\Omega, F, P)$ with $E[\\\\exp(Y_1)] = c < \\\\infty$. Then $X_n = \\\\exp(Y_1 + \\\\ldots + Y_n) c^{-n}$ is a martingale with respect to the natural filtration.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 8.7",
        "name": "Conditional Expectation Martingale",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 6.11",
            "Theorem 5.24"
        ],
        "statement": "Let $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ be a filtered probability space and $X$ an integrable random variable. Then $X_n = E[X | F_n]$, $n \\\\geq 0$, is an $(F_n)_{n\\\\geq 0}$-martingale.",
        "proof": "Indeed, $X_n$ is certainly $F_n$-measurable and integrable and, by the tower property of conditional expectation, $E[X_{n+1} | F_n] = E[E[X | F_{n+1}] | F_n] = E[X | F_n] = X_n$ a.s. We note also that $X$ is automatically UI by Theorem 6.11 and if $X_n \\\\to X$ in probability then it already converges in $L^1$ by Theorem 5.24."
    },
    {
        "type": "example",
        "id": "Example 8.8",
        "name": "Increasing Process Submartingale",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "An integrable adapted process $X$ which is increasing, $X_n \\\\geq X_{n-1}$ a.s., $n \\\\geq 1$, is a submartingale.",
        "proof": ""
    },
    {
        "type": "proposition",
        "id": "Proposition 8.9",
        "name": "Convex Function Submartingale",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Jensen's inequality"
        ],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space. Suppose that $(X_n)_{n\\\\geq 0}$ is a martingale with respect to the filtration $(F_n)_{n\\\\geq 0}$. Let $f$ be a convex function on $\\\\mathbb{R}$. If $f(X_n)$ is an integrable random variable for each $n \\\\geq 0$, then $(f(X_n))_{n\\\\geq 0}$ is a submartingale w.r.t $(F_n)_{n\\\\geq 0}$.",
        "proof": "Since $X_n$ is $F_n$-measurable, so is $f(X_n)$. By Jensen's inequality for conditional expectations and the martingale property of $(X_n)$, \\n\\n$E[f(X_{n+1}) | F_n] \\\\geq f(E[X_{n+1} | F_n]) = f(X_n) \\\\quad a.s.$"
    },
    {
        "type": "corollary",
        "id": "Corollary 8.10",
        "name": "Common Submartingales",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Proposition 8.9"
        ],
        "statement": "If $(X_n)_{n\\\\geq 0}$ is a martingale w.r.t. $(F_n)_{n\\\\geq 0}$ and $K \\\\in \\\\mathbb{R}$ then (subject to integrability) $(|X_n|)_{n\\\\geq 0}$, $(X^2_n)_{n\\\\geq 0}$, $(e^{X_n})_{n\\\\geq 0}$, $(e^{-X_n})_{n\\\\geq 0}$, $(\\\\max(X_n, K))_{n\\\\geq 0}$ are all submartingales w.r.t. $(F_n)_{n\\\\geq 0}$.",
        "proof": "This follows directly from Proposition 8.9 by applying it to the convex functions $f(x) = |x|$, $f(x) = x^2$, $f(x) = e^x$, $f(x) = e^{-x}$, and $f(x) = \\\\max(x, K)$, respectively. Each of these functions is convex on $\\\\mathbb{R}$, so the result follows immediately."
    },
    {
        "type": "definition",
        "id": "Definition 8.11",
        "name": "Predictable process",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, P)$ be a probability space and $(F_n)_{n\\\\geq 0}$ a filtration. A sequence $(V_n)_{n\\\\geq 1}$ of random variables is predictable with respect to $(F_n)_{n\\\\geq 0}$ if $V_n$ is $F_{n-1}$-measurable for all $n \\\\geq 1$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 8.12",
        "name": "Discrete stochastic integral",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Lemma 6.7"
        ],
        "statement": "Let $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ be a filtered probability space and $(Y_n)_{n\\\\geq 0}$ a martingale. Suppose that $(V_n)_{n\\\\geq 1}$ is predictable w.r.t. $(F_n)$, and let $X_0 = 0$ and\\n\\n$X_n = \\\\sum_{k=1}^{n} V_k(Y_k - Y_{k-1}), \\\\quad n \\\\geq 1$.\\n\\nIf each $X_n$ is integrable then $(X_n)_{n\\\\geq 0}$ is a martingale w.r.t. $(F_n)$.\\n\\nAn important special case when all $X_n$ are automatically integrable is when all $V_n$ are bounded. The sequence $(X_n)_{n\\\\geq 0}$ is called a martingale transform and is often denoted $((V \\\\circ Y)_n)_{n\\\\geq 0}$.",
        "proof": "For $k \\\\leq n$, all $Y_k$ and $V_k$ are $F_n$-measurable, so $X_n$ is $F_n$-measurable. Also,\\n\\n$E[X_{n+1} - X_n | F_n] \\\\stackrel{a.s.}{=} E[V_{n+1}(Y_{n+1} - Y_n) | F_n] \\\\stackrel{a.s.}{=} V_{n+1}E[Y_{n+1} - Y_n | F_n] = 0 \\\\quad a.s.$\\n\\nwhere the second equality follows from taking out what is known (Lemma 6.7), since $V_{n+1}$ is $F_n$-measurable by the predictability assumption. The final equality follows from the martingale property of $(Y_n)$. Therefore, $(X_n)_{n\\\\geq 0}$ is a martingale."
    },
    {
        "type": "proposition",
        "id": "Proposition 8.13",
        "name": "Supermartingale Transform",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.12"
        ],
        "statement": "Let $(Y_n)_{n\\\\geq 0}$ be a supermartingale on a filtered probability space $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$, $(V_n)_{n\\\\geq 1}$ a non-negative predictable process and let $X_0 = 0$ and\\n\\n$X_n = \\\\sum_{k=1}^{n} V_k(Y_k - Y_{k-1}), \\\\quad n \\\\geq 1$.\\n\\nIf $X_n$ is integrable, $n \\\\geq 0$, then $X$ is a supermartingale.",
        "proof": "The proof follows the same structure as the proof of Theorem 8.12. For $k \\\\leq n$, all $Y_k$ and $V_k$ are $F_n$-measurable, so $X_n$ is $F_n$-measurable. For the supermartingale property, we have:\\n\\n$E[X_{n+1} - X_n | F_n] = E[V_{n+1}(Y_{n+1} - Y_n) | F_n] = V_{n+1}E[Y_{n+1} - Y_n | F_n] \\\\leq 0 \\\\quad a.s.$\\n\\nwhere the second equality follows from taking out what is known (Lemma 6.7), since $V_{n+1}$ is $F_n$-measurable by the predictability assumption. The inequality follows from the supermartingale property of $(Y_n)$ and the non-negativity of $V_{n+1}$. Therefore, $(X_n)_{n\\\\geq 0}$ is a supermartingale."
    },
    {
        "type": "theorem",
        "id": "Theorem 8.15",
        "name": "Doob's Decomposition Theorem",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ be a filtered probability space and $X = (X_n)_{n\\\\geq 0}$ an integrable adapted process. Then\\n\\n(i) $(X_n)_{n\\\\geq 0}$ has a Doob decomposition\\n\\n$X_n = X_0 + M_n + A_n$\\n\\nwhere $(M_n)_{n\\\\geq 0}$ is a martingale w.r.t. $(F_n)_{n\\\\geq 0}$, $(A_n)_{n\\\\geq 1}$ is predictable w.r.t. $(F_n)$, and $M_0 = 0 = A_0$.\\n\\n(ii) Doob decompositions are essentially unique: if $X_n = X_0 + \\\\tilde{M}_n + \\\\tilde{A}_n$ is another Doob decomposition of $(X_n)_{n\\\\geq 0}$ then\\n\\n$P\\\\left(M_n = \\\\tilde{M}_n, A_n = \\\\tilde{A}_n \\\\text{ for all } n \\\\geq 0\\\\right) = 1$.\\n\\n(iii) $(X_n)_{n\\\\geq 0}$ is a submartingale if and only if $(A_n)_{n\\\\geq 0}$ in (32) is an increasing process (i.e., $A_{n+1} \\\\geq A_n$ a.s. for all $n$) and a supermartingale if and only if $(A_n)_{n\\\\geq 0}$ is a decreasing process.",
        "proof": "\\\\begin{proof}\\n(i). Let\\n\\n$A_n = \\\\sum_{k=1}^n E[X_k - X_{k-1} | F_{k-1}] = \\\\sum_{k=1}^n \\\\left(E[X_k | F_{k-1}] - X_{k-1}\\\\right)$\\n\\nand\\n\\n$M_n = \\\\sum_{k=1}^n \\\\left(X_k - E[X_k | F_{k-1}]\\\\right)$.\\n\\nThen $M_n + A_n = \\\\sum_{k=1}^n(X_k - X_{k-1}) = X_n - X_0$, so (32) holds. The $k$th summand in $A_n$ is $F_{k-1}$-measurable, so $A_n$ is $F_{n-1}$-measurable, i.e., $A$ is a predictable process. Also, as $X$ is integrable so are $(M_n)_{n\\\\geq 0}$ and $(A_n)_{n\\\\geq 0}$. Finally, since\\n\\n$E[M_{n+1} - M_n | F_n] = E\\\\left[X_{n+1} - E[X_{n+1} | F_n] \\\\mid F_n\\\\right] = 0 \\\\quad \\\\text{a.s.}$,\\n\\nthe process $(M_n)_{n\\\\geq 0}$ is a martingale.\\n\\n(ii) For uniqueness, note that in any Doob decomposition, by predictability we have\\n\\n$A_{n+1} - A_n = E[A_{n+1} - A_n | F_n]$\\n\\n$= E[(X_{n+1} - X_n) - (M_{n+1} - M_n) | F_n]$\\n$= E[X_{n+1} - X_n | F_n] \\\\quad \\\\text{a.s.}$,\\n\\nwhich combined with $A_0 = 0$ proves uniqueness of $(A_n)$. Since $M_n = X_n - X_0 - A_n$, uniqueness of $(M_n)$ follows.\\n\\n(iii) Just note that\\n\\n$E[X_{n+1} | F_n] - X_n = E[X_{n+1} - X_n | F_n] = A_{n+1} - A_n \\\\quad \\\\text{a.s.}$\\n\\nas shown above.\\n\\\\end{proof}"
    },
    {
        "type": "theorem",
        "id": "Theorem 8.16",
        "name": "Stopped Martingale",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.12",
            "Proposition 8.2"
        ],
        "statement": "Let $X$ be a martingale on a filtered probability space $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ and $\\\\tau$ be a finite stopping time. Then $X^\\\\tau = (X_{\\\\tau\\\\wedge n} : n \\\\geq 0)$ is a martingale with respect to $(F_n)_{n\\\\geq 0}$ and with respect to $(F_{\\\\tau\\\\wedge n})_{n\\\\geq 0}$.",
        "proof": "\\\\begin{proof}\\nNote that $\\\\{\\\\tau \\\\geq k\\\\} = \\\\{\\\\tau > k - 1\\\\} \\\\in F_{k-1}$ so that $V_k = 1_{k\\\\leq\\\\tau}$, $k \\\\geq 1$, is predictable. We have\\n\\n$X_0 + \\\\sum_{k=1}^n V_k(X_k - X_{k-1}) = X_0 + \\\\sum_{k=1}^{\\\\tau\\\\wedge n} (X_k - X_{k-1}) = X_{\\\\tau\\\\wedge n}$\\n\\nand the result follows by Theorem 8.12 and Proposition 8.2.\\n\\\\end{proof}"
    },
    {
        "type": "theorem",
        "id": "Theorem 8.17",
        "name": "Optional Sampling Theorem",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.16",
            "Proposition 7.9",
            "Proposition 8.13",
            "Theorem 8.12"
        ],
        "statement": "Let $X$ be a martingale on a filtered probability space $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ and $\\\\tau, \\\\rho$ be two bounded stopping times, $\\\\tau \\\\leq \\\\rho$. Then\\n\\n$E[X_\\\\rho | F_\\\\tau] = X_\\\\tau \\\\text{ a.s.}$\\n\\nand in particular $E[X_\\\\rho] = E[X_\\\\tau] = E[X_0]$.\\nSimilarly, if $X$ is a sub- (resp. super-) martingale then $E[X_\\\\rho | F_\\\\tau] \\\\geq X_\\\\tau$ (resp. $E[X_\\\\rho | F_\\\\tau] \\\\leq X_\\\\tau$) a.s.",
        "proof": "\\\\begin{proof}\\nConsider first the case when $\\\\rho = n$ is a constant. Then (33) follows by simply checking the defining relationship for the conditional expectation since for any $A \\\\in F_\\\\tau$ we have\\n\\n$E[X_n1_A] = \\\\sum_{k=0}^n E[X_n1_A1_{\\\\tau=k}] = \\\\sum_{k=0}^n E[X_k1_A1_{\\\\tau=k}] = \\\\sum_{k=0}^n E[X_\\\\tau 1_A1_{\\\\tau=k}] = E[X_\\\\tau 1_A]$,\\n\\nwhere the first equality follows since $\\\\tau \\\\leq n$ and the second by definition of $F_\\\\tau$ in (31) and since $X$ is a martingale.\\n\\nConsider now the general case. The process $Y_n = X_{\\\\rho\\\\wedge n} - X_{\\\\tau\\\\wedge n}$, $n \\\\geq 0$, is a martingale as a difference of two martingales, by Theorem 8.16. It follows that:\\n\\n$0 = Y_{\\\\tau\\\\wedge n} = E[Y_n | F_{\\\\tau\\\\wedge n}] = E[X_{\\\\rho\\\\wedge n} | F_{\\\\tau\\\\wedge n}] - X_{\\\\tau\\\\wedge n} \\\\quad \\\\text{a.s.}$\\n\\nwhere the first equality is by definition, the second follows from the case of a deterministic $\\\\rho$ shown above and the third since $X_{\\\\tau\\\\wedge n}$ is $F_{\\\\tau\\\\wedge n}$-measurable by Proposition 7.9. It suffices to take $n$ large enough so that $n \\\\geq \\\\rho \\\\geq \\\\tau$.\\n\\nThe proof for sub-/super- martingales is the same but uses Proposition 8.13 instead of Theorem 8.12.\\n\\\\end{proof}"
    },
    {
        "type": "example",
        "id": "Example 8.18",
        "name": "Unbounded Stopping Time",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Proposition 7.6"
        ],
        "statement": "Let $(Y_k)_{k\\\\geq 1}$ be i.i.d. random variables with $P(Y_k = 1) = P(Y_k = -1) = \\\\frac{1}{2}$. Set $M_n = \\\\sum_{k=1}^n Y_k$. Thus $M_n$ is the position of a simple random walk started from the origin after $n$ steps. In particular, $(M_n)_{n\\\\geq 0}$ is a martingale and $E[M_n] = 0$ for all $n$.\\n\\nNow let $\\\\tau = h_{\\\\{1\\\\}} = \\\\min\\\\{n : M_n = 1\\\\}$, a stopping time by Proposition 7.6. It is easy to show, e.g., in analogy to Exercise 3.21, that $\\\\tau < \\\\infty$ a.s. and hence $M_\\\\tau = 1$ a.s. But then $E[M_\\\\tau] = 1 \\\\neq 0 = E[M_0]$.",
        "proof": ""
    },
    {
        "type": "corollary",
        "id": "Corollary 8.19",
        "name": "Optional Stopping Variants",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Proposition 8.9",
            "Theorem 8.17"
        ],
        "statement": "Let $(M_n)_{n\\\\geq 0}$ be a martingale on a filtered probability space $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ and $\\\\tau$ an a.s. finite stopping time. Then\\n\\n$E[M_\\\\tau 1_{\\\\tau<\\\\infty}] = E[M_0]$\\n\\nif either of the following two conditions holds:\\n\\n(i) $\\\\{M_n : n \\\\geq 0\\\\}$ is uniformly integrable;\\n\\n(ii) $E[\\\\tau] < \\\\infty$ and there exists $L \\\\in \\\\mathbb{R}$ such that\\n\\n$E\\\\left[|M_{n+1} - M_n| \\\\mid F_n\\\\right] \\\\leq L$,\\n\\na.s. for all $n$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 8.17",
        "name": "Doob's Optional Sampling Theorem",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 5.24",
            "Theorem 8.17"
        ],
        "statement": "Let $X$ be a martingale on a filtered probability space $(\\\\Omega, \\\\mathcal{F}, (\\\\mathcal{F}_n)_{n\\\\geq 0}, P)$ and $\\\\tau, \\\\rho$ be two bounded stopping times, $\\\\tau \\\\leq \\\\rho$. Then $E[X_\\\\rho | \\\\mathcal{F}_\\\\tau] = X_\\\\tau$ a.s. and in particular $E[X_\\\\rho] = E[X_\\\\tau] = E[X_0]$. Similarly, if $X$ is a sub- (resp. super-) martingale then $E[X_\\\\rho | \\\\mathcal{F}_\\\\tau] \\\\geq X_\\\\tau$ (resp. $E[X_\\\\rho | \\\\mathcal{F}_\\\\tau] \\\\leq X_\\\\tau$) a.s.",
        "proof": "We have $E[(|M_{\\\\tau\\\\wedge n}| - K)^+] \\\\leq E[(|M_n| - K)^+]$. It follows, by Remark 5.21, that the family $(M_{\\\\tau\\\\wedge n} : n \\\\geq 0)$ is Uniformly Integrable. We have $M_{\\\\tau\\\\wedge n} \\\\to M_\\\\tau 1_{\\\\tau<\\\\infty}$ a.s., since $\\\\tau$ is a.s. finite, and hence also in $L^1$ by Theorem 5.24. In particular $E[M_{\\\\tau\\\\wedge n}] \\\\to E[M_\\\\tau 1_{\\\\tau<\\\\infty}]$. We conclude since, by Theorem 8.17, $E[M_{\\\\tau\\\\wedge n}] = E[M_0]$."
    },
    {
        "type": "corollary",
        "id": "Corollary 8.19",
        "name": "Optional Stopping Variants",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.17",
            "Theorem 5.24"
        ],
        "statement": "Let $(M_n)_{n\\\\geq 0}$ be a martingale on a filtered probability space $(\\\\Omega, \\\\mathcal{F}, (\\\\mathcal{F}_n)_{n\\\\geq 0}, P)$ and $\\\\tau$ an a.s. finite stopping time. Then $E[M_\\\\tau 1_{\\\\tau<\\\\infty}] = E[M_0]$ if either of the following two conditions holds: (i) $\\\\{M_n : n \\\\geq 0\\\\}$ is uniformly integrable; (ii) $E[\\\\tau] < \\\\infty$ and there exists $L \\\\in \\\\mathbb{R}$ such that $E[|M_{n+1} - M_n| | \\\\mathcal{F}_n] \\\\leq L$, a.s. for all $n$.",
        "proof": "(ii) Replacing $M_n$ by $M_n - M_0$, we assume without loss of generality that $M_0 = 0$. Then\\n\\n$|M_{n\\\\wedge\\\\tau}| = |M_{n\\\\wedge\\\\tau} - M_{0\\\\wedge\\\\tau}| \\\\leq \\\\sum_{i=1}^{n} |M_{i\\\\wedge\\\\tau} - M_{(i-1)\\\\wedge\\\\tau}| \\\\leq \\\\sum_{i=1}^{\\\\infty} |M_{i\\\\wedge\\\\tau} - M_{(i-1)\\\\wedge\\\\tau}| = \\\\sum_{i=1}^{\\\\infty} 1_{\\\\tau\\\\geq i}|M_i - M_{i-1}|.$\\n\\nNow\\n\\n$E\\\\left[\\\\sum_{i=1}^{\\\\infty} 1_{\\\\tau\\\\geq i}|M_i - M_{i-1}|\\\\right] = \\\\sum_{i=1}^{\\\\infty} E[1_{\\\\tau\\\\geq i}|M_i - M_{i-1}|]$ (by monotone convergence)\\n\\n$= \\\\sum_{i=1}^{\\\\infty} E\\\\left[E[1_{\\\\tau\\\\geq i}|M_i - M_{i-1}| | \\\\mathcal{F}_{i-1}]\\\\right]$ (tower property)\\n\\n$= \\\\sum_{i=1}^{\\\\infty} E\\\\left[1_{\\\\tau\\\\geq i}E[|M_i - M_{i-1}| | \\\\mathcal{F}_{i-1}]\\\\right]$ (since $\\\\{\\\\tau \\\\geq i\\\\} \\\\in \\\\mathcal{F}_{i-1}$)\\n\\n$\\\\leq L \\\\sum_{i=1}^{\\\\infty} E[1_{\\\\tau\\\\geq i}] = L \\\\sum_{i=1}^{\\\\infty} P[\\\\tau \\\\geq i] = LE[\\\\tau] < \\\\infty$.\\n\\nThe result now follows, as above, by DCT with the function on the right hand side of (34) as the dominating function."
    },
    {
        "type": "example",
        "id": "Example 8.20",
        "name": "Pattern Occurrence",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.19"
        ],
        "statement": "Suppose that $(\\\\Omega, \\\\mathcal{F}, P)$ is a probability space and $(X_i)_{i\\\\geq 1}$ are i.i.d. random variables with $P[X_i = j] = p_j > 0$ for each $j = 0, 1, 2, \\\\ldots$. What is the expected number of random variables that must be observed before the subsequence $0, 1, 2, 0, 1$ occurs?",
        "proof": "Consider a casino offering fair bets, where the expected gain from each bet is zero. In particular, a gambler betting \u00a3$a$ on the outcome of the next random variable being a $j$ will lose with probability $1 - p_j$ and will win \u00a3$a/p_j$ with probability $p_j$. (Her expected pay-out is $0(1 - p_j) + p_ja/p_j = a$, the same as the stake.)\\n\\nImagine a sequence of gamblers betting at the casino, each with an initial fortune of \u00a31.\\nGambler $i$ bets \u00a31 that $X_i = 0$; she is out if she loses and, if she wins, she bets her entire fortune of \u00a3$1/p_0$ that $X_{i+1} = 1$; if she wins again she bets her fortune of \u00a3$1/(p_0 p_1)$ that $X_{i+2} = 2$; if she wins that bet, then she bets \u00a3$1/(p_0 p_1 p_2)$ that $X_{i+3} = 0$; if she wins that bet then she bets her total fortune of \u00a3$1/(p_0^2 p_1 p_2)$ that $X_{i+4} = 1$; if she wins she quits with a fortune of \u00a3$1/(p_0^2 p_1 p_2)$.\\n\\nLet $M_n$ be the casino's winnings after $n$ games (so when $X_n$ has just been revealed). Then $(M_n)_{n\\\\geq 0}$ is a mean zero martingale w.r.t. the filtration $(\\\\mathcal{F}_n)_{n\\\\geq 0}$ where $\\\\mathcal{F}_n = \\\\sigma(X_1, \\\\ldots, X_n)$. Write $\\\\tau$ for the number of random variables to be revealed before we see the required pattern. Let $\\\\varepsilon = p_0^2 p_1 p_2$ and note that $P(\\\\tau > 5) \\\\leq (1 - \\\\varepsilon)$ and more generally, $P(\\\\tau > 5n) \\\\leq (1 - \\\\varepsilon)^n$ so that $E[\\\\tau] = \\\\sum_{n\\\\geq 0} P(\\\\tau \\\\geq n) < \\\\infty$. Since at most 5 people bet at any one time, $|M_{n+1} - M_n|$ is bounded by a constant (say $L = 5/(p_0^2 p_1 p_2)$), so condition (ii) of Theorem 8.19 is satisfied (with this $L$).\\n\\nWhen $X_\\\\tau$ is revealed each of the gamblers $1, 2, \\\\ldots, \\\\tau$ have paid \u00a31 to enter.\\n\\n- Gambler $\\\\tau - 4$ has won \u00a3$1/(p_0^2 p_1 p_2)$,\\n- Gamblers $\\\\tau - 3$ and $\\\\tau - 2$ have both lost and are out,\\n- Gambler $\\\\tau - 1$ has won \u00a3$1/(p_0 p_1)$,\\n- Gambler $\\\\tau$ has lost and is out.\\n\\nOf course, gamblers $\\\\tau + 1, \\\\tau + 2, \\\\ldots$ have not bet at all yet and all gamblers prior to $\\\\tau - 4$ have lost and are out.\\n\\nBy Theorem 8.19 $E[M_\\\\tau] = 0$, so taking expectations,\\n\\n$M_\\\\tau = \\\\tau - \\\\frac{1}{p_0^2 p_1 p_2} - \\\\frac{1}{p_0 p_1}$.\\n\\n$E[\\\\tau] = \\\\frac{1}{p_0^2 p_1 p_2} + \\\\frac{1}{p_0 p_1}$.\\n\\nThe same trick can be used to calculate the expected time until any specified (finite) pattern occurs in i.i.d. data."
    },
    {
        "type": "theorem",
        "id": "Theorem 8.21",
        "name": "Doob's maximal inequality",
        "topic": "Maximal Inequalities",
        "previous_results": [
            "Proposition 8.13"
        ],
        "statement": "Let $(X_n)_{n\\\\geq 0}$ be a submartingale on $(\\\\Omega, \\\\mathcal{F}, (\\\\mathcal{F}_n)_{n\\\\geq 0}, P)$. Then, for $\\\\lambda > 0$, $Y^\\\\lambda_n = (X_n - \\\\lambda)1_{\\\\{\\\\max_{k\\\\leq n} X_k\\\\geq\\\\lambda\\\\}}$, $n \\\\geq 0$, is a submartingale. In particular, $\\\\lambda P(\\\\max_{k\\\\leq n} X_k \\\\geq \\\\lambda) \\\\leq E[X_n1_{\\\\{\\\\max_{k\\\\leq n} X_k\\\\geq\\\\lambda\\\\}}] \\\\leq E[|X_n|]$.",
        "proof": "Let $\\\\tau = h_{[\\\\lambda,\\\\infty)} = \\\\inf\\\\{n \\\\geq 0 : X_n \\\\geq \\\\lambda\\\\}$ and set $V_n = 1_{\\\\{\\\\tau\\\\leq n-1\\\\}}$, $n \\\\geq 1$. Let $\\\\overline{X}_n := \\\\max_{k\\\\leq n} X_k$ and note that $V_n = 1_{\\\\{\\\\overline{X}_{n-1}\\\\geq\\\\lambda\\\\}}$. Applying Proposition 8.13 to $-X$ and $V$ we deduce that $(V \\\\circ X)_0 = 0$,\\n\\n$(V \\\\circ X)_n = \\\\sum_{k=1}^{n} V_k(X_k - X_{k-1}) = X_{n\\\\vee\\\\tau} - X_\\\\tau = (X_n - X_\\\\tau)1_{\\\\{\\\\tau\\\\leq n\\\\}}$, $n \\\\geq 1$,\\n\\nis a submartingale. Further, $X_\\\\tau \\\\geq \\\\lambda$ by definition so that $(X_\\\\tau - \\\\lambda)1_{\\\\{\\\\tau\\\\leq n\\\\}}$, $n \\\\geq 0$, is an adapted integrable and non-decreasing process and hence a submartingale. This shows that $Y^\\\\lambda$ is a sum of two submartingales and hence also a submartingale. In particular\\n\\n$0 \\\\leq E[(X_0 - \\\\lambda)1_{\\\\{X_0\\\\geq\\\\lambda\\\\}}] = E[Y^\\\\lambda_0] \\\\leq E[Y^\\\\lambda_n] = E[(X_n - \\\\lambda)1_{\\\\{\\\\tau\\\\leq n\\\\}}] = E[X_n1_{\\\\{\\\\overline{X}_n\\\\geq\\\\lambda\\\\}}] - \\\\lambda P(\\\\overline{X}_n \\\\geq \\\\lambda)$.\\n\\nRearranging we obtain the first required inequality and the second one is trivial."
    },
    {
        "type": "corollary",
        "id": "Corollary 8.22",
        "name": "Maximal Martingale Bound",
        "topic": "Maximal Inequalities",
        "previous_results": [
            "Theorem 8.21",
            "Proposition 8.9"
        ],
        "statement": "Let $p \\\\geq 1$ and $(M_n)_{n\\\\geq 0}$ be a martingale on a filtered probability space $(\\\\Omega, \\\\mathcal{F}, (\\\\mathcal{F}_n)_{n\\\\geq 0}, P)$ with $M_n \\\\in L^p$ for all $n \\\\geq 0$. Then, for any $n \\\\geq 0$ and $\\\\lambda > 0$ $P\\\\left(\\\\max_{n\\\\leq N} |M_n| \\\\geq \\\\lambda\\\\right) \\\\leq \\\\frac{E[|M_N|^p]}{\\\\lambda^p}$.",
        "proof": "This follows by applying Theorem 8.21 to $(|M_n|^p)_{n\\\\geq 0}$ which is a submartingale by Proposition 8.9."
    },
    {
        "type": "theorem",
        "id": "Theorem 8.23",
        "name": "Doob's Lp inequality",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.21",
            "Lemma 5.15"
        ],
        "statement": "Let $p > 1$ and $(X_n)_{n\\\\geq 0}$ be a non-negative submartingale on $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ with $X_n \\\\in L^p$ for all $n \\\\geq 0$. Then $\\\\max_{k\\\\leq n} X_k \\\\in L^p$ and\\n\\n$E[X^p_n] \\\\leq E\\\\left(\\\\max_{k\\\\leq n} X^p_k\\\\right) \\\\leq \\\\left(\\\\frac{p}{p-1}\\\\right)^p E[X^p_n].$",
        "proof": "The result follows instantly from Theorem 8.21 and Lemma 5.15."
    },
    {
        "type": "proposition",
        "id": "Proposition 8.24",
        "name": "Supermartingale maximal inequality",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.21"
        ],
        "statement": "Let $(X_n)_{n\\\\geq 0}$ be a supermartingale on a filtered probability space $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$. Then\\n\\n$\\\\lambda P(\\\\max_{k\\\\leq n} |X_k| \\\\geq \\\\lambda) \\\\leq E[X_0] + 2E[X^-_n], \\\\forall\\\\lambda, n \\\\geq 0.$",
        "proof": "Applying Doob's optional sampling theorem to $X$ and the stopping time $\\\\tau = \\\\min\\\\{k : X_k \\\\geq \\\\lambda\\\\} \\\\wedge n$, we obtain\\n\\n$E[X_0] \\\\geq E[X_\\\\tau] \\\\geq \\\\lambda P(\\\\max_{k\\\\leq n} X_k \\\\geq \\\\lambda) + E[X_n1_{\\\\{\\\\max_{k\\\\leq n} X_k<\\\\lambda\\\\}}].$\\n\\nThis leads to\\n\\n$\\\\lambda P(\\\\max_{k\\\\leq n} X_k \\\\geq \\\\lambda) \\\\leq E[X_0] + E[X^-_n].$\\n\\nOn the other hand, the process $(X^-_n)_{n\\\\geq 0}$ is a non-negative submartingale so we may apply Theorem 8.21 directly to it giving\\n\\n$\\\\lambda P(\\\\max_{k\\\\leq n} X^-_k \\\\geq \\\\lambda) \\\\leq E[X^-_n].$\\n\\nCombining, we obtain the desired result."
    },
    {
        "type": "definition",
        "id": "Definition 8.25",
        "name": "Upcrossings",
        "topic": "The Upcrossing Lemma and Martingale Convergence",
        "previous_results": [],
        "statement": "If $x = (x_n)_{n\\\\geq 0}$ is a sequence of real numbers and $a < b$ are fixed, define two integer-valued sequences $(\\\\rho_k)_{k\\\\geq 1} = (\\\\rho_k([a, b], x))_{k\\\\geq 1}$ and $(\\\\tau_k)_{k\\\\geq 0} = (\\\\tau_k([a, b], x))_{k\\\\geq 0}$ recursively as follows:\\n\\nLet $\\\\tau_0 = 0$ and for $k \\\\geq 1$ let\\n\\n$\\\\rho_k = \\\\inf\\\\{n \\\\geq \\\\tau_{k-1} : x_n \\\\leq a\\\\},$\\n$\\\\tau_k = \\\\inf\\\\{n \\\\geq \\\\rho_k : x_n \\\\geq b\\\\},$\\n\\nwith the usual convention that $\\\\inf \\\\emptyset = \\\\infty$.\\n\\nLet\\n\\n$U_n([a, b], x) = \\\\max\\\\{k \\\\geq 0 : \\\\tau_k \\\\leq n\\\\}$\\n\\nbe the number of upcrossings of $[a, b]$ by $x$ by time $n$ and let\\n\\n$U([a, b], x) = \\\\sup_n U_n([a, b], x) = \\\\sup\\\\{k \\\\geq 0 : \\\\tau_k < \\\\infty\\\\}$\\n\\nbe the total number of upcrossings of $[a, b]$ by $x$.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 8.26",
        "name": "Doob's Upcrossing Lemma",
        "topic": "The Upcrossing Lemma and Martingale Convergence",
        "previous_results": [
            "Proposition 8.13"
        ],
        "statement": "Let $X = (X_n)_{n\\\\geq 0}$ be a supermartingale on a filtered probability space $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$ and $a < b$ some fixed real numbers. Then, for every $n \\\\geq 0$,\\n\\n$E[U_n([a, b], X)] \\\\leq \\\\frac{E[(X_n - a)^-]}{b - a}.$",
        "proof": "$\\\\rho_k, \\\\tau_k$ are simply first hitting times after previous hitting times. It is an easy induction to check that for $k \\\\geq 1$, the random variables $\\\\rho_k = \\\\rho_k([a, b], X)$ and $\\\\tau_k = \\\\tau_k([a, b], X)$ are stopping times. Now set\\n\\n$V_n = \\\\sum_{k\\\\geq 1} 1_{\\\\{\\\\rho_k<n\\\\leq\\\\tau_k\\\\}}.$\\n\\nNotice that $V_n$ only takes the values $0$ and $1$. It is $1$ at time $n$ if $X$ is in the process of making an upcrossing from $a$ to $b$ or if $\\\\rho_k < n$ and $\\\\tau_k = \\\\infty$. It encodes our investment strategy above: we hold one unit of stock during an upcrossing or if $\\\\tau_k$ is infinite for some $k$ and $n > \\\\rho_k$.\\n\\nNotice that\\n\\n$\\\\{\\\\rho_k < n \\\\leq \\\\tau_k\\\\} = \\\\{\\\\rho_k \\\\leq n - 1\\\\} \\\\cap \\\\{\\\\tau_k \\\\leq n - 1\\\\}^c \\\\in F_{n-1}.$\\n\\nSo $(V_n)_{n\\\\geq 1}$ is non-negative and predictable so, by Proposition 8.13, $(V \\\\circ X)_n, n \\\\geq 0$ is a supermartingale. We write $U_n = U_n([a, b], X)$ and compute directly:\\n\\n$(V \\\\circ X)_n = \\\\sum_{k=1}^n V_k(X_k - X_{k-1})$\\n\\n$= \\\\sum_{i=1}^{U_n} (X_{\\\\tau_i} - X_{\\\\rho_i}) + 1_{\\\\{\\\\rho_{U_n+1}<n\\\\}}(X_n - X_{\\\\rho_{U_n+1}})$\\n\\n$\\\\geq (b - a)U_n - (X_n - a)^-.$\\n\\nFor the last step, note that if indicator function in the previous equation is non-zero, then $\\\\rho_{U_n+1} < \\\\infty$, so $X_{\\\\rho_{U_n+1}} \\\\leq a$. Hence $X_n - X_{\\\\rho_{U_n+1}} \\\\geq X_n - a \\\\geq -(X_n - a)^-$. Taking expectations,\\n\\n$0 = E[(V \\\\circ X)_0] \\\\geq E[(V \\\\circ X)_n] \\\\geq (b - a)E[U_n] - E[(X_n - a)^-]$\\n\\nand rearranging gives the result."
    },
    {
        "type": "lemma",
        "id": "Lemma 8.27",
        "name": "Convergence via upcrossings",
        "topic": "The Upcrossing Lemma and Martingale Convergence",
        "previous_results": [],
        "statement": "A real sequence $x = (x_n)$ converges to a limit in $[-\\\\infty, \\\\infty]$ if and only if $U([a, b], x) < \\\\infty$ for all $a, b \\\\in \\\\mathbb{Q}$ with $a < b$.",
        "proof": "From the definitions/basic analysis, $x$ converges if and only if $\\\\liminf_{n\\\\to\\\\infty} x_n = \\\\limsup_{n\\\\to\\\\infty} x_n$.\\n\\n(i) If $U([a, b], x) = \\\\infty$, then\\n\\n$\\\\liminf_{n\\\\to\\\\infty} x_n \\\\leq a < b \\\\leq \\\\limsup_{n\\\\to\\\\infty} x_n$\\n\\nand so $x$ does not converge.\\n\\n(ii) If $x$ does not converge, then we can choose rationals $a$ and $b$ with\\n\\n$\\\\liminf_{n\\\\to\\\\infty} x_n < a < b < \\\\limsup_{n\\\\to\\\\infty} x_n,$\\n\\nand then $U([a, b], x) = \\\\infty$."
    },
    {
        "type": "definition",
        "id": "Definition 8.28",
        "name": "Bounded in Lp",
        "topic": "Martingales in discrete time",
        "previous_results": [],
        "statement": "Let $(X_n)$ be a sequence of random variables on a probability space $(\\\\Omega, F, P)$, and let $p \\\\geq 1$. We say that $(X_n)$ is bounded in $L^p$ if\\n\\n$\\\\sup_n E[|X_n|^p] < \\\\infty$.\\n\\nNote that the condition says exactly that the set $\\\\{X_n : n \\\\geq 0\\\\}$ of random variables is a bounded subset of $L^p(\\\\Omega, F, P)$: there is some $K$ such that $||X_n||_p \\\\leq K$ for all $n$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 8.29",
        "name": "Doob's Forward Convergence Theorem",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Lemma 8.27",
            "Doob's Upcrossing Lemma"
        ],
        "statement": "Let $X$ be a sub- or super- martingale on a filtered probability space $(\\\\Omega, F, (F_n)_{n\\\\geq 0}, P)$. If $X$ is bounded in $L^1$ then $(X_n)_{n\\\\geq 0}$ converges a.s to a limit $X_\\\\infty$, and $X_\\\\infty$ is integrable.",
        "proof": "Considering $(-X_n)$ if necessary, we may suppose without loss of generality that $X = (X_n)$ is a super-martingale.\\n\\nFix rationals $a < b$. Then by Doob's Upcrossing Lemma\\n\\n$E[U_n([a, b], X)] \\\\leq \\\\frac{E[(X_n - a)^-]}{b - a} \\\\leq \\\\frac{E[|X_n|] + |a|}{b - a}$.\\n\\nSince $U_n(\\\\cdot \\\\cdot \\\\cdot) \\\\uparrow U(\\\\cdot \\\\cdot \\\\cdot)$ as $n \\\\to \\\\infty$, by the Monotone Convergence Theorem\\n\\n$E[U([a, b], X)] = \\\\lim_{n\\\\to\\\\infty} E[U_n([a, b], X)] \\\\leq \\\\sup_n \\\\frac{E[|X_n|] + |a|}{b - a} < \\\\infty$.\\n\\nHence $P[U([a, b], X) = \\\\infty] = 0$. Since $\\\\mathbb{Q}$ is countable, it follows that\\n\\n$P\\\\left(\\\\exists a, b \\\\in \\\\mathbb{Q}, a < b, \\\\text{ s.t. } U([a, b], X) = \\\\infty\\\\right) = 0$.\\n\\nSo by Lemma 8.27 $(X_n)_{n\\\\geq 0}$ converges a.s. to some $X_\\\\infty$. (Specifically, we may take $X_\\\\infty = \\\\liminf_{n\\\\to\\\\infty} X_n$, which is always defined, and measurable.) It remains to check that $X_\\\\infty$ is integrable. Since $|X_n| \\\\to |X_\\\\infty|$ a.s., Fatou's Lemma gives\\n\\n$E[|X_\\\\infty|] = E\\\\left(\\\\liminf_{n\\\\to\\\\infty} |X_n|\\\\right) \\\\leq \\\\liminf_{n\\\\to\\\\infty} E[|X_n|] \\\\leq \\\\sup_n E[|X_n|]$,\\n\\nwhich is finite by assumption."
    },
    {
        "type": "corollary",
        "id": "Corollary 8.30",
        "name": "Non-negative supermartingale convergence",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.29"
        ],
        "statement": "If $(X_n)_{n\\\\geq 0}$ is a non-negative supermartingale, then $X_\\\\infty = \\\\lim_{n\\\\to\\\\infty} X_n$ exists a.s. and is integrable.",
        "proof": "Since $E[|X_n|] = E[X_n] \\\\leq E[X_0]$ we may apply Theorem 8.29."
    },
    {
        "type": "example",
        "id": "Example 8.31",
        "name": "Galton\u2013Watson branching process",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Definition 0.1",
            "Corollary 8.30",
            "Lemma 4.14",
            "Lemma 5.13",
            "Theorem 5.24"
        ],
        "statement": "Recall Definition 0.1: let $X$ be a non-negative integer valued random variable with $0 < m = E[X] < \\\\infty$. Let $(X_{n,r})_{n,r\\\\geq 1}$ be an array of i.i.d. random variables with the same distribution as $X$. Set $Z_0 = 1$ and\\n\\n$Z_{n+1} = \\\\sum_{r=1}^{Z_n} X_{n+1,r} = \\\\sum_{r=1}^{\\\\infty} X_{n+1,r}1_{\\\\{Z_n\\\\geq r\\\\}}$\\n\\nso $Z_{n+1}$ is the number of individuals in generation $(n + 1)$ of our branching process. Finally, let $M_n = Z_n/m^n$, and let $F_n = \\\\sigma (\\\\{X_{i,r} : i \\\\leq n, r \\\\geq 1\\\\})$.",
        "proof": "By cMCT (which applies since everything is non-negative)\\n\\n$E[Z_{n+1} | F_n] = \\\\sum_{r=1}^{\\\\infty} E[1_{\\\\{Z_n\\\\geq r\\\\}}X_{n+1,r} | F_n] \\\\text{ a.s.} = \\\\sum_{r=1}^{\\\\infty} 1_{\\\\{Z_n\\\\geq r\\\\}}E[X_{n+1,r} | F_n] \\\\text{ a.s.} = \\\\sum_{r=1}^{\\\\infty} 1_{\\\\{Z_n\\\\geq r\\\\}}E[X_{n+1,r}] \\\\text{ a.s.} = \\\\sum_{r=1}^{\\\\infty} 1_{\\\\{Z_n\\\\geq r\\\\}}m = Z_n m$,\\n\\nand in particular $Z_n$, $M_n$ are both integrable. Clearly, both are $F_n$-measurable and $E[M_{n+1} | F_n] = M_n$ a.s. We conclude that $(M_n)_{n\\\\geq 0}$ is a non-negative martingale and, by Corollary 8.30, it converges a.s. to a finite limit $M_\\\\infty$.\\n\\nIf $m < 1$ then by the above $(Z_n)_{n\\\\geq 0}$ is a non-negative supermartingale and hence also converges a.s. to a finite limit $Z_\\\\infty$. But since $M_n = Z_n/m^n$ converges, we necessarily have $Z_\\\\infty = 0$ a.s. Since $Z_n$ is integer valued it has to be equal to 0 from some point onwards, i.e., $Z_n = 0$ a.s., for $n \\\\geq \\\\tau$, where $\\\\tau = \\\\tau(\\\\omega)$ is the extinction time which we conclude has to be finite a.s. Note that $\\\\tau = \\\\inf\\\\{n : Z_n = 0\\\\}$ is a stopping time.\\n\\nIt follows that $M_\\\\infty = 0$ a.s. as well since $M_n = 0$ for $n \\\\geq \\\\tau$. In particular, $M_n$ does not converge to $M_\\\\infty$ in $L^1$ by Lemma 4.14, and hence also not in any other $L^p$ for $p > 1$ by Lemma 5.13.\\n\\nWhat is happening for our subcritical branching process is that although for large $n$, $M_n$ is very likely to be zero, if it is not zero then it is very big with sufficiently high probability that $E[M_n]$ is constant and does not converge to 0. This mirrors what we saw with sequences in Example 5.3. Finally note that, by Theorem 5.24, we can also conclude that $\\\\{M_n : n \\\\geq 0\\\\}$ is not Uniformly Integrable."
    },
    {
        "type": "theorem",
        "id": "Theorem 8.32",
        "name": "UI Martingale Characterization",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Proposition 5.22",
            "Theorem 8.29",
            "Theorem 5.24",
            "Proposition 1.24",
            "Theorem 6.11",
            "Theorem 8.33"
        ],
        "statement": "Let $(M_n)_{n\\\\geq 0}$ be a martingale on a filtered probability space $(\\\\Omega, \\\\mathcal{F}, (\\\\mathcal{F}_n)_{n\\\\geq 0}, P)$. TFAE\\n\\n(i) $M$ is uniformly integrable,\\n\\n(ii) there is some $\\\\mathcal{F}_\\\\infty$-measurable random variable $M_\\\\infty$ such that $M_n \\\\to M_\\\\infty$ almost surely and in $L^1$,\\n\\n(iii) there is an integrable $\\\\mathcal{F}_\\\\infty$-measurable random variable $M_\\\\infty$ such that $M_n = E[M_\\\\infty | \\\\mathcal{F}_n]$ a.s. for all $n$.\\n\\nFurther, under these conditions, if $M_\\\\infty \\\\in L^p$ for $p > 1$ then the convergence $M_n \\\\to M_\\\\infty$ also holds in $L^p$.",
        "proof": "$(i) \\\\Rightarrow (ii)$: $M$ is UI so in particular, by Proposition 5.22, bounded in $L^1$ and hence, by Doob's Forward Convergence Theorem (Theorem 8.29) it converges a.s. to some integrable $M_\\\\infty$. Since a.s. convergence implies convergence in probability, $M_n \\\\to M_\\\\infty$ in $L^1$ by Theorem 5.24. Each $M_n$ is $\\\\mathcal{F}_\\\\infty$-measurable and hence so is $M_\\\\infty$ by Proposition 1.24.\\n\\n$(ii) \\\\Rightarrow (iii)$: Since $(M_n)$ is a martingale, for $m \\\\geq n$, we have\\n\\n$E[M_m | \\\\mathcal{F}_n] = M_n$ a.s.,\\n\\nso, by the defining relation (24) for the conditional expectation,\\n\\n$E[M_m1_A] = E[M_n1_A]$, for all $A \\\\in \\\\mathcal{F}_n$.\\n\\nSince\\n\\n$|E[M_\\\\infty1_A] - E[M_m1_A]| \\\\leq E[|(M_\\\\infty - M_m)1_A|] \\\\leq E[|M_\\\\infty - M_m|] \\\\to 0$,\\n\\nit follows that\\n\\n$E[M_\\\\infty1_A] = E[M_n1_A]$ for all $A \\\\in \\\\mathcal{F}_n$.\\n\\nSince $M_n$ is $\\\\mathcal{F}_n$-measurable, this shows that $M_n = E[M_\\\\infty | \\\\mathcal{F}_n]$ a.s.\\n\\n$(iii) \\\\Rightarrow (i)$ by Theorem 6.11.\\n\\nThe last assertion follows instantly from the Dominated Convergence Theorem and Theorem 8.33 below."
    },
    {
        "type": "theorem",
        "id": "Theorem 8.33",
        "name": "UI Martingale Properties",
        "topic": "Martingales in discrete time",
        "previous_results": [
            "Theorem 8.17",
            "Theorem 5.24",
            "Theorem 6.11",
            "Theorem 8.21",
            "Lemma 5.15"
        ],
        "statement": "On a filtered probability space $(\\\\Omega, \\\\mathcal{F}, (\\\\mathcal{F}_n)_{n\\\\geq 0}, P)$, let $M$ be a UI martingale so that $M_n = E[M_\\\\infty | \\\\mathcal{F}_n]$ for some $M_\\\\infty \\\\in L^1(\\\\Omega, \\\\mathcal{F}_\\\\infty, P)$. Then for any stopping times $\\\\tau \\\\leq \\\\rho$\\n\\n$E[M_\\\\rho | \\\\mathcal{F}_\\\\tau] = M_\\\\tau$ a.s.\\n\\nand in particular $E[M_\\\\tau] = E[M_0]$.\\n\\nFurther, Doob's maximal and $L^p$ inequalities extend to $n = \\\\infty$. Specifically, with $M^*_\\\\infty = \\\\max_{n\\\\geq 0} |M_n|$ we have\\n\\n$\\\\lambda P[M^*_\\\\infty \\\\geq \\\\lambda] \\\\leq E[|M_\\\\infty|1_{\\\\{M^*_\\\\infty \\\\geq\\\\lambda\\\\}}]$, $\\\\lambda \\\\geq 0$.\\n\\nFurther, if $M_\\\\infty \\\\in L^p$ for some $p > 1$ then, with $p^{-1} + q^{-1} = 1$,\\n\\n$\\\\|M_\\\\infty\\\\|_p \\\\leq \\\\|M^*_\\\\infty\\\\|_p \\\\leq q\\\\|M_\\\\infty\\\\|_p$\\n\\nand $M_n \\\\to M_\\\\infty$ in $L^p$.",
        "proof": "First note that if $\\\\tau$ is bounded, $\\\\tau \\\\leq n$ and $\\\\rho = \\\\infty$ then by Theorem 8.17\\n\\n$M_\\\\tau = E[M_n | \\\\mathcal{F}_\\\\tau] = E[E[M_\\\\infty | \\\\mathcal{F}_n] | \\\\mathcal{F}_\\\\tau] = E[M_\\\\infty | \\\\mathcal{F}_\\\\tau]$.\\n\\nIt remains the establish the same for any stopping time $\\\\tau$ and $\\\\rho = \\\\infty$ as the general case then follows by the tower property.\\n\\nLet $A \\\\in \\\\mathcal{F}_\\\\tau$ and note that $A \\\\cap \\\\{\\\\tau \\\\leq n\\\\}$ is in $\\\\mathcal{F}_n$, by definition of $\\\\mathcal{F}_\\\\tau$, but also in $\\\\mathcal{F}_{\\\\tau\\\\wedge n}$ as is easy to verify.\\n\\nThen\\n\\n$E[M_\\\\infty1_{A\\\\cap\\\\{\\\\tau<\\\\infty\\\\}}] = \\\\lim_{n\\\\to\\\\infty} E[M_\\\\infty1_{A\\\\cap\\\\{\\\\tau\\\\leq n\\\\}}] = \\\\lim_{n\\\\to\\\\infty} E[M_{\\\\tau\\\\wedge n}1_{A\\\\cap\\\\{\\\\tau\\\\leq n\\\\}}] = E[M_\\\\tau 1_{A\\\\cap\\\\{\\\\tau<\\\\infty\\\\}}]$,\\n\\nwhere the first equality follows by the MCT, the second follows since we already have the desired property for bounded stopping times and the last equality is a consequence of Theorem 5.24 thanks to uniform integrability of the family $M_{\\\\tau\\\\wedge n} = E[M_\\\\infty | \\\\mathcal{F}_{\\\\tau\\\\wedge n}]$, $n \\\\geq 0$, (by Theorem 6.11) and a.s. convergence $M_{\\\\tau\\\\wedge n}1_{A\\\\cap\\\\{\\\\tau\\\\leq n\\\\}} \\\\to M_\\\\tau 1_A$ (and hence also in probability). Finally, the equality $E[M_\\\\infty1_{A\\\\cap\\\\{\\\\tau=\\\\infty\\\\}}] = E[M_\\\\tau 1_{A\\\\cap\\\\{\\\\tau=\\\\infty\\\\}}]$ is obvious. This establishes (39).\\n\\nWe turn to the two remaining assertions. By conditional Jensen's inequality $(|M_n|)_{0\\\\leq n\\\\leq\\\\infty}$ is a submartingale. By Doob's maximal inequality, Theorem 8.21, with $M^*_n = \\\\max_{k\\\\leq n} |M_k|$, we have\\n\\n$\\\\lambda P(M^*_n \\\\geq \\\\lambda) \\\\leq E[|M_n|1_{\\\\{M^*_n \\\\geq\\\\lambda\\\\}}] \\\\leq E[|M_\\\\infty|1_{\\\\{M^*_n \\\\geq\\\\lambda\\\\}}]$\\n\\nsince $\\\\{M^*_n \\\\geq \\\\lambda\\\\} \\\\in \\\\mathcal{F}_n$ and $E[|M_\\\\infty| | \\\\mathcal{F}_n] \\\\geq |M_n|$. Taking the limit in $n \\\\to \\\\infty$, using MCT on the left and DCT on the right, we see that the maximal inequality (40) holds as required. Suppose now that $M_\\\\infty \\\\in L^p$ for some $p > 1$. Then Doob's $L^p$ inequality (41) follows by Lemma 5.15. It shows in particular that $|M_n|^p \\\\leq (M^*_\\\\infty)^p \\\\in L^1$ and hence $M_n \\\\to M_\\\\infty$ in $L^p$ by the DCT."
    },
    {
        "type": "definition",
        "id": "Definition 9.1",
        "name": "Backwards Martingale",
        "topic": "Some applications of the martingale theory",
        "previous_results": [
            "Theorem 6.11"
        ],
        "statement": "Given $\\\\sigma$-algebras $(\\\\mathcal{F}_{-n})_{n\\\\geq 0}$ with $\\\\mathcal{F}_{-n} \\\\subseteq \\\\mathcal{F}$ and\\n\\n$\\\\cdot \\\\cdot \\\\cdot \\\\subseteq \\\\mathcal{F}_{-(n+1)} \\\\subseteq \\\\mathcal{F}_{-n} \\\\subseteq \\\\cdot \\\\cdot \\\\cdot \\\\subseteq \\\\mathcal{F}_{-2} \\\\subseteq \\\\mathcal{F}_{-1} \\\\subseteq \\\\mathcal{F}_0$,\\n\\na backwards martingale w.r.t. $(\\\\mathcal{F}_{-n})$ is a sequence $(M_{-n})_{n\\\\geq 0}$ of integrable random variables, each $M_{-n}$ is $\\\\mathcal{F}_{-n}$-measurable and\\n\\n$E[M_{-n+1} | \\\\mathcal{F}_{-n}] = M_{-n}$ a.s.\\n\\nfor all $n \\\\geq 1$.\\n\\nFor any backwards martingale, we have\\n\\n$E[M_0 | \\\\mathcal{F}_{-n}] = M_{-n}$ a.s.\\n\\nSince $M_0$ is integrable, it follows from Theorem 6.11 that $(M_{-n})_{n\\\\geq 0}$ is automatically uniformly integrable.\\n\\nDoob's Upcrossing Lemma (Lemma 8.26), dealt with martingales on a finite set of time points. We can apply it to $(M_{-m}, M_{-m+1}, \\\\ldots, M_{-1}, M_0)$, to see that if $U_m([a, b], M)$ is the number of upcrossings of $[a, b]$ by the backwards martingale between times $-m$ and $0$, then\\n\\n$E[U_m([a, b], M)] \\\\leq \\\\frac{E[(M_0 - a)^-]}{b - a}$.\\n\\nMimicking the proof of Doob's Forward Convergence Theorem (Theorem 8.29), we let $m \\\\to \\\\infty$ and use Monotone Convergence Theorem to conclude that $U([a, b], M) = U_\\\\infty([a, b], M)$ is integrable and hence finite a.s.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 9.2",
        "name": "Backwards Martingale Convergence",
        "topic": "Some applications of the martingale theory",
        "previous_results": [
            "Theorem 8.32"
        ],
        "statement": "Let $(M_{-n})_{n\\\\geq 0}$ be a backwards martingale w.r.t. $(\\\\mathcal{F}_{-n})_{n\\\\geq 0}$. Then $M_{-n}$ converges a.s. and in $L^1$ as $n \\\\to \\\\infty$ to the random variable $M_{-\\\\infty} = E[M_0 | \\\\mathcal{F}_{-\\\\infty}]$.\\n\\nNote that we can replace $M_0$ by any other fixed element of the sequence: $M_{-\\\\infty} = E[M_{-k} | \\\\mathcal{F}_{-\\\\infty}]$ for all $k \\\\geq 0$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 9.3",
        "name": "Kolmogorov's Strong Law of Large Numbers",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Theorem 9.2",
            "Theorem 3.14"
        ],
        "statement": "Let $(X_n)_{n\\\\geq1}$ be a sequence of i.i.d. random variables each of which is integrable and has mean $m$, and set $S_n = \\\\sum_{k=1}^n X_k$. Then $\\\\frac{S_n}{n} \\\\to m$ a.s. and in $L^1$.",
        "proof": "For $n \\\\geq 1$ set $F_{-n} = \\\\sigma (S_n, S_{n+1}, S_{n+2}, \\\\ldots) = \\\\sigma (S_n, X_{n+1}, X_{n+2}, \\\\ldots)$, noting that $F_{-n-1} \\\\subseteq F_{-n}$. Conditioning on $F_{-n}$ preserves the symmetry between $X_1, \\\\ldots, X_n$, since none of $S_n, S_{n+1}, \\\\ldots$ is affected by permuting $X_1, \\\\ldots, X_n$. Hence, $E[X_1 | F_{-n}] = E[X_2 | F_{-n}] = \\\\cdot \\\\cdot \\\\cdot = E[X_n | F_{-n}]$ and so they are all equal (a.s.) to their average: $E[X_i | F_{-n}] = \\\\frac{1}{n} E[X_1 + \\\\cdot \\\\cdot \\\\cdot + X_n | F_{-n}] = \\\\frac{1}{n} E[S_n | F_{-n}] = \\\\frac{1}{n} S_n, 1 \\\\leq i \\\\leq n$. Let $M_{-n} = S_n/n$. Then, for $n \\\\geq 2$, $E[M_{-n+1} | F_{-n}] = \\\\frac{1}{n - 1} E[S_{n-1} | F_{-n}] = \\\\frac{1}{n - 1} \\\\sum_{i=1}^{n-1} E[X_i | F_{-n}] = \\\\frac{S_n}{n} = M_{-n}$. In other words, $(M_{-n})_{n\\\\geq1}$ is a backwards martingale w.r.t. $(F_{-n})_{n\\\\geq1}$. Thus, by Theorem 9.2, $S_n/n$ converges a.s. and in $L^1$ to $M_{-\\\\infty} = E[M_{-1} | F_{-\\\\infty}]$, where $F_{-\\\\infty} = \\\\bigcup_{k\\\\geq1} F_{-k}$. Now by $L^1$ convergence, $E[M_{-\\\\infty}] = \\\\lim_{n\\\\to\\\\infty} E[M_{-n}] = E[M_{-1}] = E[S_1] = m$. In terms of the random variables $X_1, X_2, \\\\ldots$, the limit $M_{-\\\\infty} = \\\\liminf S_n/n$ is a tail random variable, so by Kolmogorov's 0-1 law (Theorem 3.14) it is a.s. constant, so $M_{-\\\\infty} = m$ a.s."
    },
    {
        "type": "definition",
        "id": "Definition 9.4",
        "name": "Exchangeability",
        "topic": "Applications of Martingale Theory",
        "previous_results": [],
        "statement": "The random variables $X_1, \\\\ldots, X_n$ are said to be exchangeable if the vector $(X_{i_1}, \\\\ldots, X_{i_n})$ has the same probability distribution for every permutation $i_1, \\\\ldots, i_n$ of $1, \\\\ldots, n$.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 9.5",
        "name": "Exchangeable Variables",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Definition 9.4"
        ],
        "statement": "Let $X_1, \\\\ldots, X_n$ be the results of $n$ successive samples without replacement from a pool of at least $n$ values (some of which may be the same). Then the random variables $X_1, \\\\ldots, X_n$ are exchangeable but not independent.",
        "proof": ""
    },
    {
        "type": "definition",
        "id": "Definition 9.6",
        "name": "Doob's backward martingale",
        "topic": "Applications of Martingale Theory",
        "previous_results": [],
        "statement": "The martingale $Z_j = \\\\frac{S_{n+1-j}}{n + 1 - j}, j = 1, 2, \\\\ldots, n,$ is sometimes called Doob's backward martingale.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 9.7",
        "name": "The ballot problem",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Definition 9.6",
            "Theorem 8.17"
        ],
        "statement": "In an election between candidates A and B, candidate A receives $n$ votes and candidate B receives $m$ votes, where $n > m$. Assuming that in the count of votes all orderings are equally likely, what is the probability that A is always ahead of B during the count?",
        "proof": "Let $X_i = 1$ if the $i$th vote counted is for A and $-1$ if the $i$th vote counted is for B, and let $S_k = \\\\sum_{i=1}^k X_i$. Because all orderings of the $n + m$ votes are equally likely, $X_1, \\\\ldots, X_{n+m}$ are exchangeable, so $Z_j = \\\\frac{S_{n+m+1-j}}{n + m + 1 - j}, j = 1, 2, \\\\ldots, n + m,$ is a Doob backward martingale. Because $Z_1 = \\\\frac{S_{n+m}}{n + m} = \\\\frac{n - m}{n + m}$, the mean of this martingale is $(n - m)/(n + m)$. Because $n > m$, either (i) A is always ahead in the count, or (ii) there is a tie at some point. Case (ii) happens if and only if some $S_j = 0$, i.e., if and only if some $Z_j = 0$. Define the bounded stopping time $\\\\tau$ by $\\\\tau = \\\\min\\\\{ j \\\\geq 1 : Z_j = 0 \\\\text{ or } j = n + m\\\\}$. In case (i), $Z_{\\\\tau} = Z_{n+m} = X_1 = 1$. (If A is always ahead, he must receive the first vote.) Clearly, in case (ii), $Z_{\\\\tau} = 0$, so $Z_{\\\\tau} = \\\\begin{cases} 1 & \\\\text{if A is always ahead,} \\\\\\\\ 0 & \\\\text{otherwise.} \\\\end{cases}$ By Theorem 8.17, $E[Z_{\\\\tau}] = (n - m)/(n + m)$ and so $P[\\\\text{A is always ahead}] = \\\\frac{n - m}{n + m}$."
    },
    {
        "type": "lemma",
        "id": "Lemma 9.8",
        "name": "MGF Bound",
        "topic": "Applications of Martingale Theory",
        "previous_results": [],
        "statement": "Let $Y$ be a random variable with mean $0$, taking values in $[-c, c]$. Then\\n\\n$E[e^{\\\\theta Y}] \\\\leq \\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2\\\\right)$.\\n\\nLet $G$ be a $\\\\sigma$-algebra, and $Y$ be a random variable with $E[Y|G] = 0$ a.s. and $Y \\\\in [-c, c]$ a.s. Then\\n\\n$E[e^{\\\\theta Y} | G] \\\\leq \\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2\\\\right)$ a.s.",
        "proof": "Let $f(y) = e^{\\\\theta y}$. Since $f$ is convex,\\n\\n$f(y) \\\\leq \\\\frac{c-y}{2c}f(-c) + \\\\frac{c+y}{2c}f(c)$\\n\\nfor all $y \\\\in [-c, c]$. Then taking expectations,\\n\\n$E[f(Y)] \\\\leq E\\\\left[\\\\frac{c-Y}{2c}f(-c) + \\\\frac{c+Y}{2c}f(c)\\\\right]$\\n\\n$= \\\\frac{1}{2}f(-c) + \\\\frac{1}{2}f(c)$\\n\\n$= \\\\frac{1}{2}\\\\frac{e^{-\\\\theta c} + e^{\\\\theta c}}{2}$.\\n\\nNow, comparing Taylor expansions term by term,\\n\\n$\\\\frac{e^{-\\\\theta c} + e^{\\\\theta c}}{2} = \\\\sum_{n=0}^{\\\\infty}\\\\frac{(\\\\theta c)^{2n}}{(2n)!} \\\\leq \\\\sum_{n=0}^{\\\\infty}\\\\frac{(\\\\theta c)^{2n}}{2^n n!} = \\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2\\\\right)$,\\n\\ngiving part (i).\\n\\nFor the conditional version of the statement, consider any $G \\\\in \\\\mathcal{G}$ with $P[G] > 0$. Then $E[Y 1_G] = 0$, so $E[Y | G] = 0$. Applying part (i) with probability measure $P[\\\\cdot | G]$, we obtain $E[e^{\\\\theta Y} | G] \\\\leq \\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2\\\\right)$.\\n\\nNow consider the $\\\\mathcal{G}$-measurable set $G := \\\\{\\\\omega : E[e^{\\\\theta Y}|\\\\mathcal{G}](\\\\omega) > \\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2\\\\right)\\\\}$. If this set has positive probability, it contradicts the previous paragraph. So indeed $E[e^{\\\\theta Y} | \\\\mathcal{G}] \\\\leq \\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2\\\\right)$ a.s. as required."
    },
    {
        "type": "lemma",
        "id": "Lemma 9.9",
        "name": "Martingale MGF Bound",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Lemma 9.8"
        ],
        "statement": "Suppose $M$ is a martingale with $M_0 = 0$ and $|M_n - M_{n-1}| \\\\leq c$ a.s. for all $n$. Then\\n\\n$E[e^{\\\\theta M_n}] \\\\leq \\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2 n\\\\right)$.",
        "proof": "Let $W_n = e^{\\\\theta M_n}$, so that $W_n$ is non-negative and $W_n = W_{n-1}e^{\\\\theta(M_n-M_{n-1})}$.\\n\\nThen applying Lemma 9.8(ii) with $Y = M_n - M_{n-1}$ and $\\\\mathcal{G} = \\\\mathcal{F}_{n-1}$,\\n\\n$E(W_n | \\\\mathcal{F}_{n-1}) = W_{n-1}E[e^{\\\\theta(M_n-M_{n-1})} | \\\\mathcal{F}_{n-1}] \\\\leq W_{n-1}\\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2\\\\right)$ a.s.\\n\\nTaking expectations we obtain $E[W_n] \\\\leq \\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2\\\\right)E[W_{n-1}]$ and the result follows by induction."
    },
    {
        "type": "theorem",
        "id": "Theorem 9.10",
        "name": "Azuma-Hoeffding Inequality",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Lemma 9.9"
        ],
        "statement": "Suppose $M$ is a martingale with $M_0 = 0$ and $|M_n - M_{n-1}| \\\\leq c$ a.s. for all $n$. Then\\n\\n$P(M_n \\\\geq a) \\\\leq \\\\exp\\\\left(-\\\\frac{1}{2}\\\\frac{a^2}{c^2n}\\\\right)$,\\n\\nand\\n\\n$P(|M_n| \\\\geq a) \\\\leq 2\\\\exp\\\\left(-\\\\frac{1}{2}\\\\frac{a^2}{c^2n}\\\\right)$.",
        "proof": "$P(M_n \\\\geq a) \\\\leq P(e^{\\\\theta M_n} \\\\geq e^{\\\\theta a}) \\\\leq e^{-\\\\theta a}\\\\exp\\\\left(\\\\frac{1}{2}\\\\theta^2 c^2 n\\\\right)$\\n\\nusing Markov's inequality. Now we are free to optimise over $\\\\theta$. The RHS is minimised when $\\\\theta = a/(c^2n)$, giving the required bound.\\n\\nThe same argument applies replacing $M$ by the martingale $-M$. Summing the two bounds then gives the bound for $|M|$."
    },
    {
        "type": "definition",
        "id": "Definition 9.11",
        "name": "Discrete Lipschitz Function",
        "topic": "Applications of Martingale Theory",
        "previous_results": [],
        "statement": "Let $h$ be a function of $n$ variables. The function $h$ is said to be $c$-Lipschitz, where $c > 0$, if changing the value of any one coordinate causes the value of $h$ to change by at most $c$. That is, whenever $x = (x_1, \\\\ldots, x_n)$ and $y = (y_1, \\\\ldots, y_n)$ differ in at most one coordinate, then $|h(x) - h(y)| \\\\leq c$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 9.12",
        "name": "Concentration of Lipschitz Functions",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Theorem 9.10"
        ],
        "statement": "Suppose $h$ is a $c$-Lipschitz function, and $X_1, \\\\ldots, X_n$ are independent random variables. Then\\n\\n$P(|h(X_1, \\\\ldots, X_n) - E[h(X_1, \\\\ldots, X_n)]| \\\\geq a) \\\\leq 2\\\\exp\\\\left(-\\\\frac{1}{2}\\\\frac{a^2}{c^2n}\\\\right)$.",
        "proof": "The proof is based on the idea of the Doob martingale. We reveal information about the underlying random variables $X_1, \\\\ldots, X_n$ one step at a time, gradually acquiring a more precise idea of the value $h(X_1, \\\\ldots, X_n)$.\\n\\nFor $0 \\\\leq k \\\\leq n$, let $\\\\mathcal{F}_k = \\\\sigma(X_1, \\\\ldots, X_k)$, and let\\n\\n$M_k = E[h(X_1, \\\\ldots, X_n) | \\\\mathcal{F}_k] - E[h(X_1, \\\\ldots, X_n)]$.\\n\\nThen $M_0 = 0$, and $M_n = h(X_1, \\\\ldots, X_n) - E[h(X_1, \\\\ldots, X_n)]$.\\n\\nWe claim $|M_{k+1} - M_k| \\\\leq c$ a.s. To show this, let $\\\\hat{X}_{k+1}$ be a random variable with the same distribution as $X_{k+1}$, which is independent of $X_1, \\\\ldots, X_n$.\\n\\nThen\\n\\n$E[h(X_1, \\\\ldots, X_k, X_{k+1}, \\\\ldots, X_n) | \\\\mathcal{F}_k]$\\n$= E[h(X_1, \\\\ldots, X_k, \\\\hat{X}_{k+1}, \\\\ldots, X_n) | \\\\mathcal{F}_k]$\\n$= E[h(X_1, \\\\ldots, X_k, \\\\hat{X}_{k+1}, \\\\ldots, X_n) | \\\\mathcal{F}_{k+1}]$.\\n\\nThis gives\\n\\n$M_{k+1} - M_k = E[h(X_1, \\\\ldots, X_k, \\\\hat{X}_{k+1}, \\\\ldots, X_n) - h(X_1, \\\\ldots, X_k, X_{k+1}, \\\\ldots, X_n) | \\\\mathcal{F}_{k+1}]$.\\n\\nBut the difference between the two values of $h$ inside the conditional expectation on the RHS is in $[-c, c]$, so we obtain $|M_{k+1} - M_k| \\\\leq c$ a.s. as required. Now the required estimate for $M_n$ follows from the Azuma-Hoeffding bound (Theorem 9.10)."
    },
    {
        "type": "example",
        "id": "Example 9.13",
        "name": "Longest Common Subsequence",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Theorem 9.12"
        ],
        "statement": "Let $X = (X_1, X_2, \\\\ldots, X_m)$ and $Y = (Y_1, Y_2, \\\\ldots, Y_m)$ be two independent sequences, each with independent entries. Let $L_m$ be the length of the longest sequence which is a subsequence (not necessarily consecutive) of both sequences. For example, if $m = 12$ and $X =\\",
        "CAGGGTAGTAAG\\": "and $Y =\\",
        "CGTGTGAAAACT\\": "then both $X$ and $Y$ contain the substring \\",
        "CGGTAAA\\": "and $L_m = 7$. Changing a single entry can't change the length of the longest common subsequence by more than 1. We can apply Theorem 9.12 with $n = 2m$ and $c = 1$, to get $P(|L_m - E[L_m]| \\\\geq a) \\\\leq 2 \\\\exp\\\\left(-\\\\frac{a^2}{4m}\\\\right)$. We obtain that for large $m$, \\",
        "fluctuations\\": "f $L_m$ around its mean are on the scale at most $\\\\sqrt{m}$. Note that we didn't require the sequences $X$ and $Y$ to have the same distribution, or for the entries of each sequence to be identically distributed. As suggested by the choice of strings above, longest common subsequence problems arise for example in computational biology, involving the comparison of DNA strings (which evolve via mutation, insertion or deletion of individual nucleotides).",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 9.14",
        "name": "Minimum-length Matching",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Theorem 9.12"
        ],
        "statement": "Suppose there are $m$ red points in the box $[0, 1]^2 \\\\subset \\\\mathbb{R}^2$, with positions $R_1, \\\\ldots, R_m$, and $m$ blue points with positions $B_1, \\\\ldots, B_m$. Let $X$ be the length of the minimal-length matching, which joins pairs consisting of one blue and one red point. That is, $X_m = \\\\min\\\\sum_{k=1}^{m}\\\\|R_k - B_{i_k}\\\\|$, where the minimum is taken over all permutations $i_1, i_2, \\\\ldots, i_m$ of $1, 2, \\\\ldots, m$, and $\\\\|r - b\\\\|$ denotes Euclidean distance between $r$ and $b$. Alternatively let $Y$ be the length of the minimal-length alternating tour, a path which visits all $2m$ points, alternating between red and blue, and returning to its starting point: $Y_m = \\\\min\\\\left\\\\{\\\\sum_{k=1}^{m}\\\\|R_{i_k} - B_{j_k}\\\\| + \\\\sum_{k=1}^{m-1}\\\\|B_{j_k} - R_{i_{k+1}}\\\\| + \\\\|B_{j_m} - R_{i_1}\\\\|\\\\right\\\\}$, where now the minimum is over all pairs of permutations $i_1, i_2, \\\\ldots, i_m$ and $j_1, j_2, \\\\ldots, j_m$ of $1, 2, \\\\ldots, m$. Moving a single point cannot change $X_m$ by more than $\\\\sqrt{2}$, and cannot change $Y_m$ by more than $2\\\\sqrt{2}$. If the positions of the points are independent, then applying Theorem 9.12 with $n = 2m$ and the appropriate value of $c$, we obtain $P(|X_m - E[X_m]| \\\\geq a) \\\\leq 2 \\\\exp\\\\left(-\\\\frac{a^2}{8m}\\\\right)$ and $P(|Y_m - E[Y_m]| \\\\geq a) \\\\leq 2 \\\\exp\\\\left(-\\\\frac{a^2}{32m}\\\\right)$. Again this gives concentration of $X_m$ and $Y_m$ around their means on the scale of $\\\\sqrt{m}$. This may be a poor bound; for example if all the points are i.i.d. uniform on the box $[0, 1]^2$, then in fact the means themselves grow like $\\\\sqrt{m}$ as $m \\\\to \\\\infty$. However, we didn't assume identical distribution. For example we might have red points uniform on the left half $[0, 1/2] \\\\times [0, 1]$, and blue points uniform on the right half $[1/2, 1] \\\\times [0, 1]$, in which case the means grow linearly in $m$, and the $O(\\\\sqrt{m})$ fluctuation bound is more interesting.",
        "proof": ""
    },
    {
        "type": "example",
        "id": "Example 9.15",
        "name": "Chromatic Number",
        "topic": "Applications of Martingale Theory",
        "previous_results": [
            "Theorem 9.12"
        ],
        "statement": "The Erd\u0151s-R\u00e9nyi random graph model $G(N, p)$ consists of a graph with $N$ vertices, in which each edge (out of the $\\\\binom{N}{2}$ possible edges) appears independently with probability $p$. If $p = 1/2$, then the graph is uniformly distributed over all possible graphs with $N$ vertices. The chromatic number $\\\\chi(G)$ of a graph $G$ is the minimal number of colours needed to colour the vertices of $G$ so that any two adjacent vertices have different colours. Consider applying Theorem 9.12 to the chromatic number $\\\\chi(G)$ of a random graph $G \\\\sim G(N, 1/2)$. We could write $\\\\chi(G)$ as a function of $\\\\binom{N}{2}$ independent Bernoulli random variables, each one encoding the presence or absence of a given edge. Adding or removing a single edge cannot change the chromatic number by more than 1. This would give us a fluctuation bound on $\\\\chi(G)$ on the order of $N$ as $N \\\\to \\\\infty$. However, for large $N$ this is an extremely poor, in fact trivial, result, since $\\\\chi(G)$ itself is known to be on the order of $N/ \\\\log(N)$. We can do much better. For $2 \\\\leq k \\\\leq N$, let $X_k$ consist of a collection of $k - 1$ Bernoulli random variables, encoding the presence or absence of the $k - 1$ edges $\\\\{1, k\\\\}, \\\\{2, k\\\\}, \\\\ldots, \\\\{k - 1, k\\\\}$. It's still the case that $X_2, \\\\ldots, X_N$ are independent. All the information in $X_k$ concerns edges that intersect the vertex $k$; changing the status of any subset of these edges can only change the chromatic number by at most 1 (consider recolouring vertex $k$ as necessary). The Doob martingale from the proof of Theorem 9.12 involves revealing information about the graph vertex by vertex, rather than edge by edge, and is called the vertex exposure martingale. Applying the theorem with $n = N - 1$ and $c = 1$, we obtain $P(|\\\\chi(G) - E[\\\\chi(G)]| \\\\geq a) \\\\leq 2 \\\\exp\\\\left(-\\\\frac{a^2}{2(N - 1)}\\\\right)$, giving a concentration bound on the scale of $\\\\sqrt{N}$ for large $N$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 9.16",
        "name": "Khinchine/Kolmogorov",
        "topic": "The Law of the Iterated Logarithm",
        "previous_results": [],
        "statement": "Let $X_1, X_2, \\\\ldots$ be i.i.d. with mean 0 and variance 1. Let $S_n = \\\\sum_{i=1}^n X_i$, $n \\\\geq 1$. Then, almost surely, $\\\\limsup_{n\\\\to\\\\infty} \\\\frac{S_n}{\\\\sqrt{2n \\\\log \\\\log n}} = 1$, $\\\\liminf_{n\\\\to\\\\infty} \\\\frac{S_n}{\\\\sqrt{2n \\\\log \\\\log n}} = -1$.",
        "proof": ""
    },
    {
        "type": "theorem",
        "id": "Theorem 9.17",
        "name": "Strassen",
        "topic": "The Law of the Iterated Logarithm",
        "previous_results": [
            "Theorem 9.16"
        ],
        "statement": "We have $P(K = \\\\Gamma) = 1$.",
        "proof": ""
    },
    {
        "type": "lemma",
        "id": "Lemma 9.18",
        "name": "Measure Restriction Equality",
        "topic": "Radon-Nikodym Theorem and changes of measure",
        "previous_results": [
            "Theorem 4.15"
        ],
        "statement": "Let $(\\\\Omega, F, P)$ be a given probability space and $G$ a sub-$\\\\sigma$-algebra in $F$. Then $\\\\tilde{P} = P|_G$, the measure $P$ restricted to $G$, is a probability measure on $G$ and $E_P[X] = E_{\\\\tilde{P}}[X]$ for any $G$-measurable random variable $X$.",
        "proof": "This follows directly from Theorem 4.15, observing that $P \\\\circ X^{-1} = \\\\tilde{P} \\\\circ X^{-1}$ since $G$-measurability of $X$ precisely means that $X^{-1}(A)$ is in $G$, on which $P$ and $\\\\tilde{P}$ agree, for any Borel set $A$."
    },
    {
        "type": "lemma",
        "id": "Lemma 9.19",
        "name": "Absolute Continuity Consequence",
        "topic": "Radon-Nikodym Theorem and changes of measure",
        "previous_results": [
            "Lemma 3.19",
            "Lemma 3.18"
        ],
        "statement": "Suppose $Q \\\\ll P$ are two probability measures on $F$. Then $\\\\forall\\\\varepsilon > 0 \\\\exists\\\\delta > 0 \\\\forall A \\\\in F, P(A) < \\\\delta \\\\Rightarrow Q(A) < \\\\varepsilon$.",
        "proof": "Suppose this is not the case. Then for some $\\\\varepsilon > 0$, there exists a sequence of events $(A_n)$ such that $P(A_n) < 2^{-n}$ but $Q(A_n) \\\\geq \\\\varepsilon$, $n \\\\geq 1$. Let $B = \\\\limsup B_n = \\\\{B_n \\\\text{ i.o.}\\\\}$. Then Lemma 3.19 (BC1) instantly gives $P(B) = 0$ but reverse Fatou for sets, Lemma 3.18, gives $Q(B) \\\\geq \\\\limsup_{n\\\\to\\\\infty} Q(A_n) \\\\geq \\\\varepsilon$ which is a contradiction to $Q \\\\ll P$."
    },
    {
        "type": "theorem",
        "id": "Theorem 4.9",
        "name": "Radon-Nikodym Theorem",
        "topic": "Radon-Nikodym Theorem and changes of measure",
        "previous_results": [
            "Lemma 9.19",
            "Definition 5.17",
            "Theorem 8.32",
            "Lemma 1.12"
        ],
        "statement": "Let $\\\\mu, \\\\nu$ be two probability measures on a measurable space $(\\\\Omega, F)$. Then $\\\\nu \\\\ll \\\\mu$ if and only if there exists a non-negative random variable $f$ such that $\\\\nu(A) = \\\\int_A f d\\\\mu, A \\\\in F$. The function $f$ is often denoted $\\\\frac{d\\\\nu}{d\\\\mu}$ and is called the Radon-Nikodym derivative of $\\\\nu$ w.r.t. $\\\\mu$. Further, $\\\\nu \\\\sim \\\\mu$ if and only if $f > 0$ $\\\\mu$-a.s. (and then also $\\\\nu$-a.s.) in which case $\\\\frac{d\\\\mu}{d\\\\nu} = \\\\frac{1}{f}$.",
        "proof": "We write $\\\\nu = Q$ and $\\\\mu = P$ and divide by the total mass of $\\\\mu$, so that $P$ is a probability measure.\\n\\nStep 1: separable $\\\\sigma$-algebras.\\n\\nSuppose that $F = \\\\sigma(F_1, F_2, \\\\ldots)$ is generated by a countable sequence of sets. Note that we can replace $F_2$ by $F_2 \\\\cap F_1^c$, and $F_3$ by $F_3 \\\\cap (F_1 \\\\cup F_2)^c$, etc., and that we can always add $(\\\\cup_n F_n)^c$ to the list, so that with no loss of generality, we can assume that $F = \\\\sigma(A_1, A_2, \\\\ldots)$ for $(A_n)$ a partition of $\\\\Omega$. Let $F_n := \\\\sigma(A_1, \\\\ldots, A_n)$ and\\n\\n$Z_n = \\\\sum_{k=1}^n a_k 1_{A_k}$, where $a_k = \\\\frac{Q(A_k)}{P(A_k)}1_{P(A_k)>0}$.\\n\\nThen, by definition, $Z_n$ is $F_n$-measurable and integrable since $E[Z_n] = Q(\\\\Omega)$. Further, by direct computation, $E[Z_n1_A] = Q(A)$, for any $A \\\\in F_n$, noting such $A$ is in fact a finite union of atoms $(A_k)_{k\\\\leq n}$ or their complements. We conclude that $E[Z_{n+1}1_A] = E[Z_n1_A]$, $A \\\\in F_n$ and hence $(Z_n)$ is a martingale.\\n\\nNow, take $\\\\varepsilon > 0$ and $\\\\delta$ given via Lemma 9.19. Let $K > Q(\\\\Omega)/\\\\delta$. Markov's inequality, Lemma 5.6, gives\\n\\n$P(Z_n > K) \\\\leq \\\\frac{E[Z_n]}{K} = \\\\frac{Q(\\\\Omega)}{K} < \\\\delta, \\\\forall n \\\\geq 1$.\\n\\nAnd hence, we obtain\\n\\n$\\\\varepsilon > Q(Z_n > K) = E[Z_n1_{Z_n>K}], \\\\forall n \\\\geq 1$.\\n\\nThis, by Definition 5.17, ensures UI of $(Z_n)$ and hence, by Theorem 8.32, there exists a non-negative integrable $F$-measurable $Z$ such that $Z_n = E[Z | F_n]$. This instantly gives us $Q(A) = E[Z_n1_A] = E[Z1_A]$ for $A \\\\in F_n$ and hence $Q(A) = E[Z1_A]$ for $A \\\\in \\\\cup_{n\\\\geq 1} F_n$, which is a $\\\\pi$-system. It remains to invoke the $\\\\pi$-$\\\\lambda$ systems lemma, Lemma 1.12, to conclude that\\n\\n$Q(A) = E[Z1_A], A \\\\in F$,\\n\\ni.e. $Z = \\\\frac{dQ}{dP}$.\\n\\nStep 2: please see Williams' book."
    }
]
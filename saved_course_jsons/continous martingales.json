[
    {
        "type": "definition",
        "id": "Definition 2.1",
        "name": "Stochastic Process",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "A stochastic process, indexed by some set $T$, is a collection of random variables $\\\\{X_t\\\\}_{t\\\\in T}$, defined on a common probability space $(\\\\Omega, F, P)$ and taking values in a common state space $(E, \\\\mathcal{E})$.\\n\\nFor us, $T$ will generally be either $[0, \\\\infty)$ or $[0, T]$ and we think of $X_t$ as a random quantity that evolves with time.",
        "proof": "",
        "preconditions": [
            "stochastic process"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 2.2",
        "name": "Filtration",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "A collection $\\\\{F_t, t \\\\in [0, \\\\infty)\\\\}$ of $\\\\sigma$-algebras of sets in $F$ is a filtration if $F_t \\\\subseteq F_{t+s}$ for $t, s \\\\in [0, \\\\infty)$. (Intuitively, $F_t$ corresponds to the information known to an observer at time $t$.)\\n\\nIn particular, for a process $X$ we define $F^X_t = \\\\sigma(\\\\{X(s) : s \\\\leq t\\\\})$ (that is $F^X_t$ is the information obtained by observing $X$ up to time $t$) to be the natural filtration associated with the process $X$.",
        "proof": "",
        "preconditions": [
            "filtered probability space"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 2.3",
        "name": "Sample Path",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "The mapping $t \\\\mapsto X_t(\\\\omega)$ for a fixed $\\\\omega \\\\in \\\\Omega$, represents a realisation of our stochastic process, called a sample path or trajectory. We shall assume that\\n\\n$(t, \\\\omega) \\\\mapsto X_t(\\\\omega) : ([0, \\\\infty) \\\\times \\\\Omega, \\\\mathcal{B}([0, \\\\infty)) \\\\otimes F) \\\\mapsto (\\\\mathbb{R}, \\\\mathcal{B}(\\\\mathbb{R}))$\\n\\nis measurable (i.e. $\\\\forall A \\\\in \\\\mathcal{B}(\\\\mathbb{R})$, $\\\\{(t, \\\\omega) : X_t \\\\in A\\\\}$ is in the product $\\\\sigma$-algebra $\\\\mathcal{B}([0, \\\\infty)) \\\\otimes F$). Our stochastic process is then said to be measurable.",
        "proof": "",
        "preconditions": [
            "stochastic process",
            "measurable space with sigma-finite measure"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 2.4",
        "name": "Process Modifications",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "Let $X,Y$ be two stochastic processes defined on a common probability space $(\\\\Omega, F, P)$.\\n\\ni. We say that $X$ is a modification of $Y$ if, for all $t \\\\geq 0$, we have $X_t = Y_t$ a.s.;\\n\\nii. We say that $X$ and $Y$ are indistinguishable if\\n\\n$P(X_t = Y_t \\\\text{ for all } 0 \\\\leq t < \\\\infty) = 1,$\\n\\nor equivalently, $X_t(\\\\omega) = Y_t(\\\\omega)$ for all $t$, for all $\\\\omega \\\\not\\\\in N$ where $P(N) = 0$.",
        "proof": "",
        "preconditions": [
            "stochastic process"
        ]
    },
    {
        "type": "example",
        "id": "Example 2.5",
        "name": "Modification Example",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [
            "Definition 2.4"
        ],
        "statement": "Let $T \\\\sim U([0, 1])$ be a uniform random variable, and take the random process $X_t = 1_{t=T}$. Then $Y_t := 0$ is a modification of $X_t$, as $Y_t = X_t$ a.s. for each $t$. However, $Y$ and $X$ are not indistinguishable, as $X \\\\neq Y$ for some $t$ with positive probability (in fact, with probability 1).\\n\\nIf $X$ and $Y$ are modifications of one another then, in particular, they have the same finite dimensional distributions,\\n\\n$P [(X_{t_1}, \\\\ldots, X_{t_n}) \\\\in A] = P [(Y_{t_1}, \\\\ldots, Y_{t_n}) \\\\in A]$\\n\\nfor any finite collection of times $\\\\{t_1, t_2, \\\\ldots, t_n\\\\}$ for any $n$, and all measurable sets $A$, but indistinguishability is a much stronger property.",
        "proof": "",
        "preconditions": [
            "stochastic process"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 2.6",
        "name": "Cylinder Set",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "An $n$-dimensional cylinder set in $R^{[0,\\\\infty)}$ is a set of the form\\n\\n$C = \\\\{\\\\omega \\\\in R^{[0,\\\\infty)} : (\\\\omega(t_1), \\\\ldots, \\\\omega(t_n)) \\\\in A\\\\}$\\n\\nfor some $0 \\\\leq t_1 < t_2 \\\\ldots < t_n$ and $A \\\\in B(R^n)$.\\n\\nLet $C$ be the family of all finite-dimensional cylinder sets and $B(R^{[0,\\\\infty)})$ the $\\\\sigma$-algebra it generates. This is small enough to be able to build probability measures on $B(R^{[0,\\\\infty)})$ using Carath\u00e9odory's Theorem. On the other hand $B(R^{[0,\\\\infty)})$ only contains events which can be defined using at most countably many coordinates. In particular, the set\\n\\n$\\\\{\\\\omega \\\\in R^{[0,\\\\infty)} : \\\\omega(t) \\\\text{ is continuous}\\\\}$\\n\\nis not $B(R^{[0,\\\\infty)})$-measurable.",
        "proof": "",
        "preconditions": [
            "measurable space with sigma-finite measure"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 2.7",
        "name": "Consistent Distributions",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [
            "Definition 2.6"
        ],
        "statement": "A family of finite dimensional distributions is called consistent if for any $t = (t_1,t_2, \\\\ldots,t_n) \\\\in T$ and $1 \\\\leq j \\\\leq n$\\n\\n$P_t(A_1 \\\\times A_2 \\\\times \\\\ldots \\\\times A_{j-1} \\\\times R \\\\times A_{j+1} \\\\times \\\\ldots \\\\times A_n)$\\n\\n$= P_s(A_1 \\\\times A_2 \\\\times \\\\ldots \\\\times A_{j-1} \\\\times A_{j+1} \\\\times \\\\ldots \\\\times A_n)$\\n\\nwhere $A_i \\\\in B(R)$ and $s := (t_1,t_2, \\\\ldots,t_{j-1},t_{j+1}, \\\\ldots,t_n)$.\\n\\n(In other words, if we integrate out over the distribution at the $j$th time point then we recover the corresponding marginal for the remaining lower dimensional vector.)",
        "proof": "",
        "preconditions": [
            "measurable space with sigma-finite measure"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 2.8",
        "name": "Daniell\u2013Kolmogorov Extension Theorem",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [
            "Definition 2.6",
            "Definition 2.7"
        ],
        "statement": "Let $\\\\{P_t : t \\\\in T\\\\}$ be a consistent family of finite-dimensional distributions. Then there exists a probability measure $P$ on $(R^{[0,\\\\infty)}, B(R^{[0,\\\\infty)}))$ such that for any $n$, $t = (t_1, \\\\ldots,t_n) \\\\in T$ and $A \\\\in B(R^n)$,\\n\\n$P_t(A) = P[\\\\{\\\\omega \\\\in R^{[0,\\\\infty)} : (\\\\omega(t_1), \\\\ldots, \\\\omega(t_n)) \\\\in A\\\\}]$.",
        "proof": "We will not prove this here, but notice that the equation defines $P$ on the cylinder sets and so if we can establish countable additivity then the proof reduces to an application of Carath\u00e9odory's extension theorem. Uniqueness is a consequence of the Monotone Class Lemma.",
        "preconditions": [
            "measurable space with sigma-finite measure"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 2.9",
        "name": "Kolmogorov\u2013\u010centsov continuity criterion",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [
            "Theorem 2.8"
        ],
        "statement": "Suppose that a stochastic process $(X_t : t \\\\leq T)$ defined on $(\\\\Omega, F, P)$ satisfies\\n\\n$E[|X_t - X_s|^\\\\alpha] \\\\leq C|t - s|^{1+\\\\beta}$, $0 \\\\leq s,t \\\\leq T$\\n\\nfor some strictly positive constants $\\\\alpha$, $\\\\beta$ and $C$.\\nThen there exists $\\\\tilde{X}$, a modification of $X$, whose paths are $\\\\gamma$-locally H\u00f6lder continuous $\\\\forall\\\\gamma \\\\in (0, \\\\beta/\\\\alpha)$ a.s., i.e.\\n\\n$\\\\sup_{s,t,\\\\in[0,T]} \\\\frac{|\\\\tilde{X}_t - \\\\tilde{X}_s|}{|t - s|^\\\\gamma} < \\\\infty$ a.s.\\n\\nIn particular, the sample paths of $\\\\tilde{X}$ are a.s. continuous (and uniformly continuous on $[0, T]$).",
        "proof": "See appendix (not examinable)",
        "preconditions": [
            "stochastic process"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 3.1",
        "name": "Symmetric Simple Random Walk",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "The discrete time stochastic process $\\\\{S_n\\\\}_{n\\\\geq0}$ is a symmetric simple random walk under the measure $P$ if $S_n = \\\\sum_{i=1}^n \\\\xi_i$, where the $\\\\xi_i$ can take only the values $\\\\pm1$, and are i.i.d. under $P$ with $P[\\\\xi_i = -1] = 1/2 = P[\\\\xi_i = 1]$.",
        "proof": "",
        "preconditions": [
            "stochastic process"
        ]
    },
    {
        "type": "lemma",
        "id": "Lemma 3.2",
        "name": "Random Walk Martingale",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "\\\\{S_n\\\\}_{n\\\\geq 0} is a P-martingale (with respect to the natural filtration) and $\\\\text{cov}(S_n, S_m) = n \\\\wedge m$.",
        "proof": "This is stated without proof in the text.",
        "preconditions": [
            "martingale"
        ]
    },
    {
        "type": "lemma",
        "id": "Lemma 3.5",
        "name": "Normal Distribution Properties",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "(i) Let $Z, Z'$ be independent random variables with $Z \\\\sim N(\\\\mu, \\\\Sigma)$, $Z' \\\\sim N(\\\\mu', \\\\Sigma')$. Then $Z + Z' \\\\sim N(\\\\mu + \\\\mu', \\\\Sigma + \\\\Sigma')$. Equivalently, their densities satisfy the convolution property\\n\\n$\\\\int_{\\\\mathbb{R}^d} \\\\phi_{(\\\\mu,\\\\Sigma)}(y)\\\\phi_{(\\\\mu',\\\\Sigma')}(x - y)dy = \\\\phi_{(\\\\mu+\\\\mu',\\\\Sigma+\\\\Sigma')}(x)$.\\n\\n(ii) If $Z_i \\\\sim N(\\\\mu_i, \\\\Sigma_i)$ is a sequence of independent normal random variables such that $\\\\mu^* = \\\\sum_{i\\\\in\\\\mathbb{N}} \\\\mu_i$ and $\\\\Sigma^* = \\\\sum_{i\\\\in\\\\mathbb{N}} \\\\Sigma_i$ exist (i.e. the sums converge), then the sequence of partial sums $\\\\sum_{i=1}^n Z_i$ converges in ($L^2$, and hence in) probability to a random variable with distribution\\n\\n$\\\\sum_{i\\\\in\\\\mathbb{N}} Z_i \\\\sim N(\\\\mu^*, \\\\Sigma^*)$.\\n\\n(iii) If the pair $(Z, Z')$ is a multivariate normal random variable, then $Z$ and $Z'$ are normal, and are independent if and only if their covariance is zero, that is, $E[(Z - \\\\mu)(Z' - \\\\mu')^\\\\top] = 0$.",
        "proof": "The proof is left as an exercise (see Appendix B).",
        "preconditions": [
            "Gaussian vector"
        ]
    },
    {
        "type": "lemma",
        "id": "Lemma 3.6",
        "name": "Continuity of Limit",
        "topic": "Brownian Motion",
        "previous_results": [
            "Lemma A.7"
        ],
        "statement": "Let $\\\\{X^n\\\\}_{n\\\\in\\\\mathbb{N}}$ be a sequence of a.s. continuous functions which converge uniformly on compacts in probability to a process $X$, that is, for any $\\\\varepsilon > 0$,\\n\\n$P\\\\left(\\\\lim_n \\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X_s\\\\| < \\\\varepsilon\\\\right) = 1$\\n\\nfor all $t$. Then $X$ is also continuous.",
        "proof": "For fixed $t$, by Lemma A.7, taking a subsequence in $n$, we can assume that the convergence is almost sure, that is,\\n\\n$P\\\\left(\\\\lim_n \\\\left(\\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X_s\\\\|\\\\right) = 0\\\\right) = 1$\\n\\nFixing $\\\\omega$, this is a statement of uniform convergence of $X^{n_j} \\\\to X$, and the continuity of the limit is classical, as for any $\\\\varepsilon > 0$, we can find $\\\\delta, m > 0$ such that\\n\\n$\\\\|X_s - X_{s+\\\\delta}\\\\| \\\\leq \\\\|X^{n_m}_s - X_s\\\\| + \\\\|X^{n_m}_{s+\\\\delta} - X_{s+\\\\delta}\\\\| + \\\\|X^{n_m}_s - X^{n_m}_{s+\\\\delta}\\\\|$\\n$\\\\leq 2\\\\sup_{s\\\\in[0,t]} \\\\{\\\\|X^{n_m}_s - X_s\\\\|\\\\} + \\\\|X^{n_m}_s - X^{n_m}_{s+\\\\delta}\\\\|$\\n$\\\\leq 3\\\\varepsilon$.",
        "preconditions": [
            "continuous stochastic process",
            "sequence converging in probability"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 3.8",
        "name": "L\u00e9vy's Construction Convergence",
        "topic": "Brownian Motion",
        "previous_results": [
            "Lemma 3.6"
        ],
        "statement": "The processes $X^n$ defined in (4) converge a.s. uniformly on compacts to a process $X$. In its natural filtration, the limit is a Brownian motion starting at zero.",
        "proof": "We first show that the processes converge. We consider the case where $X$ is a Brownian motion in two dimensions, as this implies all other cases by the triangle inequality, and is notationally simpler. From our construction, we can see that\\n\\n$$\\\\|X^n_s - X^{n+1}_s\\\\| = \\\\sup_{s\\\\in[0,t]} \\\\max_{\\\\{s\\\\in D_{n+1}\\\\backslash D_n:s<t\\\\}} \\\\|2^{-(n/2+1)}Z_s\\\\|.$$\\n\\nThe set $\\\\{s \\\\in D_{n+1} \\\\backslash D_n : s < t\\\\}$ contains at most $t2^n$ elements, and the $Z_s$ are independent $N(0, I_d)$ random variables. It is standard that $\\\\|Z_s\\\\|^2$ has a $\\\\chi^2$-distribution with $d = 2$ degrees of freedom, so if $F(x) := P(\\\\|Z_s\\\\|^2 \\\\leq x)$ is the distribution function of $\\\\|Z_s\\\\|^2$ we have\\n\\n$$P\\\\left( \\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X^{n+1}_s\\\\| > \\\\varepsilon\\\\right) = P\\\\left(\\\\max_{\\\\{s\\\\in D_{n+1}\\\\backslash D_n:s<t\\\\}} \\\\|Z_s\\\\| > 2^{n/2+1}\\\\varepsilon\\\\right)$$\\n\\n$$\\\\leq \\\\sum_{\\\\{s\\\\in D_{n+1}\\\\backslash D_n, s<t\\\\}} P\\\\left(\\\\|Z_s\\\\| > 2^{n/2+1}\\\\varepsilon\\\\right) = t2^n(1 - F(2^{n+2}\\\\varepsilon^2)).$$\\n\\nBy changing into polar coordinates, it is easy to show that $F(x) = 1 - e^{-x/2}$ (this simple form is the reason we chose $d = 2$). Therefore,\\n\\n$$P\\\\left( \\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X^{n+1}_s\\\\| > \\\\varepsilon\\\\right) \\\\leq t2^n \\\\exp(-2^{n+1}\\\\varepsilon^2).$$\\n\\nIn particular,\\n\\n$$P\\\\left( \\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X^{n+1}_s\\\\| > n^{-3}\\\\right) \\\\leq t2^n \\\\exp(-2^{n+1}n^{-6}).$$\\n\\nTaking $N$ large enough that $N \\\\log(2) - 2^{N+1}N^{-6} < -N$, for all $n > N$ we have\\n\\n$$P\\\\left( \\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X^{n+1}_s\\\\| > n^{-3}\\\\right) \\\\leq te^{-n}.$$\\n\\nBy the Borel\u2013Cantelli Lemma, as this sequence is summable we have\\n\\n$$P\\\\left( \\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X^{n+1}_s\\\\| > n^{-3} \\\\text{ for infinitely many } n\\\\right) = 0.$$\\n\\nIn particular, with probability one, taking $N$ sufficiently large, for all $n \\\\geq N$,\\n\\n$$\\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X^{n+1}_s\\\\| \\\\leq n^{-3}$$\\n\\nand by the triangle inequality, recalling that $\\\\sum_n n^{-2} = \\\\pi^2/6$, for $N < n < m$,\\n\\n$$\\\\sup_{s\\\\in[0,t]} \\\\|X^n_s - X^m_s\\\\| \\\\leq \\\\left(\\\\sum_{j=n}^{m-1} \\\\sup_{s\\\\in[0,t]} \\\\|X^j_s - X^{j+1}_s\\\\|\\\\right) \\\\leq \\\\frac{\\\\pi^2}{6n}.$$\\n\\nTherefore, with probability one, the processes $X^n$ are converging uniformly on the interval $[0,t]$. By Lemma 3.6, $X$ is a continuous process.\\n\\nWe now need to show that $X$ is a Brownian motion in its natural filtration, that is, that the increment $X_t - X_s$ is normally distributed and independent of $\\\\mathcal{F}_s = \\\\sigma(X_u, u \\\\leq s)$. First note that for $s,t$ with $t \\\\in D_n \\\\backslash D_{n+1}$ and $\\\\lceil s \\\\rceil_n < t$, the random variable $Z_t$ is not involved in the construction of $X_s$. Hence, as $X$ generates the filtration and the $\\\\{Z_u\\\\}_{u\\\\in\\\\cup_n D_n}$ are independent, we see that $Z_t$ is independent of $\\\\mathcal{F}_s$.\\n\\nIt is clear that if $s,t$ are integers with $s < t$, then\\n\\n$$X_t - X_s = X^0_t - X^0_s = \\\\sum_{\\\\{k\\\\in D_0:s<k<t\\\\}} Z_k \\\\sim N(0, (t - s)I_d).$$\\n\\nFurthermore, in this case $X_t - X_s$ is independent of $\\\\mathcal{F}_s$, as $Z_k = Z_{\\\\lceil k \\\\rceil_0}$ is independent of $\\\\mathcal{F}_s$ for all $s < k$.\\n\\nNow suppose that the result holds for $s,t \\\\in D_n$. Then we see that for any $u \\\\in D_{n+1} \\\\backslash D_n$,\\n\\n$$X_u - X_{\\\\lfloor u \\\\rfloor_n} = \\\\frac{X_{\\\\lceil u \\\\rceil_n} - X_{\\\\lfloor u \\\\rfloor_n}}{2} + 2^{-(n/2+1)}Z_u \\\\sim N(0, 2^{-(n+1)}I_d)$$\\n\\nwhich is independent of $\\\\mathcal{F}_{\\\\lfloor u \\\\rfloor_n}$. Similarly,\\n\\n$$X_{\\\\lceil u \\\\rceil_n} - X_u = \\\\frac{X_{\\\\lceil u \\\\rceil_n} - X_{\\\\lfloor u \\\\rfloor_n}}{2} - 2^{-(n/2+1)}Z_u \\\\sim N(0, 2^{-(n+1)}I_d),$$\\n\\nwhich is independent of $\\\\mathcal{F}_{\\\\lfloor u \\\\rfloor_n}$. Therefore, for any $s,t \\\\in D_{n+1}$,\\n\\n$$X_t - X_s = (X_t - X_{\\\\lfloor t \\\\rfloor_n}) + (X_{\\\\lfloor t \\\\rfloor_n} - X_{\\\\lceil s \\\\rceil_n}) + (X_{\\\\lceil s \\\\rceil_n} - X_s),$$\\n\\nwhich is the sum of three independent normal random variables, so\\n\\n$$X_t - X_s \\\\sim N(0, (t - s)I_d).$$\\n\\nThe first two terms are independent of $\\\\mathcal{F}_{\\\\lceil s \\\\rceil_n} \\\\supseteq \\\\mathcal{F}_s$. We know the last term is independent of $\\\\mathcal{F}_{\\\\lfloor s \\\\rfloor_n}$, and we can compute\\n\\n$$E[(X_{\\\\lceil s \\\\rceil_n} - X_s)(X_s - X_{\\\\lfloor s \\\\rfloor_n})^\\\\top] = 0$$\\n\\nso $(X_{\\\\lceil s \\\\rceil_n} - X_s)$ is independent of the increment $X_s - X_{\\\\lfloor s \\\\rfloor_n}$, as uncorrelated Gaussians are independent. As we can write\\n\\n$$\\\\mathcal{F}_s = \\\\mathcal{F}_{\\\\lfloor s \\\\rfloor_n} \\\\vee \\\\sigma(X_s - X_{\\\\lfloor s \\\\rfloor_n}) \\\\vee \\\\sigma(Z_u; u \\\\in (\\\\lfloor s \\\\rfloor_n, s)),$$\\n\\nwe see that $X_{\\\\lceil s \\\\rceil_n} - X_s$ is independent of $\\\\mathcal{F}_s$. Therefore $X_t - X_s$ is normally distributed and independent of $\\\\mathcal{F}_s$, as desired.\\n\\nFinally, for any $s < t$ we can find sequences $s_n \\\\downarrow s$, $t_n \\\\uparrow t$ with $s_n,t_n \\\\in D_n$ and $s_0 \\\\leq t_0$. Then $X_{t_n} - X_{s_n} \\\\sim N(0, (t_n - s_n)I_d)$, and by continuity of $X$ we see\\n\\n$$X_t - X_s = X_{t_0} - X_{s_0} + \\\\sum_{n=1}^{\\\\infty} (X_{t_n} - X_{t_{n-1}} - X_{s_n} + X_{s_{n-1}}) \\\\sim N(0, (t - s)I_d).$$\\n\\nAll the terms in this sum are independent of $\\\\mathcal{F}_s$, as required. As $X_0 = 0$ by construction, we see that $X$ is a Brownian motion starting at zero, in its natural filtration.",
        "preconditions": [
            "stochastic process",
            "continuous paths",
            "right-continuous filtration",
            "complete filtration",
            "Brownian motion"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 3.9",
        "name": "Wiener Measure",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "The Wiener measure $W$ is the image of $P$ under the mapping in (5); it is the probability measure on the space of continuous functions such that the canonical process, i.e. $(B_t(\\\\omega) = \\\\omega(t), t \\\\geq 0)$, is a Brownian motion.\\n\\nIn other words, $W$ is the unique probability measure on $(C(R_+, R), \\\\mathcal{B}(C(R_+, R)))$ such that\\n\\ni. $W(\\\\{\\\\omega \\\\in C(R_+, R), \\\\omega(0) = 0\\\\}) = 1$;\\n\\nii. for any $n \\\\geq 1$, $\\\\forall 0 = t_0 < t_1 < \\\\ldots < t_n$, $A \\\\in \\\\mathcal{B}(R^n)$\\n\\n$W(\\\\{\\\\omega \\\\in C(R_+, R) : (\\\\omega(t_1), \\\\ldots, \\\\omega(t_n)) \\\\in A\\\\}) = \\\\frac{1}{(2\\\\pi)^{n/2}\\\\sqrt{t_1(t_2 - t_1) \\\\ldots (t_n - t_{n-1})}} \\\\int_A \\\\exp\\\\left(-\\\\sum_{i=1}^n \\\\frac{(y_i - y_{i-1})^2}{2(t_i - t_{i-1})}\\\\right) dy_1 \\\\cdot \\\\cdot \\\\cdot dy_n$,\\n\\nwhere $y_0 := 0$.",
        "proof": "",
        "preconditions": [
            "Brownian motion",
            "continuous paths",
            "Gaussian process"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 3.10",
        "name": "Brownian Motion Properties",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "Let $B$ be a standard real-valued Brownian motion. Then\\n\\ni. $-B_t$ is also a Brownian motion, (symmetry)\\n\\nii. $\\\\forall c \\\\geq 0$, $cB_{t/c^2}$ is a Brownian motion, (scaling)\\n\\niii. $X_0 = 0$, $X_t := tB_{1/t}$ is a Brownian motion, (time inversion)\\n\\niv. for $t \\\\in [0, 1]$, $X_t := B_1 - B_{1-t}$ is a Brownian motion, (time reversal)\\n\\nv. $\\\\forall s \\\\geq 0$, $\\\\tilde{B}_t = B_{t+s} - B_s$ is a Brownian motion independent of $\\\\sigma(B_u : u \\\\leq s)$, (simple Markov property).",
        "proof": "",
        "preconditions": [
            "Brownian motion"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 3.11",
        "name": "Variation of Function",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "Let $\\\\pi$ be a partition of $[0, T]$, $N(\\\\pi)$ the number of intervals that make up $\\\\pi$ and $\\\\|\\\\pi\\\\|$ be the mesh of $\\\\pi$ (that is the length of the longest interval in the partition). Write $0 = t_0 < t_1 < \\\\ldots < t_{N(\\\\pi)} = T$ for the endpoints of the intervals of the partition. Then the variation of a function $f : [0, T] \\\\to R$ is\\n\\n$$\\\\lim_{\\\\delta \\\\to 0} \\\\sup_{\\\\pi:\\\\|\\\\pi\\\\|=\\\\delta} \\\\sum_{j=1}^{N(\\\\pi)} |f(t_j) - f(t_{j-1})|.$$",
        "proof": "",
        "preconditions": [
            "function on a closed interval"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 3.12",
        "name": "p-variation of Function",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "In the notation of Definition 3.11, the $p$-variation of a function $f : [0, T] \\\\to R$ is defined as\\n\\n$$\\\\lim_{\\\\delta \\\\to 0} \\\\sup_{\\\\pi:\\\\|\\\\pi\\\\|=\\\\delta} \\\\sum_{j=1}^{N(\\\\pi)} |f(t_j) - f(t_{j-1})|^p.$$",
        "proof": "",
        "preconditions": [
            "function on a closed interval"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 3.13",
        "name": "Quadratic Variation",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "Let $B_t$ denote Brownian motion under $P$ and for a partition $\\\\pi$ of $[0, T]$ define\\n\\n$S(\\\\pi) = \\\\sum_{j=1}^{N(\\\\pi)} |B_{t_j} - B_{t_{j-1}}|^2$.\\n\\nLet $\\\\pi_n$ be a sequence of partitions with $\\\\|\\\\pi_n\\\\| \\\\to 0$. Then\\n\\n$E[|S(\\\\pi_n) - T|^2] \\\\to 0$ as $n \\\\to \\\\infty$.\\n\\nWe say that the quadratic variation process of Brownian motion, which we denote by $\\\\{\\\\langle B\\\\rangle_t\\\\}_{t\\\\geq 0}$ is $\\\\langle B\\\\rangle_t = t$.",
        "proof": "We expand the expression inside the expectation and make use of our knowledge of the normal distribution. Let $\\\\{t_{n,j}\\\\}_{j=0}^{N(\\\\pi_n)}$ denote the endpoints of the intervals that make up the partition $\\\\pi_n$. First observe that\\n\\n$|S(\\\\pi_n) - T|^2 = \\\\left|\\\\sum_{j=1}^{N(\\\\pi_n)} \\\\left(|B_{t_{n,j}} - B_{t_{n,j-1}}|^2 - (t_{n,j} - t_{n,j-1})\\\\right)\\\\right|^2$.\\n\\nIt is convenient to write $\\\\delta_{n,j}$ for $|B_{t_{n,j}} - B_{t_{n,j-1}}|^2 - (t_{n,j} - t_{n,j-1})$. Then\\n\\n$|S(\\\\pi_n) - T|^2 = \\\\sum_{j=1}^{N(\\\\pi_n)} \\\\left(\\\\delta_{n,j}^2 + 2\\\\sum_{k>j} \\\\delta_{n,j}\\\\delta_{n,k}\\\\right)$.\\n\\nNote that since Brownian motion has independent increments,\\n\\n$E[\\\\delta_{n,j}\\\\delta_{n,k}] = E[\\\\delta_{n,j}]E[\\\\delta_{n,k}] = 0$ if $j \\\\neq k$.\\n\\nAlso\\n\\n$E[\\\\delta_{n,j}^2] = E\\\\left[|B_{t_{n,j}} - B_{t_{n,j-1}}|^4 - 2|B_{t_{n,j}} - B_{t_{n,j-1}}|^2(t_{n,j} - t_{n,j-1}) + (t_{n,j} - t_{n,j-1})^2\\\\right]$.\\n\\nFor a normally distributed random variable, $X$, with mean zero and variance $\\\\lambda$, $E[|X|^4] = 3\\\\lambda^2$, so we have\\n\\n$E[\\\\delta_{n,j}^2] = 3(t_{n,j} - t_{n,j-1})^2 - 2(t_{n,j} - t_{n,j-1})^2 + (t_{n,j} - t_{n,j-1})^2 = 2(t_{n,j} - t_{n,j-1})^2 \\\\leq 2\\\\|\\\\pi_n\\\\|(t_{n,j} - t_{n,j-1})$.\\n\\nSumming over $j$\\n\\n$E[|S(\\\\pi_n) - T|^2] \\\\leq 2\\\\sum_{j=1}^{N(\\\\pi_n)} \\\\|\\\\pi_n\\\\|(t_{n,j} - t_{n,j-1}) = 2\\\\|\\\\pi_n\\\\|T \\\\to 0$ as $n \\\\to \\\\infty$.",
        "preconditions": [
            "Brownian motion",
            "partition with mesh tending to zero",
            "quadratic variation"
        ]
    },
    {
        "type": "corollary",
        "id": "Corollary 3.14",
        "name": "Infinite Variation",
        "topic": "Brownian Motion",
        "previous_results": [
            "Theorem 3.13"
        ],
        "statement": "Brownian sample paths are of infinite variation on any interval almost surely.",
        "proof": "This is stated as an exercise in the text, but follows from Theorem 3.13. If Brownian motion had finite variation on some interval $[0,T]$, then the quadratic variation would be zero. However, Theorem 3.13 establishes that the quadratic variation of Brownian motion on $[0,T]$ is $T > 0$ almost surely, which gives a contradiction.",
        "preconditions": [
            "Brownian motion",
            "process of finite variation"
        ]
    },
    {
        "type": "corollary",
        "id": "Corollary 3.15",
        "name": "H\u00f6lder Continuity",
        "topic": "Brownian Motion",
        "previous_results": [
            "Theorem 3.13"
        ],
        "statement": "Brownian sample paths are almost surely nowhere locally H\u00f6lder continuous of order $\\\\gamma > \\\\frac{1}{2}$.",
        "proof": "This is stated as an exercise in the text. If Brownian motion were H\u00f6lder continuous of order $\\\\gamma > \\\\frac{1}{2}$ on some interval, then for some constant $C$, we would have $|B_t - B_s| \\\\leq C|t-s|^\\\\gamma$ for all $s,t$ in that interval. This would imply that the quadratic variation would be bounded by $\\\\sum_j C^2|t_j-t_{j-1}|^{2\\\\gamma} \\\\leq C^2\\\\|\\\\pi\\\\|^{2\\\\gamma-1}T$, which tends to zero as $\\\\|\\\\pi\\\\| \\\\to 0$ when $\\\\gamma > \\\\frac{1}{2}$. This contradicts Theorem 3.13, which establishes that the quadratic variation is $T > 0$.",
        "preconditions": [
            "Brownian motion",
            "continuous paths"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 3.16",
        "name": "L\u00e9vy's Modulus",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "For $B$ a Brownian motion,\\n\\n$\\\\lim\\\\sup_{\\\\varepsilon\\\\downarrow 0} \\\\sup_{0\\\\leq s<t\\\\leq 1,t-s\\\\leq\\\\varepsilon} \\\\frac{|B_t - B_s|}{\\\\sqrt{2\\\\varepsilon \\\\log(1/\\\\varepsilon)}} = 1$ a.s.\\n\\nConsequently, Brownian sample paths are almost surely nowhere locally H\u00f6lder continuous of order $\\\\gamma = 1/2$, and the 2-variation is almost surely infinite.",
        "proof": "Omitted (proof is a careful calculation with estimates of normal random variables, see, for example, Revuz & Yor, p30ff)",
        "preconditions": [
            "Brownian motion"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 3.17",
        "name": "Blumenthal's 0-1 Law",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "Fix a Brownian motion $(B_t)_{t\\\\geq 0}$ on $(\\\\Omega, \\\\mathcal{F}, P)$. Recall $B_0 = 0$. For every $t \\\\geq 0$ we set $\\\\mathcal{F}_t := \\\\sigma(B_u : u \\\\leq t)$, so that $\\\\mathcal{F}_s \\\\subset \\\\mathcal{F}_t$ if $s \\\\leq t$. We also set $\\\\mathcal{F}_{0+} := \\\\cap_{s>0}\\\\mathcal{F}_s$. Then the $\\\\sigma$-field $\\\\mathcal{F}_{0+}$ is trivial in the sense that $P[A] = 0$ or $1$ for every $A \\\\in \\\\mathcal{F}_{0+}$.",
        "proof": "Let $0 < t_1 < t_2 \\\\cdots < t_k$ and let $g : \\\\mathbb{R}^k \\\\to \\\\mathbb{R}$ be a bounded continuous function. Also, fix $A \\\\in \\\\mathcal{F}_{0+}$. Then by continuity and dominated convergence\\n\\n$E[1_A g(B_{t_1}, \\\\ldots, B_{t_k})] = \\\\lim_{\\\\varepsilon\\\\downarrow 0} E[1_A g(B_{t_1} - B_{\\\\varepsilon}, \\\\ldots, B_{t_k} - B_{\\\\varepsilon})]$.\\n\\nIf $0 < \\\\varepsilon < t_1$, the variables $B_{t_1} - B_{\\\\varepsilon}, \\\\ldots, B_{t_k} - B_{\\\\varepsilon}$ are independent of $\\\\mathcal{F}_{\\\\varepsilon}$ (by the Markov property) and thus also of $\\\\mathcal{F}_{0+}$. It follows that\\n\\n$E[1_A g(B_{t_1}, \\\\ldots, B_{t_k})] = \\\\lim_{\\\\varepsilon\\\\downarrow 0} E[1_A g(B_{t_1} - B_{\\\\varepsilon}, \\\\ldots, B_{t_k} - B_{\\\\varepsilon})] = P[A]E[g(B_{t_1}, \\\\ldots, B_{t_k})]$.\\n\\nWe have thus obtained that $\\\\mathcal{F}_{0+}$ is independent of $\\\\sigma(B_{t_1}, \\\\ldots, B_{t_k})$. Since this holds for any finite collection $\\\\{t_1, \\\\ldots, t_k\\\\}$ of (strictly) positive reals, $\\\\mathcal{F}_{0+}$ is independent of $\\\\sigma(B_t, t > 0)$. However, $\\\\sigma(B_t, t > 0) = \\\\sigma(B_t, t \\\\geq 0)$, since $B_0$ is the pointwise limit of $B_t$ when $t \\\\to 0$. Since $\\\\mathcal{F}_{0+} \\\\subset \\\\sigma(B_t, t \\\\geq 0)$, we conclude that $\\\\mathcal{F}_{0+}$ is independent of itself and so must be trivial.",
        "preconditions": [
            "Brownian motion",
            "filtered probability space"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 3.18",
        "name": "Brownian Path Properties",
        "topic": "Brownian Motion",
        "previous_results": [
            "Theorem 3.17"
        ],
        "statement": "Let $B$ be a standard real-valued Brownian motion, as above.\\n\\ni. Then, a.s., for every $\\\\varepsilon > 0$,\\n\\n$\\\\sup_{0\\\\leq s\\\\leq \\\\varepsilon} B_s > 0$ and $\\\\inf_{0\\\\leq s\\\\leq \\\\varepsilon} B_s < 0$.\\n\\nIn particular, $\\\\inf\\\\{t > 0 : B_t = 0\\\\} = 0$ a.s.\\n\\nii. For every $a \\\\in \\\\mathbb{R}$, let $T_a := \\\\inf\\\\{t \\\\geq 0 : B_t = a\\\\}$ (with the convention that $\\\\inf \\\\emptyset = \\\\infty$). Then a.s. for each $a \\\\in \\\\mathbb{R}$, $T_a < \\\\infty$. Consequently, we have a.s.\\n\\n$\\\\limsup_{t\\\\to\\\\infty} B_t = +\\\\infty$, $\\\\liminf_{t\\\\to\\\\infty} B_t = -\\\\infty$.",
        "proof": "(i) Let $\\\\varepsilon_p$ be a sequence of strictly positive reals decreasing to zero and set $A := \\\\cup_{p\\\\geq 0}\\\\{\\\\sup_{0\\\\leq s\\\\leq \\\\varepsilon_p} B_s > 0\\\\}$. Since this is a monotone decreasing intersection, $A \\\\in \\\\mathcal{F}_{0+}$. On the other hand, by monotonicity,\\n\\n$P[A] = \\\\lim_{p\\\\to\\\\infty} \\\\downarrow P[\\\\sup_{0\\\\leq s\\\\leq \\\\varepsilon_p} B_s > 0]$,\\n\\nwhere $\\\\lim\\\\downarrow$ denotes a decreasing limit, and\\n\\n$P[\\\\sup_{0\\\\leq s\\\\leq \\\\varepsilon_p} B_s > 0] \\\\geq P[B_{\\\\varepsilon_p} > 0] = \\\\frac{1}{2}$.\\n\\nSo $P[A] \\\\geq 1/2$ and by Blumenthal's 0-1 law $P[A] = 1$. Hence a.s. for all $\\\\varepsilon > 0$, $\\\\sup_{0\\\\leq s\\\\leq \\\\varepsilon} B_s > 0$. Replacing $B$ by $-B$ we obtain $P[\\\\inf_{0\\\\leq s\\\\leq \\\\varepsilon} B_s < 0] = 1$.\\n\\n(ii) Write\\n\\n$1 = P[\\\\sup_{0\\\\leq s\\\\leq 1} B_s > 0] = \\\\lim_{\\\\delta \\\\downarrow 0} \\\\uparrow P[\\\\sup_{0\\\\leq s\\\\leq 1} B_s > \\\\delta]$.\\n\\nNow, writing $c = 1/\\\\delta$ in the Brownian scaling of Proposition 3.10 ii, we have that $B^{\\\\delta}_t = \\\\delta^{-1} B_{t\\\\delta^2}$ is a Brownian motion. Thus for any $\\\\delta > 0$,\\n\\n$P[\\\\sup_{0\\\\leq s\\\\leq 1} B_s > \\\\delta] = P[\\\\sup_{0\\\\leq s\\\\leq 1/\\\\delta^2} B^{\\\\delta}_s > 1] = P[\\\\sup_{0\\\\leq s\\\\leq 1/\\\\delta^2} B_s > 1]$.\\n\\nIf we let $\\\\delta \\\\downarrow 0$, we find\\n\\n$P[\\\\sup_{s\\\\geq 0} B_s > 1] = \\\\lim_{\\\\delta \\\\downarrow 0} P[\\\\sup_{0\\\\leq s\\\\leq 1/\\\\delta^2} B_s > 1] = \\\\lim_{\\\\delta \\\\downarrow 0} \\\\uparrow P[\\\\sup_{0\\\\leq s\\\\leq 1} B_s > \\\\delta] = 1$.\\n\\nAnother scaling argument shows that, for every $M > 0$,\\n\\n$P[\\\\sup_{s\\\\geq 0} B_s > M] = 1$\\n\\nand replacing $B$ with $-B$,\\n\\n$P[\\\\inf_{s\\\\geq 0} B_s < -M] = 1$.\\n\\nContinuity of sample paths completes the proof of (ii).",
        "preconditions": [
            "Brownian motion",
            "stopping time"
        ]
    },
    {
        "type": "corollary",
        "id": "Corollary 3.20",
        "name": "Non-monotone Brownian paths",
        "topic": "Brownian Motion",
        "previous_results": [
            "Proposition 3.18"
        ],
        "statement": "The map $t \\\\mapsto B_t$ is a.s. not monotone on any non-trivial interval.",
        "proof": "This follows directly from Proposition 3.18. Since for every $\\\\varepsilon > 0$, we have both $\\\\sup_{0 \\\\leq s \\\\leq \\\\varepsilon} B_s > 0$ and $\\\\inf_{0 \\\\leq s \\\\leq \\\\varepsilon} B_s < 0$ almost surely, the path of Brownian motion must oscillate between positive and negative values in any interval $[0,\\\\varepsilon]$. This property holds for any interval by the time-homogeneity of Brownian motion. Therefore, Brownian motion cannot be monotone on any non-trivial interval almost surely.",
        "preconditions": [
            "Brownian motion"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 4.1",
        "name": "Right continuous filtration",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "We say that $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq0}$ is right continuous if for each $t \\\\geq 0$,\\n\\n$\\\\mathcal{F}_t = \\\\mathcal{F}_{t+} \\\\equiv \\\\cap_{\\\\varepsilon>0}\\\\mathcal{F}_{t+\\\\varepsilon}$.\\n\\nWe say that $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq0}$ is complete if $(\\\\Omega, \\\\mathcal{F}, P)$ is complete (contains all subsets of the $P$-null sets) and $\\\\{A \\\\in \\\\mathcal{F} : P[A] = 0\\\\} \\\\subset \\\\mathcal{F}_0$ (and hence $\\\\subset \\\\mathcal{F}_t$ for all $t$).",
        "proof": "",
        "preconditions": [
            "filtered probability space",
            "right-continuous filtration",
            "complete filtration"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 4.2",
        "name": "Usual conditions",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [
            "Definition 4.1"
        ],
        "statement": "A filtration $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq0}$ (or the filtered space $(\\\\Omega, \\\\mathcal{F}, \\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq0}, P)$) is said to satisfy the usual conditions if it is right-continuous and complete.",
        "proof": "",
        "preconditions": [
            "right-continuous filtration",
            "complete filtration",
            "usual conditions"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 4.3",
        "name": "Adapted process",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "A process $X$ is adapted to a filtration $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq0}$ if $X(t)$ is $\\\\mathcal{F}_t$-measurable for each $t \\\\geq 0$ (if and only if $\\\\mathcal{F}^X_t \\\\subset \\\\mathcal{F}_t$ for all $t$).",
        "proof": "",
        "preconditions": [
            "stochastic process",
            "filtered probability space",
            "adapted stochastic process"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 4.4",
        "name": "Progressive process",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [
            "Definition 4.3"
        ],
        "statement": "A process $X$ is $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq0}$-progressive (or progressively measurable) if for each $t \\\\geq 0$, the mapping $(s, \\\\omega) \\\\mapsto X_s(\\\\omega)$ is measurable on $([0,t] \\\\times \\\\Omega, \\\\mathcal{B}([0,t]) \\\\otimes \\\\mathcal{F}_t)$.",
        "proof": "",
        "preconditions": [
            "progressively measurable process",
            "filtered probability space"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 4.5",
        "name": "Right-continuous adaptedness",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [
            "Definition 4.3",
            "Definition 4.4"
        ],
        "statement": "An adapted process $(X_t)$ whose paths are all right-continuous (or are all left-continuous) is progressively measurable.",
        "proof": "We present the argument for a right-continuous $X$. For $t > 0$, $n \\\\geq 1$, $k = 0, 1, 2 \\\\ldots, 2^n - 1$ let $X^{(n)}_s(\\\\omega) := X_{\\\\frac{k+1}{2^n}t}(\\\\omega)$ for $\\\\frac{k}{2^n}t < s \\\\leq \\\\frac{k+1}{2^n}t$. Also define $X^{(n)}_0(\\\\omega) = X_0(\\\\omega)$.\\n\\nClearly $(X^{(n)}_s: s \\\\leq t)$ takes finitely many values and is $\\\\mathcal{B}([0,t])\\\\otimes\\\\mathcal{F}_t$\u2013measurable.\\n\\nFurther, by right continuity, $X_s(\\\\omega) = \\\\lim_{n\\\\rightarrow\\\\infty} X^{(n)}_s(\\\\omega)$, and hence is also measurable (as a limit of measurable mappings).",
        "preconditions": [
            "adapted stochastic process",
            "right-continuous stochastic process"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 4.6",
        "name": "Filtration Brownian Motion",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "A process $B$ is an $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$-Brownian motion if it is adapted to $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$, $B_0 = 0$, $B$ has continuous paths, $B_t - B_s \\\\sim N(0,t - s)$ for $t > s$ and $B_t - B_s$ is independent of $\\\\mathcal{F}_s$ for all $t > s$.\\n\\nEquivalently, $B$ is adapted, a Brownian motion in its own filtration, and $B_t - B_s$ is independent of $\\\\mathcal{F}_s$ for all $t > s$.",
        "proof": "",
        "preconditions": [
            "filtered probability space",
            "adapted stochastic process",
            "continuous paths",
            "Brownian motion"
        ]
    },
    {
        "type": "example",
        "id": "Example 4.7",
        "name": "Non-Brownian Motion",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [
            "Definition 4.6"
        ],
        "statement": "Let $B$ be a Brownian motion (in its natural filtration), and let $\\\\mathcal{F}_t = \\\\sigma (B_s; s \\\\leq t) \\\\vee \\\\sigma (B_T)$ for some $T > 0$. Then $B$ is not an $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$-Brownian motion.",
        "proof": "",
        "preconditions": [
            "Brownian motion",
            "filtered probability space"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 4.8",
        "name": "Stopping Time",
        "topic": "Stopping Times",
        "previous_results": [],
        "statement": "Let $(\\\\Omega, \\\\mathcal{F}, \\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}, P)$ be a filtered probability space. A random variable $\\\\tau : \\\\Omega \\\\to [0, +\\\\infty]$ is called a stopping time (relative to $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$) if $\\\\{\\\\tau \\\\leq t\\\\} \\\\in \\\\mathcal{F}_t, \\\\forall t \\\\geq 0$.\\n\\nStopping times are sometimes called optional times (for example, in the 'optional stopping theorem').\\n\\nThe 'first time a certain phenomenon occurs' will be a stopping time. Our fundamental examples will be first hitting times of sets. If $X$ is a stochastic process and $\\\\Gamma \\\\in \\\\mathcal{B}(\\\\mathbb{R})$ we set\\n\\n$H_\\\\Gamma(\\\\omega) := \\\\inf\\\\{t \\\\geq 0 : X_t(\\\\omega) \\\\in \\\\Gamma\\\\}$.",
        "proof": "",
        "preconditions": [
            "filtered probability space",
            "stochastic process",
            "stopping time"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 4.10",
        "name": "Stopping Time Filtration",
        "topic": "Stopping Times",
        "previous_results": [
            "Definition 4.8"
        ],
        "statement": "Given a stopping time $\\\\tau$ relative to $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$ we define\\n\\n$\\\\mathcal{F}_\\\\tau := \\\\{A \\\\in \\\\mathcal{F} : A \\\\cap \\\\{\\\\tau \\\\leq t\\\\} \\\\in \\\\mathcal{F}_t \\\\forall t \\\\geq 0\\\\}$,\\n$\\\\mathcal{F}_{\\\\tau-} := \\\\sigma (\\\\{A \\\\cap \\\\{\\\\tau > t\\\\} : t \\\\geq 0, A \\\\in \\\\mathcal{F}_t\\\\})$\\n\\n(which satisfy all the natural properties).",
        "proof": "",
        "preconditions": [
            "stopping time",
            "filtered probability space"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 4.11",
        "name": "Stopping Time Properties",
        "topic": "Stopping Times",
        "previous_results": [
            "Definition 4.8",
            "Definition 4.10"
        ],
        "statement": "Let $\\\\tau$ be a stopping time. Then\\n\\n(i) $\\\\mathcal{F}_{\\\\tau-}$ and $\\\\mathcal{F}_\\\\tau$ are $\\\\sigma$-algebras and $\\\\tau$ is $\\\\mathcal{F}_{\\\\tau-}$-measurable.\\n\\n(ii) $\\\\mathcal{F}_{\\\\tau-} \\\\subseteq \\\\mathcal{F}_\\\\tau$\\n\\n(iii) If $\\\\tau = t$ then $\\\\mathcal{F}_\\\\tau = \\\\mathcal{F}_t$\\n\\n(iv) If $\\\\tau$ and $\\\\rho$ are stopping times then so are $\\\\tau \\\\wedge \\\\rho$, $\\\\tau \\\\vee \\\\rho$ and $\\\\tau + \\\\rho$ and $\\\\{\\\\tau \\\\leq \\\\rho\\\\} \\\\in \\\\mathcal{F}_{\\\\tau\\\\wedge\\\\rho}$. Further if $\\\\tau \\\\leq \\\\rho$ then $\\\\mathcal{F}_\\\\tau \\\\subseteq \\\\mathcal{F}_\\\\rho$.\\n\\n(v) If $\\\\tau$ is a stopping time and $\\\\rho$ is a $[0, \\\\infty]$-valued random variable which is $\\\\mathcal{F}_\\\\tau$-measurable and $\\\\rho \\\\geq \\\\tau$, then $\\\\rho$ is a stopping time. In particular,\\n\\n$\\\\tau_n := \\\\sum_{k=0}^{\\\\infty} \\\\frac{k + 1}{2^n} 1_{\\\\{ \\\\frac{k}{2^n} <\\\\tau\\\\leq \\\\frac{k+1}{2^n} \\\\}} + \\\\infty 1_{\\\\{\\\\tau=\\\\infty\\\\}}$\\n\\nis a sequence of stopping times with $\\\\tau_n \\\\downarrow \\\\tau$ as $n \\\\to \\\\infty$.",
        "proof": "We prove (v):\\n\\nNote that $\\\\{\\\\rho \\\\leq t\\\\} = \\\\{\\\\rho \\\\leq t\\\\} \\\\cap \\\\{\\\\tau \\\\leq t\\\\} \\\\in \\\\mathcal{F}_t$ since $\\\\rho$ is $\\\\mathcal{F}_\\\\tau$-measurable. Hence $\\\\rho$ is a stopping time. We have $\\\\tau_n \\\\downarrow \\\\tau$ by definition, and clearly $\\\\tau_n$ is $\\\\mathcal{F}_\\\\tau$-measurable since $\\\\tau$ is $\\\\mathcal{F}_\\\\tau$-measurable.",
        "preconditions": [
            "stopping time",
            "filtered probability space"
        ]
    },
    {
        "type": "lemma",
        "id": "Lemma 4.12",
        "name": "Conditional Expectation Stopping",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "For any integrable random variable $X$, any stopping times $\\\\rho$ and $\\\\tau$, $1_{\\\\rho\\\\leq\\\\tau} E[X|F_{\\\\rho}] = 1_{\\\\rho\\\\leq\\\\tau} E[X|F_{\\\\rho\\\\wedge\\\\tau}]$.",
        "proof": "As $F_{\\\\rho\\\\wedge\\\\tau} \\\\subseteq F_{\\\\rho}$, and $1_{\\\\rho<\\\\tau}$ is $F_{\\\\rho\\\\wedge\\\\tau}$-measurable, $1_{\\\\rho\\\\leq\\\\tau} E[X|F_{\\\\rho\\\\wedge\\\\tau}] = E[1_{\\\\rho\\\\leq\\\\tau} E[X|F_{\\\\rho}]|F_{\\\\rho\\\\wedge\\\\tau}]$. Therefore, it's enough to show that $1_{\\\\rho\\\\leq\\\\tau} E[X|F_{\\\\rho}]$ is $F_{\\\\rho\\\\wedge\\\\tau}$-measurable. This follows from the fact that if $A \\\\in F_{\\\\rho}$, then $A \\\\cap \\\\{\\\\rho \\\\leq \\\\tau\\\\} \\\\cap \\\\{\\\\tau \\\\leq t\\\\} = (A \\\\cap \\\\{\\\\rho \\\\leq t\\\\}) \\\\cap \\\\{\\\\tau \\\\leq t\\\\} \\\\cap \\\\{\\\\rho \\\\wedge t \\\\leq \\\\tau \\\\wedge t\\\\}$, so $A \\\\cap \\\\{\\\\rho \\\\leq \\\\tau\\\\} \\\\in F_{\\\\tau}$.",
        "preconditions": [
            "filtered probability space",
            "stopping time"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 4.13",
        "name": "Stopped Process Measurability",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "Let $X$ be a progressively measurable process and $\\\\tau$ a stopping time. Then $X_{\\\\tau} 1_{\\\\tau<\\\\infty}$ is $F_{\\\\tau}$-measurable. The stopped process $X^{\\\\tau} = (X_{\\\\tau\\\\wedge t} : t \\\\geq 0)$ is progressively measurable (where $X_{\\\\tau} 1_{\\\\tau<\\\\infty}(\\\\omega) = X_{\\\\tau(\\\\omega)}(\\\\omega)1_{\\\\tau(\\\\omega)<\\\\infty}$).",
        "proof": "The first statement is 'easy' once we prove $X^{\\\\tau}$ is progressively measurable. Observe $X_{\\\\tau\\\\wedge s}$ on $[0,t] \\\\times \\\\Omega$ is a composition of two maps $(s, \\\\omega) \\\\mapsto(\\\\tau(\\\\omega) \\\\wedge s, \\\\omega)$, $([0,t] \\\\times \\\\Omega, B([0,t]) \\\\otimes F_t) \\\\mapsto([0,t] \\\\times \\\\Omega, B([0,t]) \\\\otimes F_t)$ and $(u, \\\\omega) \\\\mapsto X_u(\\\\omega)$, $([0,t] \\\\times \\\\Omega, B([0,t]) \\\\otimes F_t) \\\\mapsto(R, B(R))$, both of which are measurable since $\\\\tau$ is a stopping time and $X$ is progressively measurable.",
        "preconditions": [
            "progressively measurable process",
            "stopping time"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 4.14",
        "name": "Local Property",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "We say that a process $X$ locally has some property $C$ if there exists a sequence of stopping times $\\\\{\\\\tau_n\\\\}_{n\\\\in\\\\mathbb{N}}$ such that the stopped processes $X^{\\\\tau_n}$ have property $C$ for every $n$, and $\\\\tau_n \\\\uparrow \\\\infty$ almost surely. The sequence $\\\\tau_n$ is said to localize or reduce $X$.",
        "proof": "",
        "preconditions": [
            "stochastic process",
            "stopping time"
        ]
    },
    {
        "type": "example",
        "id": "Example 4.15",
        "name": "Locally Bounded Process",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "A Brownian motion is locally bounded, but is not bounded overall.",
        "proof": "",
        "preconditions": [
            "Brownian motion",
            "locally bounded process"
        ]
    },
    {
        "type": "lemma",
        "id": "Lemma 4.16",
        "name": "Process Reconstruction",
        "topic": "Stochastic Processes and Filtrations",
        "previous_results": [],
        "statement": "Given a localizing sequence $\\\\{\\\\tau_n\\\\}_{n\\\\in\\\\mathbb{N}}$ and a family of processes $\\\\{Y^n\\\\}_{n\\\\in\\\\mathbb{N}}$ such that, for all $n \\\\leq m$, $1_{t\\\\leq\\\\tau_n}Y^n = 1_{t\\\\leq\\\\tau_n}Y^m$ up to indistinguishability, there exists a process $X$ such that $1_{t\\\\leq\\\\tau_n}Y^n = 1_{t\\\\leq\\\\tau_n}X$ for all $n$.",
        "proof": "We can construct $X$ explicitly by $X_t = \\\\sum_{n\\\\in\\\\mathbb{N}} 1_{\\\\tau_n<t\\\\leq\\\\tau_{n+1}}Y^{n+1}_t$ (many constructions are possible).",
        "preconditions": [
            "stopping time"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 5.1",
        "name": "Strong Markov Property",
        "topic": "Brownian Motion",
        "previous_results": [],
        "statement": "Let $B = (B_t : t \\\\geq 0)$ be a standard Brownian motion on the filtered probability space $(\\\\Omega, F, \\\\{F_t\\\\}_{t\\\\geq 0}, P)$ and let $\\\\tau$ be a stopping time with respect to $\\\\{F_t\\\\}_{t\\\\geq 0}$. Then, conditional on $\\\\{\\\\tau < \\\\infty\\\\}$, the process $B^{(\\\\tau)}_t := B_{\\\\tau+t} - B_{\\\\tau}$ is a standard Brownian motion independent of $F_{\\\\tau}$. This is called the strong Markov property of Brownian motion.",
        "proof": "Assume that $\\\\tau < \\\\infty$ a.s..\\n\\nWe will show that $\\\\forall A \\\\in F_{\\\\tau}$, $0 \\\\leq t_1 < \\\\ldots < t_p$ and continuous and bounded functions $F$ on $\\\\mathbb{R}^p$ we have\\n$$E[1_A F(B^{(\\\\tau)}_{t_1}, \\\\ldots, B^{(\\\\tau)}_{t_p})] = P(A)E[F(B_{t_1}, \\\\ldots, B_{t_p})].$$\\n\\nGranted (11), taking $A = \\\\Omega$, we find that $B$ and $B^{(\\\\tau)}$ have the same finite dimensional distributions, and since $B^{(\\\\tau)}$ has continuous paths, it must be a Brownian motion. On the other hand (as usual using a monotone class argument), (11) says that $(B^{(\\\\tau)}_{t_1}, \\\\ldots, B^{(\\\\tau)}_{t_p})$ is independent of $F_{\\\\tau}$, and so $B^{(\\\\tau)}$ is independent of $F_{\\\\tau}$.\\n\\nTo establish (11), first observe that by continuity of $B$ and $F$,\\n$$F(B^{(\\\\tau)}_{t_1}, \\\\ldots, B^{(\\\\tau)}_{t_p}) = \\\\lim_{n\\\\to\\\\infty} \\\\sum_{k=0}^{\\\\infty} 1_{\\\\frac{k-1}{2^n} < \\\\tau \\\\leq \\\\frac{k}{2^n}} F(B_{\\\\frac{k}{2^n}+t_1} - B_{\\\\frac{k}{2^n}}, \\\\ldots, B_{\\\\frac{k}{2^n}+t_p} - B_{\\\\frac{k}{2^n}}) \\\\quad a.s.,$$\\n\\nand by the Dominated Convergence Theorem\\n$$E[1_A F(B^{(\\\\tau)}_{t_1}, \\\\ldots, B^{(\\\\tau)}_{t_p})] = \\\\lim_{n\\\\to\\\\infty} E\\\\left[\\\\sum_{k=0}^{\\\\infty} 1_A 1_{\\\\frac{k-1}{2^n} < \\\\tau \\\\leq \\\\frac{k}{2^n}} F(B_{\\\\frac{k}{2^n}+t_1} - B_{\\\\frac{k}{2^n}}, \\\\ldots, B_{\\\\frac{k}{2^n}+t_p} - B_{\\\\frac{k}{2^n}})\\\\right].$$\\n\\nFor $A \\\\in F_{\\\\tau}$, the event $A \\\\cap \\\\{\\\\frac{k-1}{2^n} < \\\\tau \\\\leq \\\\frac{k}{2^n}\\\\} \\\\in F_{\\\\frac{k}{2^n}}$, so using the simple Markov property at $k/2^n$,\\n$$E\\\\left[1_{A \\\\cap \\\\{\\\\frac{k-1}{2^n} < \\\\tau \\\\leq \\\\frac{k}{2^n}\\\\}} F(B_{\\\\frac{k}{2^n}+t_1} - B_{\\\\frac{k}{2^n}}, \\\\ldots, B_{\\\\frac{k}{2^n}+t_p} - B_{\\\\frac{k}{2^n}})\\\\right] = P\\\\left(A \\\\cap \\\\left\\\\{\\\\frac{k-1}{2^n} < \\\\tau \\\\leq \\\\frac{k}{2^n}\\\\right\\\\}\\\\right) E[F(B_{t_1}, \\\\ldots, B_{t_p})].$$\\n\\nSum over $k$ and take $n \\\\to \\\\infty$ to recover the desired result.\\n\\nIf $P(\\\\tau = \\\\infty) > 0$, the same argument gives instead\\n$$E\\\\left[1_{A \\\\cap \\\\{\\\\tau < \\\\infty\\\\}} F(B^{(\\\\tau)}_{t_1}, \\\\ldots, B^{(\\\\tau)}_{t_p})\\\\right] = P[A \\\\cap \\\\{\\\\tau < \\\\infty\\\\}] E[F(B_{t_1}, \\\\ldots, B_{t_p})].",
        "preconditions": [
            "Brownian motion",
            "filtered probability space",
            "stopping time"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 5.2",
        "name": "Reflection Principle",
        "topic": "Brownian Motion",
        "previous_results": [
            "Theorem 5.1"
        ],
        "statement": "Let $B$ be a Brownian motion and $\\\\tau$ a stopping time. Then the process $\\\\tilde{B}$ defined by\\n$$\\\\tilde{B}_t = \\\\begin{cases} B_t & t < \\\\tau, \\\\\\\\ 2B_{\\\\tau} - B_t & t \\\\geq \\\\tau. \\\\end{cases}$$\\nis a standard Brownian motion.",
        "proof": "By definition $\\\\tilde{B}$ is a Brownian motion up to the stopping time $\\\\tau$. For $t > \\\\tau$ we write $t = \\\\tau + t'$, and let $\\\\bar{B}_{t'} = B_{\\\\tau+t'} - B_{\\\\tau}$, which is a Brownian motion independent of $(\\\\tau, B_{\\\\tau})$ by the strong Markov property. Using this and the symmetry of Brownian motion, so that $\\\\bar{B} = -\\\\bar{B}$ in distribution, for $t > \\\\tau$ we have\\n$$B_t = B_{\\\\tau+t'} - B_{\\\\tau} + B_{\\\\tau} = \\\\bar{B}_{t'} + B_{\\\\tau} = -\\\\bar{B}_{t'} + B_{\\\\tau} \\\\text{ (in distribution by symmetry)} = 2B_{\\\\tau} - B_t = \\\\tilde{B}_t.$$\\n\\nThus $\\\\tilde{B}$ has the law of Brownian motion as required.",
        "preconditions": [
            "Brownian motion",
            "stopping time"
        ]
    },
    {
        "type": "corollary",
        "id": "Corollary 5.3",
        "name": "Maximum Distribution",
        "topic": "Brownian Motion",
        "previous_results": [
            "Theorem 5.2"
        ],
        "statement": "Let $S_t := \\\\sup_{u\\\\leq t} B_u$. For $a \\\\geq 0$ and $b \\\\leq a$ we have\\n$$P[S_t \\\\geq a, B_t \\\\leq b] = P[B_t \\\\geq 2a - b] \\\\quad \\\\forall t \\\\geq 0.$$\\nIn particular $S_t$ and $|B_t|$ have the same distribution.",
        "proof": "We apply the reflection principle with the stopping time $T_a$ (so $B_{T_a} = a$)\\n$$P[S_t \\\\geq a, B_t \\\\leq b] = P[T_a \\\\leq t, B_t \\\\leq b] = P[T_a \\\\leq t, \\\\tilde{B}_t \\\\leq b] = P[T_a \\\\leq t, 2a - B_t \\\\leq b] = P[B_t \\\\geq 2a - b],$$\\nas $2a - b > a$, so $\\\\{B_t \\\\geq 2a - b\\\\} \\\\subseteq \\\\{T_a \\\\leq t\\\\}$.\\n\\nWe have proved that $P[S_t \\\\geq a, B_t \\\\leq b] = P[B_t \\\\geq 2a - b]$. For the last assertion of the theorem, taking $b = a$, observe that\\n$$P[S_t \\\\geq a] = P[S_t \\\\geq a, B_t \\\\geq a] + P[S_t \\\\geq a, B_t \\\\leq a] = 2P[B_t \\\\geq a] = P[B_t \\\\geq a] + P[B_t \\\\leq -a] = P[|B_t| \\\\geq a].$$\\nwhere we used symmetry of Brownian motion.",
        "preconditions": [
            "Brownian motion"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 6.1",
        "name": "Continuous Martingales",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "An adapted stochastic process $(X_t)_{t\\\\geq 0}$ such that $X_t \\\\in L^1(P)$ (i.e. $E[|X_t|] < \\\\infty$) for any $t \\\\geq 0$, is called\\n\\ni. a martingale if $E[X_t|F_s] = X_s$ for all $0 \\\\leq s \\\\leq t$,\\n\\nii. a super-martingale if $E[X_t|F_s] \\\\leq X_s$ for all $0 \\\\leq s \\\\leq t$,\\n\\niii. a sub-martingale if $E[X_t|F_s] \\\\geq X_s$ for all $0 \\\\leq s \\\\leq t$.",
        "proof": "",
        "preconditions": [
            "adapted stochastic process"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 6.2",
        "name": "Convex Functions",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "Let $(X_t)_{t\\\\geq 0}$ be a martingale (respectively sub-martingale) and $\\\\phi : R \\\\to R$ be a convex (respectively convex and increasing) such that $E[|\\\\phi(X_t)|] < \\\\infty$ for any $t \\\\geq 0$. Then $(\\\\phi(X_t))_{t\\\\geq 0}$ is a sub-martingale.",
        "proof": "Apply the conditional Jensen inequality (see appendix, Lemma A.28).",
        "preconditions": [
            "martingale",
            "submartingale",
            "convex function",
            "non-decreasing function"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.3",
        "name": "Doob's Discrete Inequalities",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "If $(X_n)_{n\\\\geq 0}$ is a discrete martingale (or a nonnegative submartingale) w.r.t. some filtration $(F_n)$, then for any $N \\\\in \\\\mathbb{N}$, $p \\\\geq 1$ and $\\\\lambda > 0$, $$\\\\lambda^p P\\\\left(\\\\sup_{n\\\\leq N} |X_n| \\\\geq \\\\lambda\\\\right) \\\\leq E[|X_N|^p]$$ and for any $p > 1$ $$E[|X_N|^p] \\\\leq E\\\\left[\\\\sup_{n\\\\leq N} |X_n|^p\\\\right] \\\\leq \\\\left(\\\\frac{p}{p-1}\\\\right)^p E[|X_N|^p].$$",
        "proof": "",
        "preconditions": [
            "martingale",
            "submartingale",
            "filtered probability space"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.4",
        "name": "Doob's Maximal Inequalities",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.3"
        ],
        "statement": "If $(X_t)_{t\\\\geq 0}$ is a right continuous martingale or nonnegative sub-martingale, then for any $T \\\\geq 0$, $\\\\lambda > 0$, $$P\\\\left(\\\\sup_{t\\\\leq T} |X_t| \\\\geq \\\\lambda\\\\right) \\\\leq \\\\frac{1}{\\\\lambda^p}E[|X_T|^p], \\\\quad p \\\\geq 1$$ $$E\\\\left[\\\\sup_{t\\\\leq T} |X_t|^p\\\\right] \\\\leq \\\\left(\\\\frac{p}{p-1}\\\\right)^p E[|X_T|^p], \\\\quad p > 1.$$",
        "proof": "We extend the discrete-time result from Theorem 6.3 to continuous time. Suppose that $X$ is indexed by $t \\\\in [0, \\\\infty)$. Take a countable dense set $D$ in $[0, T]$, e.g., $D = \\\\mathbb{Q} \\\\cap [0, T]$, and an increasing sequence of finite subsets $D_n \\\\subseteq D_{n+1} \\\\subseteq D$ such that $\\\\cup_{n=1}^{\\\\infty}D_n = D$.\\n\\nThe inequalities from Theorem 6.3 hold for $X$ indexed by $t \\\\in D_n \\\\cup \\\\{T\\\\}$. By monotone convergence, the result extends to $t \\\\in D$. Since $X$ has right-continuous sample paths, the supremum over a countable dense set in $[0, T]$ is the same as over the whole of $[0, T]$, which gives us the desired result.",
        "preconditions": [
            "right-continuous stochastic process",
            "martingale",
            "submartingale"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 6.5",
        "name": "Brownian Motion Bound",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.4"
        ],
        "statement": "Let $(B_t)_{t\\\\geq 0}$ be Brownian motion and $S_t = \\\\sup_{u\\\\leq t} B_u$. For any $\\\\lambda > 0$ we have $$P[S_t \\\\geq \\\\lambda t] \\\\leq e^{-\\\\frac{\\\\lambda^2 t}{2}}.$$",
        "proof": "Recall that $e^{\\\\alpha B_t -\\\\alpha^2 t/2}$, $t \\\\geq 0$, is a non-negative martingale. It follows that, for $\\\\alpha \\\\geq 0$, using Doob's maximal inequality from Theorem 6.4,\\n\\n\\\\begin{align}\\nP[S_t \\\\geq \\\\lambda t] &\\\\leq P\\\\left(\\\\sup_{u\\\\leq t} \\\\left(e^{\\\\alpha B_u-\\\\alpha^2 t/2}\\\\right) \\\\geq e^{\\\\alpha\\\\lambda t-\\\\alpha^2 t/2}\\\\right) \\\\\\\\\\n&\\\\leq P\\\\left(\\\\sup_{u\\\\leq t} \\\\left(e^{\\\\alpha B_u-\\\\alpha^2 u/2}\\\\right) \\\\geq e^{\\\\alpha\\\\lambda t-\\\\alpha^2 t/2}\\\\right) \\\\\\\\\\n&\\\\leq e^{-\\\\alpha\\\\lambda t+\\\\alpha^2 t/2} E\\\\left[e^{\\\\alpha B_t -\\\\alpha^2 t/2}\\\\right] \\\\\\\\\\n&= e^{-\\\\alpha\\\\lambda t+\\\\alpha^2 t/2}\\n\\\\end{align}\\n\\nThe bound now follows since $\\\\min_{\\\\alpha\\\\geq 0} e^{-\\\\alpha\\\\lambda t+\\\\alpha^2 t/2} = e^{-\\\\lambda^2 t/2}$ (with the minimum achieved when $\\\\alpha = \\\\lambda$).",
        "preconditions": [
            "Brownian motion"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.6",
        "name": "Optional Stopping Theorem",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "If $(Y_n)_{n\\\\geq 1}$ is a supermartingale, then for any choice of bounded stopping times $S$ and $T$ such that $S \\\\leq T$, we have $$Y_S \\\\geq E[Y_T|F_S].$$",
        "proof": "",
        "preconditions": [
            "supermartingale",
            "bounded stopping time"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 6.7",
        "name": "Supermartingale Maximal Inequality",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.6"
        ],
        "statement": "Let $(X_t : t \\\\geq 0)$ be a supermartingale. Then $$P\\\\left(\\\\sup_{t\\\\in[0,T]\\\\cap\\\\mathbb{Q}} |X_t| \\\\geq \\\\lambda\\\\right) \\\\leq \\\\frac{1}{\\\\lambda}(2E[|X_T|] + E[|X_0|]), \\\\forall\\\\lambda, T > 0.$$ In particular, $\\\\sup_{t\\\\in[0,T]\\\\cap\\\\mathbb{Q}} |X_t| < \\\\infty$ a.s.",
        "proof": "Take a sequence of rational numbers $0 = t_0 < t_1 < \\\\ldots < t_n = T$. Applying Theorem 6.6 with $S = \\\\min\\\\{t_i : X_{t_i} \\\\geq \\\\lambda \\\\} \\\\wedge T$, we obtain\\n\\n$$E[X_0] \\\\geq E[X_S] \\\\geq \\\\lambda P\\\\left[ \\\\sup_{1\\\\leq i\\\\leq n} X_{t_i} \\\\geq \\\\lambda \\\\right] + E[X_T 1_{\\\\sup_{1\\\\leq i\\\\leq n} X_{t_i} < \\\\lambda}].$$\\n\\nRearranging,\\n\\n$$\\\\lambda P\\\\left( \\\\sup_{1\\\\leq i\\\\leq n} X_{t_i} \\\\geq \\\\lambda \\\\right) \\\\leq E[X_0] + E[X^-_T]$$\\n\\nwhere $X^-_T = -\\\\min(X_T, 0)$. Now $X^-_T$ is a non-negative submartingale and so we can apply Doob's inequality directly to it, from which\\n\\n$$\\\\lambda P\\\\left( \\\\sup_{1\\\\leq i\\\\leq n} X^-_{t_i} \\\\geq \\\\lambda \\\\right) \\\\leq E[X^-_T],$$\\n\\nand, since $E[X^-_T] \\\\leq E[|X_T|]$, taking the (monotone) limit in nested sequences in $[0, T] \\\\cap \\\\mathbb{Q}$, gives the result.",
        "preconditions": [
            "supermartingale"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 6.8",
        "name": "Upcrossing Number",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "Let $f : I \\\\to \\\\mathbb{R}$ be a function defined on a subset $I$ of $[0, \\\\infty)$. If $a < b$, the upcrossing number of $f$ along $[a, b]$, which we shall denote $U([a, b], (f_t)_{t\\\\in I})$ is the maximal integer $k \\\\geq 1$ such that there exists a sequence $s_1 < t_1 < \\\\cdot \\\\cdot \\\\cdot < s_k < t_k$ of elements of $I$ such that $f(s_i) < a$ and $f(t_i) > b$ for every $i = 1, \\\\ldots, k$.\\n\\nIf even for $k = 1$ there is no such sequence, we take $U([a, b], (f_t)_{t\\\\in I}) = 0$. If such a sequence exists for every $k \\\\geq 1$, we set $U([a, b], (f_t)_{t\\\\in I}) = \\\\infty$.",
        "proof": "",
        "preconditions": [
            "function on a closed interval"
        ]
    },
    {
        "type": "lemma",
        "id": "Lemma 6.9",
        "name": "Function Regularity",
        "topic": "Martingale Theory",
        "previous_results": [
            "Definition 6.8"
        ],
        "statement": "Let $D$ be a countable dense set in $[0, \\\\infty)$ and let $f$ be a real function defined on $D$. Assume that for every $T \\\\in D$\\n\\ni. $f$ is bounded on $D \\\\cap [0, T]$;\\n\\nii. for all rationals $a$ and $b$ such that $a < b$\\n\\n$U([a, b], (f_t)_{t\\\\in D\\\\cap[0,T]}) < \\\\infty$.\\n\\nThen the right limit\\n\\n$f(t+) = \\\\lim_{s\\\\downarrow t,s\\\\in D} f(s)$\\n\\nexists for every real $t \\\\geq 0$, and similarly the left limit\\n\\n$f(t-) = \\\\lim_{s\\\\uparrow t,s\\\\in D} f(s)$\\n\\nexists for any real $t > 0$.\\n\\nFurthermore, the function $g : \\\\mathbb{R}^+ \\\\to \\\\mathbb{R}$ defined by $g(t) = f(t+)$ is c\\\\`adl\\\\`ag ('continue \\\\`a droite avec des limites \\\\`a gauche'; i.e. right continuous with left limits) at every $t > 0$.",
        "proof": "",
        "preconditions": [
            "function of finite variation"
        ]
    },
    {
        "type": "lemma",
        "id": "Lemma 6.10",
        "name": "Doob's upcrossing lemma",
        "topic": "Martingale Theory",
        "previous_results": [
            "Definition 6.8"
        ],
        "statement": "Let $(X_t)_{t\\\\geq 0}$ be a supermartingale and $F$ a finite subset of $[0, T]$. If $a < b$ then\\n$E\\\\left[U\\\\left([a, b], (X_n : n \\\\in F)\\\\right)\\\\right] \\\\leq \\\\frac{E[(X_n - a)^-]}{b - a} \\\\leq \\\\frac{E[(X_T - a)^-]}{b - a}$.",
        "proof": "The last inequality follows since $(X_t - a)^-$ is a submartingale. By monotone convergence\\n\\n$\\\\lim_{k\\\\to\\\\infty} E[U([a, b - 1/k], (X_n : n \\\\in F))] = E[U([a, b), (X_n : n \\\\in F))]$\\n\\nsatisfies the same bound (and similarly for other intervals).\\n\\nTaking an increasing sequence $F_n$ and setting $\\\\cup_n F_n = F$, this immediately extends to a countable $F \\\\subset [0, T]$.",
        "preconditions": [
            "supermartingale",
            "stopping time"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.11",
        "name": "Supermartingale Convergence",
        "topic": "Martingale Theory",
        "previous_results": [
            "Lemma 6.9",
            "Lemma 6.10"
        ],
        "statement": "If $(X_t)$ is a right-continuous super-martingale and $\\\\sup_t E[X^-_t] < \\\\infty$ then $X_\\\\infty = \\\\lim_{t\\\\to\\\\infty} X_t$ exists (convergence a.s.) and $X_\\\\infty$ is in $L^1$. In particular, a non-negative right-continuous supermartingale converges a.s. as $t \\\\to \\\\infty$.",
        "proof": "By right continuity, for any $\\\\varepsilon > 0$,\\n\\n$U([a, b], (X_t)_{t\\\\in[0,T]}) \\\\leq U([a, b - \\\\varepsilon], (X_t)_{t\\\\in[0,T]\\\\cap\\\\mathbb{Q}})$.\\n\\nAlso, by Lemma 6.9, a bounded sequence $(x_n)_{n\\\\geq 1}$ converges if and only if the number of upcrossings is finite, that is $U([a, b], (x_n)_{n\\\\geq 1}) < \\\\infty$ for all $a < b$ with $a, b \\\\in \\\\mathbb{Q}$. By the above calculations and Lemma 6.10, these statements can be taken to hold almost surely for the paths of our supermartingale $X$. Hence $\\\\{X_{t_n}\\\\}$ converges a.s. for any sequence $t_n \\\\uparrow \\\\infty$, but this implies $X_t$ converges a.s. as $t \\\\to \\\\infty$.\\n\\nAs $X$ is a supermartingale\\n\\n$E[|X_t|] = E[X_t] + 2E[X^-_t] \\\\leq E[X_0] + 2E[X^-_t]$\\n\\nso by Fatou's inequality\\n\\n$E[|X_\\\\infty|] = E[\\\\lim_t |X_t|] \\\\leq \\\\liminf_t E[|X_t|] < \\\\infty$,\\n\\nthat is, $X_\\\\infty \\\\in L^1$.",
        "preconditions": [
            "right-continuous stochastic process",
            "supermartingale"
        ]
    },
    {
        "type": "example",
        "id": "Example 6.13",
        "name": "Exponential Martingale",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.11"
        ],
        "statement": "By direct calculation, we know $X_t = \\\\exp(\\\\theta B_t - \\\\theta^2 t/2)$ defines a martingale, and clearly $X \\\\geq 0$, so $X_t$ converges almost surely as $t \\\\to \\\\infty$. Restricting to $t \\\\in \\\\mathbb{N}$, from the strong law of large numbers, we know that\\n\\n$\\\\frac{B_t}{t} = \\\\frac{1}{t}\\\\sum_{s=1}^t (B_s - B_{s-1}) \\\\to 0$\\n\\nand hence as $t \\\\to \\\\infty$\\n\\n$\\\\theta B_t - \\\\frac{\\\\theta^2 t}{2} = t\\\\left(\\\\theta\\\\frac{B_t}{t} - \\\\frac{\\\\theta^2}{2}\\\\right) \\\\to -\\\\infty$.\\n\\nIt follows that $X_t \\\\to X_\\\\infty = 0$ a.s., but\\n\\n$E[|X_t - X_\\\\infty|] = E[X_t] = 1 \\\\not\\\\to 0$\\n\\nand\\n\\n$X_t \\\\neq E[X_\\\\infty|\\\\mathcal{F}_t]$.",
        "proof": "",
        "preconditions": [
            "martingale",
            "continuous stochastic process",
            "Brownian motion",
            "filtered probability space",
            "adapted stochastic process",
            "sequence converging almost surely"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.14",
        "name": "Supermartingale Limits",
        "topic": "Martingale Theory",
        "previous_results": [
            "Lemma 6.10",
            "Proposition 6.7",
            "Lemma 6.9"
        ],
        "statement": "If $(X_t : t \\\\geq 0)$ is a supermartingale, then for $P$-almost every $\\\\omega \\\\in \\\\Omega$, $\\\\forall t \\\\in (0, \\\\infty)$ $\\\\lim_{r\\\\uparrow t, r\\\\in\\\\mathbb{Q}} X_r(\\\\omega)$ and $\\\\lim_{r\\\\downarrow t, r\\\\in\\\\mathbb{Q}} X_r(\\\\omega)$ exist and are finite.",
        "proof": "Fix $T > 0$. From Lemma 6.10, as $E[(X_T - a)_-] \\\\leq E[|X_T|] + a < \\\\infty$, there exists $\\\\Omega_T \\\\subseteq \\\\Omega$, with $P(\\\\Omega_T) = 1$, such that for any $\\\\omega \\\\in \\\\Omega_T$ $\\\\forall a, b \\\\in \\\\mathbb{Q}$ with $a < b$, $U([a, b], (X_t(\\\\omega) : t \\\\in [0, T] \\\\cap \\\\mathbb{Q})) < \\\\infty$. Also, by Proposition 6.7, $\\\\sup_{t\\\\in[0,T]\\\\cap\\\\mathbb{Q}} |X_t(\\\\omega)| < \\\\infty$. It follows by Lemma 6.9 that the limits in (14) are well defined and finite for all $t \\\\leq T$ and $\\\\omega \\\\in \\\\Omega_T$. To complete the proof, take $\\\\Omega := \\\\Omega_1 \\\\cap \\\\Omega_2 \\\\cap \\\\Omega_3 \\\\cap \\\\ldots$.",
        "preconditions": [
            "supermartingale"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.15",
        "name": "Vitali convergence theorem",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "Let $\\\\{Y_k\\\\}$ be family of random variables, and suppose $Y_k \\\\to Y_\\\\infty$ in probability (or a.s.). Then $Y_k \\\\to Y_\\\\infty$ in $L^1$ if and only if $\\\\{Y_k\\\\}$ is a uniformly integrable family.",
        "proof": "See appendix (Theorem A.18)",
        "preconditions": [
            "sequence converging in probability",
            "sequence converging almost surely",
            "sequence converging in L1",
            "uniformly integrable family"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.16",
        "name": "Doob's Regularisation",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.14",
            "Theorem 6.15",
            "Lemma 6.9",
            "Lemma A.19",
            "Lemma A.29"
        ],
        "statement": "Let $X$ be a supermartingale with respect to a right-continuous and complete filtration $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$. If $t \\\\mapsto E[X_t]$ is right continuous (e.g. if $X$ is a martingale) then $X$ admits a modification with c\u00e0dl\u00e0g paths, which is also an $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$-supermartingale.",
        "proof": "By Theorem 6.14, there exists $\\\\Omega_0 \\\\subseteq \\\\Omega$, with $P[\\\\Omega_0] = 1$, such that the process $X_{t+}(\\\\omega) = \\\\begin{cases} \\\\lim_{r\\\\downarrow t, r\\\\in\\\\mathbb{Q}} X_r(\\\\omega) & \\\\omega \\\\in \\\\Omega_0 \\\\\\\\ 0 & \\\\omega \\\\notin \\\\Omega_0 \\\\end{cases}$ is well defined and adapted to $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$. By Lemma 6.9, it has c\u00e0dl\u00e0g paths.\\n\\nTo check that we really have only produced a modification of $X_t$, that is $X_t = X_{t+}$ almost surely, let $t_n \\\\downarrow t$ be a sequence of rationals. Then $\\\\{X_{t_k}\\\\}$ is uniformly integrable (see appendix, Lemma A.19) and converges a.s. to $X_{t+}$. By Vitali's convergence theorem, $X_{t_k} \\\\to X_{t+}$ in $L^1$, so we can pass to the limit $n \\\\to \\\\infty$ in the inequality $X_t \\\\geq E[X_{t_n}|\\\\mathcal{F}_t]$ to obtain $X_t \\\\geq E[X_{t+}|\\\\mathcal{F}_t] = X_{t+}$ a.s. as $\\\\mathcal{F}_t$ is right continuous.\\n\\nRight continuity of $t \\\\mapsto E[X_t]$ implies $E[X_{t+} - X_t] = 0$, so that $X_t = X_{t+}$ almost surely. It follows that $X_{s+} = X_s \\\\geq E[X_t|\\\\mathcal{F}_s] = E[X_{t+}|\\\\mathcal{F}_s]$ a.s. which confirms that the right-continuous modification is a supermartingale.",
        "preconditions": [
            "supermartingale",
            "right-continuous filtration",
            "complete filtration"
        ]
    },
    {
        "type": "corollary",
        "id": "Corollary 6.17",
        "name": "Martingale Modification",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.16"
        ],
        "statement": "If $X$ is a martingale then its c\u00e0dl\u00e0g modification is also a martingale.",
        "proof": "Applying Theorem 6.16 to $X$ and $-X$ gives the corollary.",
        "preconditions": [
            "martingale",
            "right-continuous stochastic process"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 6.19",
        "name": "Closed Martingale",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "A martingale is said to be closed if there exists a random variable $Z \\\\in L^1$ such that for every $t \\\\geq 0$, $X_t = E[Z|\\\\mathcal{F}_t]$.",
        "proof": "",
        "preconditions": [
            "martingale",
            "filtered probability space"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.20",
        "name": "Martingale Convergence Theorem",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.11",
            "Theorem 6.15",
            "Lemma A.29"
        ],
        "statement": "Let $(X_t : t \\\\geq 0)$ be a martingale with right continuous sample paths. Then TFAE:\\n\\ni. $X$ is closed;\\n\\nii. the collection $(X_t)_{t\\\\geq 0}$ is uniformly integrable;\\n\\niii. $X_t$ converges almost surely and in $L^1$ as $t \\\\to \\\\infty$.\\n\\nMoreover, if these properties hold, $X_t = E[X_\\\\infty|\\\\mathcal{F}_t]$ for every $t \\\\geq 0$, where $X_\\\\infty \\\\in L^1$ is the almost sure limit of $X_t$ as $t \\\\to \\\\infty$.",
        "proof": "That the first condition implies the second is easy. If $Z \\\\in L^1$, then $E[Z|\\\\mathcal{G}]$, where $\\\\mathcal{G}$ varies over sub $\\\\sigma$-fields of $\\\\mathcal{F}$ is uniformly integrable.\\n\\nAs ii implies Theorem 6.11, under both ii and iii we have almost sure convergence. Vitali's theorem then states that ii and iii are equivalent.\\n\\nFinally, if the third condition holds, for every $s \\\\geq 0$, pass to the limit as $t \\\\to \\\\infty$ in the equality $X_s = E[X_t|\\\\mathcal{F}_s]$ (using the fact that conditional expectation is continuous for the $L^1$-norm, see appendix, Lemma A.29) and obtain $X_s = E[X_\\\\infty|\\\\mathcal{F}_s]$.",
        "preconditions": [
            "martingale",
            "right-continuous stochastic process",
            "uniformly integrable family",
            "sequence converging almost surely",
            "sequence converging in L1"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.21",
        "name": "Optional stopping for UI discrete time martingales",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "Let $(Y_n)_{n\\\\in\\\\mathbb{N}}$ be a uniformly integrable martingale with respect to the filtration $(G_n)_{n\\\\in\\\\mathbb{N}}$, and let $Y_\\\\infty$ be the a.s. limit of $Y_n$ when $n \\\\to \\\\infty$. Then, for every choice of the stopping times $S$ and $T$ such that $S \\\\leq T$, we have $Y_T \\\\in L^1$ and $Y_S = E[Y_T |G_S]$, where $G_S = \\\\{A \\\\in G_\\\\infty : A \\\\cap \\\\{S = n\\\\} \\\\in G_n \\\\text{ for every } n \\\\in \\\\mathbb{N}\\\\}$, with the convention that $Y_T = Y_\\\\infty$ on the event $\\\\{T = \\\\infty\\\\}$, and similarly for $Y_S$.",
        "proof": "",
        "preconditions": [
            "uniformly integrable martingale",
            "stopping time"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.22",
        "name": "Optional Stopping Theorem for UI continuous time martingales",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.21",
            "Lemma 4.12"
        ],
        "statement": "Let $(X_t)_{t\\\\geq 0}$ be a uniformly integrable martingale with right continuous sample paths. Let $S$ and $T$ be two stopping times with $S \\\\leq T$. Then $X_S$ and $X_T$ are in $L^1$ and $X_S = E[X_T|F_S]$. In particular, for every stopping time $S$ we have $X_S = E[X_\\\\infty|F_S]$ and $E[X_S] = E[X_\\\\infty] = E[X_0]$.",
        "proof": "For any integer $n \\\\geq 0$ set\\n\\n$$T_n = \\\\sum_{k=0}^{\\\\infty} \\\\frac{k + 1}{2^n} 1_{\\\\{k2^{-n}<T \\\\leq(k+1)2^{-n}\\\\}} + \\\\infty 1_{\\\\{T =\\\\infty\\\\}},$$\\n\\n$$S_n = \\\\sum_{k=0}^{\\\\infty} \\\\frac{k + 1}{2^n} 1_{\\\\{k2^{-n}<S \\\\leq(k+1)2^{-n}\\\\}} + \\\\infty 1_{\\\\{S=\\\\infty\\\\}}.$$\\n\\nThen $T_n$ and $S_n$ are sequences of stopping times that decrease respectively to $T$ and $S$. Moreover, $S_n \\\\leq T_n$ for every $n \\\\geq 0$.\\n\\nFor each fixed $n$, $2^nS_n$ and $2^nT_n$ are stopping times of the discrete filtration $G^{(n)}_k = F_{k/2^n}$ and $Y^{(n)}_k = X_{k/2^n}$ is a discrete martingale with respect to this filtration.\\n\\nFrom Theorem 6.21, $Y^{(n)}_{2^nS_n}$ and $Y^{(n)}_{2^nT_n}$ are in $L^1$ and\\n\\n$$X_{S_n} = Y^{(n)}_{2^nS_n} = E[Y^{(n)}_{2^nT_n} |G^{(n)}_{2^nS_n}] = E[X_{T_n}|F_{S_n}].$$\\n\\nLet $A \\\\in F_S$. Since $F_S \\\\subseteq F_{S_n}$ we have $A \\\\in F_{S_n}$ and so $E[1_A X_{S_n}] = E[1_A X_{T_n}]$. By right continuity, $X_S = \\\\lim_{n\\\\to\\\\infty} X_{S_n}$ and $X_T = \\\\lim_{n\\\\to\\\\infty} X_{T_n}$. The limits also hold in $L^1$ (in fact, by Theorem 6.21, $X_{S_n} = E[X_\\\\infty|F_{S_n}]$ for every $n$ and so $(X_{S_n})_{n\\\\geq 1}$ and $(X_{T_n})_{n\\\\geq 1}$ are uniformly integrable). $L^1$ convergence implies that the limits $X_S$ and $X_T$ are in $L^1$ and allows us to pass to a limit, $E[1_A X_S] = E[1_A X_T]$. This holds for all $A \\\\in F_S$ and so since $X_S$ is $F_S$-measurable we conclude that $X_S = E[X_T|F_S]$, as required.",
        "preconditions": [
            "uniformly integrable martingale",
            "right-continuous stochastic process",
            "stopping time"
        ]
    },
    {
        "type": "corollary",
        "id": "Corollary 6.23",
        "name": "Bounded stopping times",
        "topic": "Martingale Theory",
        "previous_results": [
            "Theorem 6.22"
        ],
        "statement": "In particular, for any martingale with right continuous paths and two bounded stopping times, $S \\\\leq T$, we have $X_S, X_T \\\\in L^1$ and $X_S = E[X_T|F_S]$.",
        "proof": "Let $a$ be such that $S \\\\leq T \\\\leq a$. The martingale $(X_{t\\\\wedge a})_{t\\\\geq 0}$ is closed by $X_a$ and so we may apply our previous results.",
        "preconditions": [
            "martingale",
            "right-continuous stochastic process",
            "bounded stopping time"
        ]
    },
    {
        "type": "corollary",
        "id": "Corollary 6.24",
        "name": "Stopped martingale properties",
        "topic": "Martingale Theory",
        "previous_results": [
            "Corollary 6.23",
            "Lemma 4.12"
        ],
        "statement": "Suppose that $(X_t)_{t\\\\geq 0}$ is a martingale with right continuous paths and $T$ is a stopping time.\\n\\ni. $X^T = (X_{t\\\\wedge T})_{t\\\\geq 0}$ is a martingale;\\n\\nii. if, in addition, $(X_t)_{t\\\\geq 0}$ is uniformly integrable, then $X^T = (X_{t\\\\wedge T})_{t\\\\geq 0}$ is uniformly integrable and for every $t \\\\geq 0$, $X_{t\\\\wedge T} = E[X_T|F_t]$.",
        "proof": "We know $X^T_t = X_{t\\\\wedge T} = X^t_T$, and that $X_t$ is integrable. Hence, by the optional stopping theorem applied to the stopped process $X^t$, we see that $X^T_t$ is integrable for every $t$. Furthermore, for any $s < t$, as $T \\\\wedge s$ and $T \\\\wedge t$ are bounded stopping times, by the optional stopping theorem and Lemma 4.12,\\n\\n$$X^T_s = X_{T \\\\wedge s} = E[X_{T \\\\wedge t}|F_{T \\\\wedge s}] = 1_{T < s}X_T + 1_{T \\\\geq s}E[X_{T \\\\wedge t}|F_{T \\\\wedge s}]$$\\n\\n$$= 1_{T < s}X_{T \\\\wedge t} + 1_{T \\\\geq s}E[X_{T \\\\wedge t}|F_s] = E[X^T_t|F_s].$$\\n\\nTherefore $X^T$ is a martingale.",
        "preconditions": [
            "martingale",
            "right-continuous stochastic process",
            "stopping time",
            "uniformly integrable martingale"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 6.25",
        "name": "Martingale characterization",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "Suppose $M$ is a right-continuous process defined for $t < \\\\infty$, and adapted to a right-continuous filtration $\\\\{F_t\\\\}_{t<\\\\infty}$. Then $M$ is a martingale if, and only if, for every bounded stopping time $T$ we know $E[|M_T|] < \\\\infty$ and $E[M_T] = E[M_0]$.",
        "proof": "By considering the process $\\\\{M_t - M_0\\\\}_{t\\\\geq 0}$, we can assume without loss of generality that $E[M_T] = E[M_0] = 0$. If $M$ is a martingale, then $M_t = E[M_T|F_t]$, and the result follows by optional stopping and Jensen's inequality.\\n\\nConversely, consider any times $s < t \\\\in [0, \\\\infty)$ and any $A \\\\in F_s$. Define a random time $T$ by putting $T(\\\\omega) = s$ if $\\\\omega \\\\in A$ and $T(\\\\omega) = t$ if $\\\\omega \\\\notin A$. Then $T$ is a stopping time. By hypothesis\\n\\n$$E[1_A M_s] + E[1_{A^c}M_t] = E[M_T] = 0 = E[M_t] = E[1_A M_t] + E[1_{A^c}M_t].$$\\n\\nTherefore\\n\\n$$E[1_A M_s] = E[1_A M_t]$$\\n\\nfor all $A \\\\in F_s$, so $M_s = E[M_t|F_s]$ almost surely.",
        "preconditions": [
            "right-continuous stochastic process",
            "adapted stochastic process",
            "right-continuous filtration",
            "bounded stopping time"
        ]
    },
    {
        "type": "example",
        "id": "Example 6.26",
        "name": "Hitting Time Expectation",
        "topic": "Martingale Theory",
        "previous_results": [],
        "statement": "Fix $a > 0$ and let $T_a$ be the first hitting time of $a$ by standard Brownian motion. Then for each $\\\\lambda > 0$, $E[e^{-\\\\lambda T_a}] = e^{-a\\\\sqrt{2\\\\lambda}}$.",
        "proof": "Recall that $N^\\\\lambda_t = \\\\exp(\\\\lambda B_t - \\\\lambda^2 t/2)$ is a martingale. So $N^\\\\lambda_{t\\\\wedge T_a}$ is still a martingale and it is in the bounded interval $[0, e^{\\\\lambda a}]$ and hence is uniformly integrable, so $E[N^\\\\lambda_{T_a}] = E[N^\\\\lambda_0]$. That is, $e^{a\\\\lambda} E[e^{-\\\\lambda^2 T_a/2}] = E[N^\\\\lambda_0] = 1$. Replace $\\\\lambda$ by $\\\\sqrt{2\\\\lambda}$ and rearrange. Warning: This argument fails if $\\\\lambda < 0$ - the reason being that we lose the uniform integrability.",
        "preconditions": [
            "Brownian motion",
            "stopping time"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 7.1",
        "name": "Total Variation",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "The (total) variation of a function $a$ over $[0, T]$ is defined as $V(a)_T = \\\\sup_{\\\\pi} \\\\sum_{i=0}^{m(\\\\pi)-1} |a_{t_{i+1}} - a_{t_i}|$, where the supremum is over partitions $\\\\pi = \\\\{0 = t_0 < t_1 < \\\\ldots < t_{N(\\\\pi)} = T\\\\}$ of $[0, T]$. We say that $a$ is of finite variation on $[0, T]$ if $V(a)_T < \\\\infty$. The function $a$ is of finite variation if $V(a)_T < \\\\infty$ for all $T \\\\geq 0$ and of bounded variation if $\\\\lim_{T \\\\rightarrow \\\\infty} V(a)_T < \\\\infty$.",
        "proof": "",
        "preconditions": [
            "function of finite variation"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.3",
        "name": "Finite Variation Decomposition",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Definition 7.1"
        ],
        "statement": "The function $a$ is of finite variation if and only if it is equal to the difference of two non-decreasing functions, $a_1$ and $a_2$. Moreover, if $a$ is of finite variation, then $a_1$ and $a_2$ can be chosen so that $V(a)_t = a_1(t) + a_2(t)$. If $a$ is c\u00e0dl\u00e0g then $V(a)_t$ is also c\u00e0dl\u00e0g.",
        "proof": "Let $a_1(t) = \\\\frac{1}{2}(V(a)_t - a(t)) = \\\\frac{1}{2}\\\\sup_{\\\\pi} \\\\sum_{i=0}^{N(\\\\pi)-1} (|a(t_{i+1}) - a(t_i)| - (a(t_{i+1}) - a(t_i)))$. This is a non-decreasing function of $t$, as is $a_2(t) = (V(a)_t + a(t))/2$, and these functions have the required properties.",
        "preconditions": [
            "function of finite variation",
            "non-decreasing function"
        ]
    },
    {
        "type": "example",
        "id": "Example 7.4",
        "name": "Exponential Distribution Function",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Proposition 7.3"
        ],
        "statement": "For some $\\\\lambda \\\\in \\\\mathbb{R}$, let $a(t) = 1-e^{-\\\\lambda t}$. Then $\\\\mu([a, b)) = e^{-\\\\lambda a} - e^{-\\\\lambda b} = \\\\int_a^b \\\\lambda e^{-\\\\lambda t}dt$, and we find $(f \\\\cdot a)(t) = \\\\int_0^t f(s)\\\\lambda e^{-\\\\lambda s}ds$. The same approach works whenever $a$ is any distribution function.",
        "proof": "",
        "preconditions": [
            "function of finite variation",
            "non-decreasing function",
            "non-negative function"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.6",
        "name": "Associativity",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Proposition 7.3"
        ],
        "statement": "Let $a$ be of finite variation as above and $f$, $g$ measurable functions, $f$ is $a$-integrable and $g$ is $(f \\\\cdot a)$-integrable. Then $gf$ is $a$-integrable and $\\\\int_0^t g(s)d(f \\\\cdot a)(s) = \\\\int_0^t g(s)f(s)da(s)$. In our 'dot'-notation: $g \\\\cdot (f \\\\cdot a) = (gf) \\\\cdot a$.",
        "proof": "",
        "preconditions": [
            "function of finite variation",
            "measurable space with sigma-finite measure"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.7",
        "name": "Stopping",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Proposition 7.3"
        ],
        "statement": "Let $a$ be of finite variation as above and fix $t \\\\geq 0$. Set $a_t(s) = a(t \\\\wedge s)$. Then $a_t$ is of finite variation and for any measurable $a$-integrable function $f$ $\\\\int_0^{u \\\\wedge t} f(s)da(s) = \\\\int_0^u f(s)da_t(s) = \\\\int_0^u f(s)1_{[0,t]}(s)da(s)$, $u \\\\in [0, \\\\infty]$.",
        "proof": "",
        "preconditions": [
            "function of finite variation",
            "integrable function",
            "measurable space with sigma-finite measure"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.8",
        "name": "Integration by parts",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $a$ and $b$ be two right-continuous functions of finite variation with $a(0) = b(0) = 0$. Then for any $t$\\n\\n$a(t)b(t) = \\\\int_0^t a(s-)db(s) + \\\\int_0^t b(s-)da(s) + \\\\sum_{s\\\\in[0,t]} \\\\Delta a(s)\\\\Delta b(s)$\\n\\nwhere $\\\\Delta a(t) = a(t) - a(t-)$ and $a(t-) = \\\\lim_{s\\\\uparrow t} a(s)$.",
        "proof": "For a partition $\\\\pi_n$, take a telescoping sum\\n\\n$a(t)b(t) = \\\\sum_{t_i\\\\in\\\\pi_n} (a(t_i)b(t_i) - a(t_{i-1})b(t_{i-1}))$\\n$= \\\\sum_{t_i\\\\in\\\\pi_n} a(t_{i-1})(b(t_i) - b(t_{i-1})) + \\\\sum_{t_i\\\\in\\\\pi_n} b(t_{i-1})(a(t_i) - a(t_{i-1}))$\\n$+ \\\\sum_{t_i\\\\in\\\\pi_n} (a(t_i) - a(t_{i-1}))(b(t_i) - b(t_{i-1})).$\\n\\nBy dominated convergence, these converge to the stated integrals.",
        "preconditions": [
            "right-continuous stochastic process",
            "function of finite variation"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.10",
        "name": "Chain-rule",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Proposition 7.8"
        ],
        "statement": "If $F$ is a $C^1$ function and $a$ is continuous of finite variation, then $F(a(t))$ is also of finite variation and\\n\\n$F(a(t)) = F(a(0)) + \\\\int_0^t F'(a(s))da(s)$.",
        "proof": "The statement is trivially true for $F(x) = x$. Now by Proposition 7.8, it is straightforward to check that if the statement is true for $F$, then it is also true for $xF(x)$. Hence, by induction, the statement holds for all polynomials. To complete the proof, approximate $F \\\\in C^1$ by a sequence of polynomials.",
        "preconditions": [
            "C1 function",
            "continuous function",
            "function of finite variation"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.11",
        "name": "Change of variables",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "If $a$ is non-decreasing and right-continuous then so is its 'right inverse'\\n\\n$c(s) := \\\\inf\\\\{t \\\\geq 0 : a(t) > s\\\\}$,\\n\\nwhere $\\\\inf \\\\emptyset = +\\\\infty$. Let $a(0) = 0$. Then, for any Borel measurable function $f \\\\geq 0$ on $R_+$, we have\\n\\n$\\\\int_0^\\\\infty f(u)da(u) = \\\\int_0^{a(\\\\infty)} f(c(s))ds$.",
        "proof": "If $f(u) = 1_{[0,v]}(u)$, then the claim becomes\\n\\n$a(v) = \\\\int_0^\\\\infty 1_{\\\\{c(s)\\\\leq v\\\\}}ds = \\\\inf\\\\{s : c(s) > v\\\\}$,\\n\\nand equality holds by definition of $c$. Take differences to get indicators of sets $(u, v]$. The Monotone Class Theorem allows us to extend to functions of compact support and then take increasing limits to obtain the formula in general.",
        "preconditions": [
            "non-decreasing function",
            "right-continuous stochastic process",
            "non-negative function",
            "measurable space with sigma-finite measure"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 7.12",
        "name": "Finite variation process",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "An adapted right-continuous process $A = (A_t : t \\\\geq 0)$ is called a finite variation process (or a process of finite variation) if $A_0 = 0$ and $t \\\\mapsto A_t$ is (a function) of finite variation a.s..",
        "proof": "",
        "preconditions": [
            "adapted stochastic process",
            "right-continuous stochastic process",
            "process of finite variation",
            "function of finite variation"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.13",
        "name": "Stochastic integral measurability",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Proposition 4.5"
        ],
        "statement": "Let $A$ be a finite variation process and $K$ a progressively measurable process s.t.\\n\\n$\\\\forall t \\\\geq 0, \\\\forall \\\\omega \\\\in \\\\Omega, \\\\int_0^t |K_s(\\\\omega)||dA_s(\\\\omega)| < \\\\infty$.\\n\\nThen $((K \\\\cdot A)_t : t \\\\geq 0)$, defined as $(K \\\\cdot A)_t(\\\\omega) := \\\\int_0^t K_s(\\\\omega)dA_s(\\\\omega)$, is a finite variation process.",
        "proof": "The right continuity is immediate from the deterministic theory, but we need to check that $(K \\\\cdot A)_t$ is adapted (and hence progressive, by Proposition 4.5). For this we check that if $t > 0$ is fixed and $h : [0,t] \\\\times \\\\Omega \\\\to R$ is measurable with respect to $B([0,t]) \\\\otimes F_t$, and if\\n\\n$\\\\int_0^t |h(s, \\\\omega)||dA_s(\\\\omega)| < \\\\infty$\\n\\nfor every $\\\\omega \\\\in \\\\Omega$, then\\n\\n$\\\\int_0^t h(s, \\\\omega)dA_s(\\\\omega)$\\n\\nis $F_t$-measurable.\\n\\nFix $t > 0$. Consider first $h$ defined by $h(s, \\\\omega) = 1_{(u,v]}(s)1_{\\\\Gamma}(\\\\omega)$ for $(u, v] \\\\subseteq [0,t]$ and $\\\\Gamma \\\\in F_t$. Then\\n\\n$(h \\\\cdot A)_t = 1_{\\\\Gamma}(A_v - A_u)$\\n\\nis $F_t$-measurable. By the Monotone Class Theorem, $(h \\\\cdot A)_t$ is $F_t$-measurable for any $h = 1_G$ with $G \\\\in B([0,t]) \\\\otimes F_t$, or, more generally, any bounded $B([0,t]) \\\\otimes F_t$-measurable function $h$. If $h$ is a general $B([0,t]) \\\\otimes F_t$-measurable function satisfying\\n\\n$\\\\int_0^t |h(s, \\\\omega)||dA_s(\\\\omega)| < \\\\infty \\\\forall\\\\omega \\\\in \\\\Omega$,\\n\\nthen $h$ is a pointwise limit, $h = \\\\lim_{n\\\\to\\\\infty} h_n$, of simple functions with $|h| \\\\geq |h_n|$. The integrals $\\\\int h_n(s, \\\\omega)dA_s(\\\\omega)$ converge by the Dominated Convergence Theorem, and hence $\\\\int_0^t h(s, \\\\omega)dA_s(\\\\omega)$ is also $F_t$-measurable (as a limit of $F_t$-measurable functions). In particular, $(K \\\\cdot A)_t(\\\\omega)$ is $F_t$-measurable since by progressive measurability, $(s, \\\\omega) \\\\mapsto K_s(\\\\omega)$ on $[0,t]$ is $B([0,t]) \\\\otimes F_t$-measurable.",
        "preconditions": [
            "process of finite variation",
            "progressively measurable process"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 7.14",
        "name": "Continuous local martingale",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "An adapted process $(M_t : t \\\\geq 0)$ is called a continuous local martingale if it has continuous trajectories a.s. and if there exists a non-decreasing sequence of stopping times $(\\\\tau_n)_{n\\\\geq 1}$ such that $\\\\tau_n \\\\uparrow \\\\infty$ a.s. and for each $n$, $M^{\\\\tau_n} = (M_{t\\\\wedge\\\\tau_n} : t \\\\geq 0)$ is a (wlog uniformly integrable) martingale. We say $(\\\\tau_n)$ reduces or localizes $M$.",
        "proof": "",
        "preconditions": [
            "adapted stochastic process",
            "continuous paths",
            "stopping time"
        ]
    },
    {
        "type": "example",
        "id": "Example 7.15",
        "name": "Non-L1 Local Martingale",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $\\\\xi$ be a random variable not in $L^1$, and $Z$ be an independent Bernoulli random variable with $p = 1/2$. Define a filtration\\n\\n$F_t = \\\\begin{cases} \\\\{\\\\emptyset, \\\\Omega\\\\} & t < 1 \\\\\\\\ \\\\sigma(\\\\xi) & t \\\\in [1, 2) \\\\\\\\ \\\\sigma(\\\\xi, Z) & t \\\\geq 2 \\\\end{cases}$\\n\\nand a process\\n\\n$X_t = \\\\begin{cases} 0 & t < 2 \\\\\\\\ \\\\xi Z & t \\\\geq 2 \\\\end{cases}$\\n\\nBy taking the stopping times $\\\\tau_n = n1_{\\\\{|\\\\xi|<n\\\\}}$, we see that $X$ is a local martingale, but cannot be a martingale as $E[|X_2|] \\\\not< \\\\infty$.",
        "proof": "",
        "preconditions": [
            "filtered probability space",
            "stochastic process",
            "adapted stochastic process",
            "local martingale",
            "stopping time",
            "martingale"
        ]
    },
    {
        "type": "example",
        "id": "Example 7.16",
        "name": "Time-Changed Brownian Motion",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $B$ be a Brownian motion, and $\\\\xi$ an independent nonnegative random variable not in $L^1$. Then define $X_t = B_{\\\\xi^2 t}$, in the filtration $\\\\{F^X_{t+}\\\\}_{t\\\\geq 0}$. Then\\n$E[|X_t|] = E[|B_{\\\\xi^2 t}|] = E\\\\left[E\\\\left[|B_{\\\\xi^2 t}|\\\\big|\\\\xi\\\\right]\\\\right] = \\\\sqrt{2t/\\\\pi}E[\\\\xi] = \\\\infty$\\n\\nso $X$ is not a martingale. However, $\\\\xi$ is $F^X_0$-measurable (we will see this from the fact $\\\\langle X \\\\rangle_t = \\\\xi^2 t$ and right-continuity), so we can use the stopping times $\\\\tau_n = n1_{\\\\{\\\\xi<n\\\\}}$ to localize and hence verify $X^{\\\\tau_n}$ is a martingale. As $X$ is continuous, we can also localize with $\\\\tau_n = \\\\inf\\\\{t : |X_t| \\\\geq n\\\\}$.",
        "proof": "",
        "preconditions": [
            "Brownian motion",
            "continuous stochastic process",
            "martingale",
            "local martingale",
            "stopping time",
            "continuous paths",
            "right-continuous filtration"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.17",
        "name": "Local Martingale Properties",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "\\\\begin{enumerate}[i.]\\n\\\\item A non-negative continuous local martingale such that $M_0 \\\\in L^1$ is a supermartingale.\\n\\\\item A continuous local martingale $M$ such that there exists a random variable $Z \\\\in L^1$ with $|M_t| \\\\leq Z$ for every $t \\\\geq 0$ is a uniformly integrable martingale.\\n\\\\item If $M$ is a continuous local martingale and $M_0 = 0$ (or more generally $M_0 \\\\in L^1$), the sequence of stopping times\\n$T_n = \\\\inf\\\\{t \\\\geq 0 : |M_t| \\\\geq n\\\\}$\\nreduces $M$.\\n\\\\item If $M$ is a continuous local martingale, then for any stopping time $\\\\rho$, the stopped process $M^\\\\rho$ is also a continuous local martingale.\\n\\\\end{enumerate}",
        "proof": "\\\\begin{proof}\\n(i) Write $M_t = M_0 + N_t$. By definition, there exists a sequence $T_n$ of stopping times that reduces $N$. Thus, if $s \\\\leq t$, for every $n$,\\n\\n$N_{s \\\\wedge T_n} = E[N_{t \\\\wedge T_n}|\\\\mathcal{F}_s]$.\\n\\nWe can add $M_0$ to both sides ($M_0$ is $\\\\mathcal{F}_s$-measurable and in $L^1$) and we find\\n\\n$M_{s \\\\wedge T_n} = E[M_{t \\\\wedge T_n}|\\\\mathcal{F}_s]$.\\n\\nSince $M$ takes non-negative values, let $n \\\\to \\\\infty$ and apply Fatou's lemma for conditional expectations to find\\n\\n$M_s \\\\geq E[M_t|\\\\mathcal{F}_s]$.\\n\\nTaking $s = 0$, $E[M_t] \\\\leq E[M_0] < \\\\infty$. So $M_t \\\\in L^1$ for every $t \\\\geq 0$, and the above equation says that $M$ is a supermartingale.\\n\\n(ii) By the same argument,\\n\\n$M_{s \\\\wedge T_n} = E[M_{t \\\\wedge T_n}|\\\\mathcal{F}_s]$.\\n\\nSince $|M_{t \\\\wedge T_n}| \\\\leq Z$, this time apply the Dominated Convergence Theorem to see that $M_{t \\\\wedge T_n}$ converges in $L^1$ (to $M_t$) and $M_s = E[M_t|\\\\mathcal{F}_s]$.\\n\\nThe other two statements are immediate.\\n\\\\end{proof}",
        "preconditions": [
            "continuous local martingale",
            "non-negative function",
            "uniformly integrable martingale",
            "stopping time"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 7.18",
        "name": "Local Martingale Characterization",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "A continuous local martingale $M$ with $M_0 = 0$ a.s., is a process of finite variation if and only if $M$ is indistinguishable from zero.",
        "proof": "\\\\begin{proof}\\nAssume $M$ is a continuous local martingale and of finite variation. Let\\n\\n$\\\\tau_n = \\\\inf\\\\{t \\\\geq 0 : \\\\int_0^t |dM_s| \\\\geq n\\\\} = \\\\inf\\\\{t \\\\geq 0 : V(M)_t \\\\geq n\\\\}$,\\n\\nwhich are stopping times since $V(M)_t = \\\\int_0^t |dM_s|$ is continuous and adapted.\\nLet $N = M^{\\\\tau_n}$, which is bounded since\\n\\n$|N_t| = |M_{t \\\\wedge \\\\tau_n}| \\\\leq \\\\left|\\\\int_0^{t \\\\wedge \\\\tau_n} dM_u\\\\right| \\\\leq \\\\int_0^{t \\\\wedge \\\\tau_n} |dM_u| \\\\leq n$,\\n\\nand hence $(N_t)$ is a martingale.\\n\\nLet $t > 0$ and $\\\\pi = \\\\{0 = t_0 < t_1 < t_2 < \\\\ldots < t_{N(\\\\pi)} = t\\\\}$ be a partition of $[0,t]$. Then\\n\\n$E[N^2_t] = \\\\sum_{i=1}^{N(\\\\pi)} E[N^2_{t_i} - N^2_{t_{i-1}}] = \\\\sum_{i=1}^{N(\\\\pi)} E[(N_{t_i} - N_{t_{i-1}})^2]$\\n\\n$\\\\leq E\\\\left(\\\\sup_{1 \\\\leq i \\\\leq N(\\\\pi)} |N_{t_i} - N_{t_{i-1}}| \\\\cdot \\\\sum |N_{t_i} - N_{t_{i-1}}|\\\\right)$\\n\\n$\\\\leq nE\\\\left(\\\\sup_{1 \\\\leq i \\\\leq N(\\\\pi)} |N_{t_i} - N_{t_{i-1}}|\\\\right) \\\\to 0$\\n\\nas $\\\\|\\\\pi\\\\| \\\\to 0$ by the Dominated Convergence Theorem (since $|N_{t_i} - N_{t_{i-1}}| \\\\leq V(N)_t \\\\leq n$ and so $n$ is a dominating function).\\n\\nIt then follows by Fatou's Lemma that\\n\\n$E[M^2_t] = E[\\\\lim_{n \\\\to \\\\infty} M^2_{t \\\\wedge \\\\tau_n}] \\\\leq \\\\liminf_{n \\\\to \\\\infty} E[M^2_{t \\\\wedge \\\\tau_n}] = 0$\\n\\nwhich implies that $M_t = 0$ a.s., and so by continuity of paths, $P[M_t = 0 \\\\, \\\\forall t \\\\geq 0] = 1$.\\n\\\\end{proof}",
        "preconditions": [
            "continuous local martingale",
            "process of finite variation"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 7.20",
        "name": "Quadratic Variation Existence",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Theorem 7.18"
        ],
        "statement": "Let $M$ be a continuous local martingale. There exists a unique (up to indistinguishability) non-decreasing, continuous adapted finite variation process $(\\\\langle M, M \\\\rangle_t : t \\\\geq 0)$, starting in zero, such that $(M^2_t - \\\\langle M, M \\\\rangle_t : t \\\\geq 0)$ is a continuous local martingale.\\n\\nFurthermore, for any $T > 0$ and any sequence of partitions $\\\\pi_n = \\\\{0 = t^n_0 < t^n_1 < \\\\ldots < t^n_{N(\\\\pi_n)} = T\\\\}$ with $\\\\|\\\\pi_n\\\\| = \\\\sup_{1 \\\\leq i \\\\leq N(\\\\pi_n)}(t^n_i - t^n_{i-1}) \\\\to 0$ as $n \\\\to \\\\infty$\\n\\n$\\\\langle M, M \\\\rangle_T = \\\\lim_{n \\\\to \\\\infty} \\\\sum_{i=1}^{N(\\\\pi_n)} (M_{t^n_i} - M_{t^n_{i-1}})^2,$\\n\\nwhere the limit is in probability.\\n\\nThe process $\\\\langle M, M \\\\rangle$ is called the quadratic variation of $M$, or simply the increasing process of $M$, and is often denoted $\\\\langle M, M \\\\rangle_t = \\\\langle M \\\\rangle_t$.",
        "proof": "\\\\begin{proof}[Sketch of a Proof (NOT EXAMINABLE)]\\nUniqueness is a direct consequence of Theorem 7.18 since if $A, A'$ are two such processes then $(M^2 - A - (M^2 - A')) = A - A'$ is a local martingale starting in zero and of finite variation, which implies $A = A'$ by Theorem 7.18.\\n\\nThe idea of existence is as follows. First suppose that $M$ is bounded. Take a sequence of partitions $\\\\pi_n = \\\\{0 = t^n_0 < t^n_1 < \\\\cdots < t^n_{N(\\\\pi_n)} = T\\\\}$ with mesh tending to zero. Then check that\\n\\n$X^n_t := \\\\sum_{i=1}^{N(\\\\pi_n)} M_{t^n_{i-1}}(M_{t^n_i \\\\wedge t} - M_{t^n_{i-1} \\\\wedge t})$\\n\\nis a (bounded) martingale. Now observe that\\n\\n$M^2_{t^n_j} - 2X^n_{t^n_j} = \\\\sum_{i=1}^j (M_{t^n_i} - M_{t^n_{i-1}})^2$.\\n\\nA direct computation gives\\n\\n$\\\\lim_{n,m \\\\to \\\\infty} E[(X^n_t - X^m_t)^2] = 0$,\\n\\nand by Doob's $L^2$-inequality\\n\\n$\\\\lim_{n,m \\\\to \\\\infty} E[\\\\sup_{t \\\\leq T} (X^n_t - X^m_t)^2] = 0$.\\n\\nBy passing to a subsequence, $X^{n_k} \\\\to Y$ almost surely on $[0, T]$ where $(Y_t)_{t \\\\leq T}$ is a continuous process which inherits the martingale property from $X$.\\n\\n$M^2_{t^n_j} - 2X^n_{t^n_j} = \\\\sum_{i=1}^j (M_{t^n_i} - M_{t^n_{i-1}})^2 =: QV^{\\\\pi_n}_{t^n_j}(M)$\\n\\nis non-decreasing along $t^n_j : j \\\\leq N(\\\\pi_n)$. Letting $n \\\\to \\\\infty$, $M^2_t - 2Y_t$ is almost surely non-decreasing and we set $\\\\langle M, M \\\\rangle_t = M^2_t - 2Y_t$.\\n\\nTo move to a general continuous local martingale, we consider a sequence of stopped processes.\\n\\nDetails are in, for example, Le Gall's book.\\n\\\\end{proof}",
        "preconditions": [
            "continuous local martingale",
            "process of finite variation",
            "adapted stochastic process",
            "continuous paths",
            "partition with mesh tending to zero",
            "sequence converging in probability"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 7.21",
        "name": "H\u00b2 Space",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $H^2$ be the space of $L^2$-bounded c\u00e0dl\u00e0g martingales, i.e.\\n\\n$\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}, P$\u2013martingales $M$ s.t. $\\\\sup_{t\\\\geq 0} E[M^2_t] < \\\\infty$,\\n\\nand $H^{2,c}$ the subspace consisting of continuous $L^2$-bounded martingales. Finally, let $H^{2,c}_0 = \\\\{M \\\\in H^{2,c} : M_0 = 0 \\\\text{ a.s.}\\\\}$.\\n\\nWe note that the space $H^2$ is also sometimes denoted $\\\\mathcal{M}^2$.\\n\\nIt follows from Doob's $L^2$-inequality that\\n\\n$E\\\\left[\\\\sup_{t\\\\geq 0} M^2_t\\\\right] \\\\leq 4 \\\\sup_{t\\\\geq 0} E[M^2_t] < +\\\\infty, M \\\\in H^2$.\\n\\nConsequently, $\\\\{M_t : t \\\\geq 0\\\\}$ is bounded by a square integrable random variable $(\\\\sup_{t\\\\geq 0} |M_t|)$ and in particular is uniformly integrable. It follows from the martingale convergence theorem that $M_t = E[M_\\\\infty|\\\\mathcal{F}_t]$ for some square integrable random variable $M_\\\\infty$.\\n\\nConversely, we can start with a random variable $Y \\\\in L^2(\\\\Omega, \\\\mathcal{F}_\\\\infty, P)$ and define a martingale $M_t := E[Y|\\\\mathcal{F}_t] \\\\in H^2$ (and $M_\\\\infty = Y$).\\n\\nTwo $L^2$-bounded martingales $M, M'$ are indistinguishable if and only if $M_\\\\infty = M'_\\\\infty$ a.s. and so if we endow $H^2$ with the norm\\n\\n$\\\\|M\\\\|_{H^2} := \\\\sqrt{E[M^2_\\\\infty]} = \\\\|M_\\\\infty\\\\|_{L^2(\\\\Omega,\\\\mathcal{F}_\\\\infty,P)}, M \\\\in H^2,$\\n\\nthen $H^2$ can be identified with the familiar $L^2(\\\\Omega, \\\\mathcal{F}_\\\\infty, P)$ space.",
        "proof": "",
        "preconditions": [
            "martingale",
            "L2-bounded martingale",
            "uniformly integrable martingale",
            "continuous martingale",
            "right-continuous stochastic process",
            "filtered probability space",
            "square integrable function"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 7.23",
        "name": "H2,c Closure",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "$H^{2,c}$ is a closed subspace of $H^2$.",
        "proof": "This is almost a matter of writing down definitions. Suppose that the sequence $M_n \\\\in H^{2,c}$ converges in $\\\\|\\\\cdot\\\\|_{H^2}$ to some $M \\\\in H^2$. By Doob's $L^2$-inequality\\n\\n$$E\\\\left[\\\\sup_{t\\\\geq 0}|M^n_t - M_t|^2\\\\right] \\\\leq 4\\\\|M_n - M\\\\|^2_{H^2} \\\\to 0,$$\\n\\nas $n \\\\to \\\\infty$.\\n\\nPassing to a subsequence, we have $\\\\sup_{t\\\\geq 0} |M^{n_k}_t - M_t| \\\\to 0$ a.s. and hence $M$ has continuous paths a.s., which completes the proof.",
        "preconditions": [
            "Hilbert space"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 7.24",
        "name": "Martingale Characterization",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Proposition 7.17",
            "Theorem 7.18"
        ],
        "statement": "Let $M$ be a continuous local martingale with $M_0 \\\\in L^2$.\\n\\ni. TFAE\\n\\n(a) $M$ is a martingale, bounded in $L^2$;\\n(b) $E[\\\\langle M, M\\\\rangle_\\\\infty] < \\\\infty$.\\n\\nFurthermore, if these properties hold, $M^2_t -\\\\langle M, M\\\\rangle_t$ is a uniformly integrable martingale and, in particular, $E[M^2_\\\\infty] = E[M^2_0] + E[\\\\langle M, M\\\\rangle_\\\\infty]$.\\n\\nii. TFAE\\n\\n(a) $M$ is a martingale and $M_t \\\\in L^2$ for every $t \\\\geq 0$;\\n(b) $E[\\\\langle M, M\\\\rangle_t] < \\\\infty$ for every $t \\\\geq 0$.\\n\\nFurthermore, if these properties hold, $M^2_t - \\\\langle M, M\\\\rangle_t$ is a martingale.",
        "proof": "The second statement will follow from the first on applying it to $M_{t\\\\wedge a}$ for every choice of $a \\\\geq 0$.\\n\\nTo prove the first set of equivalences, without loss of generality, suppose that $M_0 = 0$ (or replace $M$ by $M - M_0$).\\n\\nSuppose that $M$ is a martingale, bounded in $L^2$. Doob's $L^2$-inequality implies that for every $T > 0$,\\n\\n$$E[\\\\sup_{0\\\\leq t\\\\leq T} M^2_t] \\\\leq 4E[M^2_T],$$\\n\\nand so, letting $T \\\\to \\\\infty$,\\n\\n$$E[\\\\sup_{t\\\\geq 0} M^2_t] \\\\leq 4 \\\\sup_{t\\\\geq 0} E[M^2_t] = C < \\\\infty.$$\\n\\nLet $S_n = \\\\inf\\\\{t \\\\geq 0 : \\\\langle M, M\\\\rangle_t \\\\geq n\\\\}$. Then the continuous local martingale $M^2_{t\\\\wedge S_n} - \\\\langle M, M\\\\rangle_{t\\\\wedge S_n}$ is dominated by $\\\\sup_{s\\\\geq 0} M^2_s + n$, which is integrable. By Proposition 7.17 this continuous local martingale is a uniformly integrable martingale, so $E[M^2_{t\\\\wedge S_n} - \\\\langle M\\\\rangle_{t\\\\wedge S_n}] = 0$, and hence\\n\\n$$E[\\\\langle M, M\\\\rangle_{t\\\\wedge S_n}] = E[M^2_{t\\\\wedge S_n}] \\\\leq E[\\\\sup_{s\\\\geq 0} M^2_s] \\\\leq C < \\\\infty.$$\\n\\nLet $n$ and then $t$ tend to infinity and use the Monotone Convergence Theorem to obtain $E[\\\\langle M, M\\\\rangle_\\\\infty] < \\\\infty$.\\n\\nConversely, assume that $E[\\\\langle M, M\\\\rangle_\\\\infty] < \\\\infty$. Set $T_n = \\\\inf\\\\{t \\\\geq 0 : |M_t| \\\\geq n\\\\}$. Then the continuous local martingale $M^2_{t\\\\wedge T_n} - \\\\langle M, M\\\\rangle_{t\\\\wedge T_n}$ is dominated by $n^2 + \\\\langle M, M\\\\rangle_\\\\infty$ which is integrable. From Proposition 7.17 again, this continuous local martingale is a uniformly integrable martingale and hence for every $t \\\\geq 0$,\\n\\n$$E[M^2_{t\\\\wedge T_n}] = E[\\\\langle M, M\\\\rangle_{t\\\\wedge T_n}] \\\\leq E[\\\\langle M, M\\\\rangle_\\\\infty] = C' < \\\\infty.$$\\n\\nLet $n \\\\to \\\\infty$ and use Fatou's lemma to see that $E[M^2_t] \\\\leq C' < \\\\infty$, so $(M_t)_{t\\\\geq 0}$ is bounded in $L^2$.\\n\\nWe still have to check that $(M_t)_{t\\\\geq 0}$ is a martingale. However, the equation above shows that $(M_{t\\\\wedge T_n})_{n\\\\geq 1}$ is uniformly integrable and so converges both almost surely and in $L^1$ to $M_t$ for every $t \\\\geq 0$. Recalling that $M^{T_n}$ is a martingale, $L^1$ convergence implies, for $s > t$,\\n\\n$$M_t = \\\\lim_n M^{T_n}_t = \\\\lim_n E[M^{T_n}_s|\\\\mathcal{F}_t] = E[\\\\lim_n M^{T_n}_s|\\\\mathcal{F}_t] = E[M_s|\\\\mathcal{F}_t]$$\\n\\nso $M$ is a martingale.\\n\\nFinally, if the two properties hold, then $M^2 - \\\\langle M, M\\\\rangle$ is dominated by $\\\\sup_{t\\\\geq 0} M^2_t + \\\\langle M, M\\\\rangle_\\\\infty$, which is integrable, and so Proposition 7.17 again says that $M^2 - \\\\langle M, M\\\\rangle$ is a uniformly integrable martingale.",
        "preconditions": [
            "continuous local martingale",
            "quadratic variation",
            "martingale",
            "L2-bounded martingale",
            "uniformly integrable martingale",
            "square integrable function"
        ]
    },
    {
        "type": "corollary",
        "id": "Corollary 7.25",
        "name": "Zero Martingale Characterization",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Theorem 7.18",
            "Theorem 7.24"
        ],
        "statement": "Let $M$ be a continuous local martingale with $M_0 = 0$. Then the following are equivalent:\\n\\ni. $M$ is indistinguishable from zero,\\n\\nii. $\\\\langle M\\\\rangle_t = 0$ for all $t \\\\geq 0$ a.s.,\\n\\niii. $M$ is a process of finite variation.",
        "proof": "We already know that the first and third statements are equivalent. That the first implies the second is trivial, so we must just show that the second implies the first. We have $\\\\langle M\\\\rangle_\\\\infty = \\\\lim_{t\\\\to\\\\infty}\\\\langle M\\\\rangle_t = 0$. From Theorem 7.24, $M \\\\in H^2$ and $E[M^2_\\\\infty] = E[\\\\langle M\\\\rangle_\\\\infty] = 0$ and so $M_t = E[M_\\\\infty|\\\\mathcal{F}_t] = 0$ almost surely.",
        "preconditions": [
            "continuous local martingale",
            "quadratic variation",
            "process of finite variation"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 7.26",
        "name": "Quadratic Co-variation",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "The quadratic co-variation between two continuous local martingales $M$, $N$ is defined by\\n\\n$$\\\\langle M, N\\\\rangle := \\\\frac{1}{2}(\\\\langle M + N, M + N\\\\rangle - \\\\langle M, M\\\\rangle - \\\\langle N, N\\\\rangle) .$$\\n\\nIt is often called the (angle) bracket process of $M$ and $N$.",
        "proof": "",
        "preconditions": [
            "continuous local martingale",
            "quadratic variation",
            "quadratic covariation"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.27",
        "name": "Bracket Properties",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Theorem 7.18",
            "Theorem 7.20"
        ],
        "statement": "For two continuous local martingales $M$, $N$\\n\\ni. the process $\\\\langle M, N\\\\rangle$ is the unique finite variation process, zero at zero, such that $(M_tN_t - \\\\langle M, N\\\\rangle_t : t \\\\geq 0)$ is a continuous local martingale;\\n\\nii. the mapping $M, N \\\\mapsto \\\\langle M, N\\\\rangle$ is bilinear and symmetric;\\n\\niii. for any stopping time $\\\\tau$,\\n\\n$$\\\\langle M^\\\\tau, N^\\\\tau \\\\rangle_t = \\\\langle M^\\\\tau, N\\\\rangle_t = \\\\langle M, N^\\\\tau \\\\rangle_t = \\\\langle M, N\\\\rangle_{\\\\tau\\\\wedge t},$$\\n\\n$t \\\\geq 0$, a.s.;\\n\\niv. for any $t > 0$ and a sequence of partitions $\\\\pi_n$ of $[0,t]$ with mesh converging to zero\\n\\n$$\\\\sum_{t_i\\\\in\\\\pi_n} (M_{t_{i+1}} - M_{t_i})(N_{t_{i+1}} - N_{t_i}) \\\\to \\\\langle M, N\\\\rangle_t,$$\\n\\nthe convergence being in probability.",
        "proof": "(i) $(M + N)^2_t - \\\\langle M + N, M + N\\\\rangle_t$ is a continuous local martingale and by adding and subtracting terms it is equal to\\n\\n$$\\\\underbrace{M^2_t - \\\\langle M, M\\\\rangle_t}_{\\\\text{l.mat}} + \\\\underbrace{N^2_t - \\\\langle N, N\\\\rangle_t}_{\\\\text{l.mat}} + \\\\underbrace{2\\\\left(M_tN_t - \\\\frac{1}{2}(\\\\langle M + N, M + N\\\\rangle_t - \\\\langle M, M\\\\rangle_t - \\\\langle N, N\\\\rangle_t)\\\\right)}_{\\\\text{hence also a l.mat}}$$\\n\\nUniqueness follows from Theorem 7.18.\\n\\n(iv) Note that\\n\\n$$(M_t + N_t - M_s - N_s)^2 - (M_t - M_s)^2 - (N_t - N_s)^2 = 2(M_t - M_s)(N_t - N_s).$$\\n\\nThe asserted convergence then follows from Theorem 7.20.\\n\\n(ii) Both properties follow from (iv). Symmetry is obvious from the definition in (20).\\n\\n(iii) Follows from (iv).",
        "preconditions": [
            "continuous local martingale",
            "process of finite variation",
            "stopping time",
            "partition with mesh tending to zero",
            "sequence converging in probability"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 7.31",
        "name": "Kunita\u2013Watanabe inequality",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $M, N$ be continuous local martingales and $K, H$ two measurable processes. Then for all $0 \\\\leq t \\\\leq \\\\infty$, $$\\\\int_0^t |H_s||K_s||d\\\\langle M, N\\\\rangle_s| \\\\leq \\\\left(\\\\int_0^t H^2_s d\\\\langle M\\\\rangle_s\\\\right)^{1/2} \\\\left(\\\\int_0^t K^2_s d\\\\langle N\\\\rangle_s\\\\right)^{1/2} \\\\quad a.s.$$",
        "proof": "We omit the proof which approximates $H, K$ by simple functions and then essentially uses the Cauchy\u2013Schwarz inequality for sums noted above.",
        "preconditions": [
            "continuous local martingale",
            "measurable space with sigma-finite measure"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 7.32",
        "name": "Continuous semimartingale",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "A stochastic process $X = (X_t : t \\\\geq 0)$ is called a continuous semimartingale if it can be written as $$X_t = X_0 + M_t + A_t, \\\\quad t \\\\geq 0$$ where $M$ is a continuous local martingale, $A$ is a continuous process of finite variation, and $M_0 = A_0 = 0$ a.s.. The decomposition is unique (up to indistinguishability). It should be remembered that there is a filtration $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$ and a probability measure $P$ implicit in our definition.",
        "proof": "",
        "preconditions": [
            "continuous semimartingale",
            "continuous local martingale",
            "process of finite variation",
            "continuous paths",
            "right-continuous filtration",
            "complete filtration"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 7.33",
        "name": "Semimartingale quadratic variation",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "A continuous semimartingale is of finite quadratic variation and in the notation above $\\\\langle X, X\\\\rangle = \\\\langle M, M\\\\rangle$.",
        "proof": "Fix $t \\\\geq 0$ and consider a sequence of partitions of $[0,t]$, $\\\\pi_m = \\\\{0 = t_0 < t_1 < \\\\ldots < t_{n_m} = t\\\\}$ with $\\\\|\\\\pi_m\\\\| \\\\to 0$ as $m \\\\to \\\\infty$. Then\\n$$\\\\sum_{i=1}^{n_m} (X_{t_i} - X_{t_{i-1}})^2 = \\\\underbrace{\\\\sum_{i=1}^{n_m} (M_{t_i} - M_{t_{i-1}})^2}_{(i)} + \\\\underbrace{\\\\sum_{i=1}^{n_m} (A_{t_i} - A_{t_{i-1}})^2}_{(ii)} + \\\\underbrace{2\\\\sum_{i=1}^{n_m} (M_{t_i} - M_{t_{i-1}})(A_{t_i} - A_{t_{i-1}})}_{(iii)}.$$\\n\\nIt follows from the properties of $M$ and $A$ that, as $m \\\\to \\\\infty$,\\n\\n$(i) \\\\to \\\\langle M, M\\\\rangle_t$,\\n\\n$(ii) \\\\leq \\\\sup_{1\\\\leq i\\\\leq n_m} |A_{t_i} - A_{t_{i-1}}| \\\\cdot V_t(A) \\\\to 0 \\\\quad a.s.$,\\n\\n$(iii) \\\\leq \\\\sup_{1\\\\leq i\\\\leq n_m} |M_{t_i} - M_{t_{i-1}}| \\\\cdot V_t(A) \\\\to 0 \\\\quad a.s.$.",
        "preconditions": [
            "continuous semimartingale",
            "quadratic variation"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 8.1",
        "name": "Stochastic integral linearity",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $M \\\\in H^{2,c}$. The mapping $\\\\phi \\\\mapsto \\\\phi \\\\bullet M$ is a linear map from $\\\\mathcal{E}$ to $H^{2,c}_0$. Moreover, $$\\\\|\\\\phi \\\\bullet M\\\\|^2_{H^{2,c}} = \\\\mathbb{E}\\\\left[\\\\int_0^{\\\\infty} \\\\phi^2_t d\\\\langle M\\\\rangle_t\\\\right].$$",
        "proof": "The proof is easy \u2013 we just need to show linearity. But given $\\\\phi, \\\\psi \\\\in \\\\mathcal{E}$, we use a refinement of the partitions on which they are constant to write them as simple functions with respect to the same partition and the result is trivial.",
        "preconditions": [
            "continuous local martingale",
            "stochastically integrable process",
            "quadratic variation"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 8.3",
        "name": "L2(M) space",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Given $M \\\\in H^{2,c}$ we denote by $L^2(M)$ the space of progressively measurable processes $K$ such that $$\\\\|K\\\\|^2_{L^2(M)} := \\\\mathbb{E}\\\\left[\\\\int_0^{\\\\infty} K^2_t d\\\\langle M\\\\rangle_t\\\\right] < +\\\\infty.$$ $L^2(M)$ is a Hilbert space, with inner product $$H, K \\\\mapsto \\\\mathbb{E}\\\\left[\\\\int_0^{\\\\infty} H_t K_t d\\\\langle M\\\\rangle_t\\\\right] = \\\\mathbb{E}[(HK \\\\cdot \\\\langle M\\\\rangle)_{\\\\infty}].$$\\n\\nWe have $\\\\mathcal{E} \\\\subseteq L^2(M)$ and (30) tells us that the map $\\\\mathcal{E} \\\\to H^{2,c}_0$ given by $\\\\phi \\\\mapsto \\\\phi \\\\bullet M$ is a linear isometry. If we can show that the elementary functions are dense in $L^2(M)$, this observation will allow us to define integrals of functions from $L^2(M)$ with respect to $M$ via a limiting procedure.",
        "proof": "",
        "preconditions": [
            "progressively measurable process",
            "continuous local martingale",
            "Hilbert space",
            "inner product space"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 8.4",
        "name": "Density of Elementary Functions",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $M \\\\in H^{2,c}$. Then $\\\\mathcal{E}$ is a dense vector subspace of $L^2(M)$.",
        "proof": "It is enough to show that if $K \\\\in L^2(M)$ is orthogonal to $\\\\phi$ for all $\\\\phi \\\\in \\\\mathcal{E}$, then $K = 0$ (as an element of $L^2(M)$). So suppose that $\\\\langle K, \\\\phi \\\\rangle_{L^2(M)} = 0$ for all $\\\\phi \\\\in \\\\mathcal{E}$. Let $X = K \\\\cdot \\\\langle M \\\\rangle$, i.e., $X_t = \\\\int_0^t K_u d\\\\langle M \\\\rangle_u$. This is well defined and, by Cauchy-Schwarz, \\n\\n$E[|X_t|] \\\\leq E\\\\left[\\\\int_0^t |K_u| d\\\\langle M \\\\rangle_u\\\\right] \\\\leq \\\\sqrt{E\\\\left[\\\\int_0^t K^2_u d\\\\langle M \\\\rangle_u\\\\right] \\\\cdot E[\\\\langle M \\\\rangle_t]} < +\\\\infty$\\n\\nsince $M \\\\in H^{2,c}$ and $K \\\\in L^2(M)$ (we took one of the functions to be identically one in Cauchy-Schwarz).\\n\\nTaking $\\\\phi = \\\\xi 1_{(s,t]} \\\\in \\\\mathcal{E}$, with $0 \\\\leq s < t$ and $\\\\xi$ a bounded $\\\\mathcal{F}_s$-measurable r.v., we have\\n\\n$0 = \\\\langle K, \\\\phi \\\\rangle_{L^2(M)} = E\\\\left[\\\\xi \\\\int_s^t K_u d\\\\langle M \\\\rangle_u\\\\right] = E[\\\\xi (X_t - X_s)]$.\\n\\nSince this holds for any $\\\\mathcal{F}_s$-measurable bounded $\\\\xi$, we conclude that $E[(X_t - X_s)|\\\\mathcal{F}_s] = 0$. In other words, $X$ is a martingale. But $X$ is also continuous and of finite variation and hence $X \\\\equiv 0$ a.s. Thus $K = 0$ $d\\\\langle M \\\\rangle$-a.e. a.s. and hence $K = 0$ in $L^2(M)$.",
        "preconditions": [
            "continuous local martingale",
            "Hilbert space"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 8.5",
        "name": "Stochastic Integral Extension",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Proposition 8.4"
        ],
        "statement": "Let $M \\\\in H^{2,c}$. The mapping $\\\\phi \\\\mapsto \\\\phi \\\\bullet M$ from $\\\\mathcal{E}$ to $H^{2,c}_0$ defined in (27) has a unique extension to a linear isometry from $L^2(M)$ to $H^{2,c}_0$ which we denote $K \\\\mapsto K \\\\bullet M$.",
        "proof": "By Proposition 8.4, we know that $\\\\mathcal{E}$ is dense in $L^2(M)$. For each $\\\\phi \\\\in \\\\mathcal{E}$, we have defined $\\\\phi \\\\bullet M$ as a stochastic integral, and we have shown that this mapping is a linear isometry, i.e., $\\\\|\\\\phi \\\\bullet M\\\\|_{H^{2,c}} = \\\\|\\\\phi\\\\|_{L^2(M)}$. By Proposition D.11 (extension of linear isometries), this mapping has a unique extension to a linear isometry from $L^2(M)$ to $H^{2,c}_0$, which we denote by $K \\\\mapsto K \\\\bullet M$.",
        "preconditions": [
            "continuous local martingale",
            "square integrable function",
            "stochastically integrable process",
            "Hilbert space"
        ]
    },
    {
        "type": "example",
        "id": "Example 8.7",
        "name": "Stochastic Integral Example",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Theorem 8.5"
        ],
        "statement": "Let $M = B^T$ be a standard Brownian motion stopped at a time $T > 0$. Consider $K_t = t^3$. To define $K \\\\bullet B$, we take a sequence of simple functions $K^n$ which converge to $K$ in the sense\\n\\n$E\\\\left[\\\\int_0^\\\\infty (K^n_t - K_t)^2 d\\\\langle M \\\\rangle_t\\\\right] = E\\\\left[\\\\int_0^T (K^n_t - t^3)^2 dt\\\\right] \\\\to 0$.\\n\\nThen the approximate integrals $K^n \\\\bullet M$ (which are defined by Riemann sums) converge in $H^2$ to a process which we call $K \\\\bullet M$, or\\n\\n$(K \\\\bullet M)_t = \\\\int_0^t s^3 dB_s$, for $t \\\\leq T$.\\n\\nNotice that if $B$ is standard Brownian motion and we calculate $(B \\\\bullet B)_t$, then\\n\\n$(B \\\\bullet B)_t = \\\\lim_{\\\\|\\\\pi\\\\|\\\\to 0} \\\\sum_{j=0}^{N(\\\\pi)-1} B_{t_j}(B_{t_{j+1}} - B_{t_j})$.\\n\\nWe also know already that the quadratic variation is\\n\\n$t = \\\\lim_{\\\\|\\\\pi\\\\|\\\\to 0} \\\\sum_{j=0}^{N(\\\\pi)-1} (B_{t_{j+1}} - B_{t_j})^2 = B^2_t - B^2_0 - 2\\\\sum_{j=0}^{N(\\\\pi)-1} B_{t_j}(B_{t_{j+1}} - B_{t_j})$,\\n\\nand so rearranging we find\\n\\n$\\\\int_0^t B_s dB_s = \\\\frac{1}{2}(B^2_t - B^2_0 - t) = \\\\frac{1}{2}(B^2_t - t)$.\\n\\nThis is not what one would have predicted from classical integration theory (the extra term here comes from the quadratic variation).\\n\\nEven more strangely, it matters that in (33) we took the left endpoint of the interval for evaluating the integrand. On the problem sheet, you are asked to evaluate\\n\\n$\\\\lim_{\\\\|\\\\pi\\\\|\\\\to 0}\\\\sum B_{t_{j+1}}(B_{t_{j+1}} - B_{t_j})$, and $\\\\lim_{\\\\|\\\\pi\\\\|\\\\to 0}\\\\sum \\\\frac{B_{t_j} + B_{t_{j+1}}}{2}(B_{t_{j+1}} - B_{t_j})$.\\n\\nEach gives a different answer.\\n\\nWe can more generally define\\n\\n$\\\\int_0^T f(B_s) \\\\circ dB_s = \\\\lim_{\\\\|\\\\pi\\\\|\\\\to 0}\\\\sum \\\\frac{f(B_{t_j}) + f(B_{t_{j+1}})}{2}(B_{t_{j+1}} - B_{t_j})$.\\n\\nThis is the so-called Stratonovich integral, and has the advantage that from the point of view of calculations, the rules of Newtonian calculus hold true. From a modelling perspective however, it can be the wrong choice. For example, suppose that we are modelling the change in a population size over time and we use $[t_i,t_{i+1})$ to represent the $(i + 1)$st generation. The change over $(t_i,t_{i+1})$ will be driven by the number of adults, so the population size at the beginning of the interval.",
        "proof": "",
        "preconditions": [
            "Brownian motion",
            "stopping time",
            "quadratic variation",
            "stochastically integrable process",
            "continuous stochastic process",
            "martingale",
            "filtered probability space"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 8.8",
        "name": "Intrinsic Characterization",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Corollary 7.25"
        ],
        "statement": "Let $M \\\\in H^{2,c}$. For any $K \\\\in L^2(M)$ there exists a unique element in $H^{2,c}_0$, denoted $K \\\\bullet M$, such that\\n\\n$\\\\langle K \\\\bullet M, N \\\\rangle = K \\\\cdot \\\\langle M, N \\\\rangle, \\\\forall N \\\\in H^{2,c}$.\\n\\nFurthermore, $\\\\|K \\\\bullet M\\\\|_{H^{2,c}} = \\\\|K\\\\|_{L^2(M)}$ and the map $K \\\\mapsto K \\\\bullet M$ from $L^2(M)$ to $H^{2,c}_0$ is a linear isometry.",
        "proof": "We first check uniqueness. Suppose that there are two such elements, $X$ and $X'$. Then\\n\\n$\\\\langle X, N \\\\rangle - \\\\langle X', N \\\\rangle = \\\\langle X - X', N \\\\rangle \\\\equiv 0, \\\\forall N \\\\in H^{2,c}$.\\n\\nTaking $N = X - X'$ we conclude, by Corollary 7.25, that $X = X'$.\\n\\nNow let us verify (34) for the It\u00f4 integral. Fix $N \\\\in H^{2,c}$. First note that for $K \\\\in L^2(M)$ the Kunita-Watanabe inequality shows that\\n\\n$E\\\\left[\\\\int_0^\\\\infty |K_s||d\\\\langle M, N \\\\rangle_s|\\\\right] \\\\leq \\\\|K\\\\|_{L^2(M)}\\\\|N\\\\|_{H^{2,c}} < \\\\infty$\\n\\nand thus the variable $\\\\int_0^\\\\infty K_s d\\\\langle M, N \\\\rangle_s = (K \\\\cdot \\\\langle M, N \\\\rangle)_{\\\\infty}$ is well defined and in $L^1$.\\n\\nIf $K$ is an elementary process, in the notation of (27) and (28),\\n\\n$\\\\langle K \\\\bullet M, N \\\\rangle = \\\\sum_{i=0}^m \\\\langle M^i, N \\\\rangle$\\n\\nwhere $\\\\langle M^i, N \\\\rangle_t = K^{(i)}(\\\\langle M, N \\\\rangle_{t_{i+1}\\\\wedge t} - \\\\langle M, N \\\\rangle_{t_i\\\\wedge t})$,\\n\\nand so\\n\\n$\\\\langle K \\\\bullet M, N \\\\rangle_t = \\\\sum K^{(i)}(\\\\langle M, N \\\\rangle_{t_{i+1}\\\\wedge t} - \\\\langle M, N \\\\rangle_{t_i\\\\wedge t}) = \\\\int_0^t K_s d\\\\langle M, N \\\\rangle_s$.\\n\\nNow observe that the mapping $X \\\\mapsto \\\\langle X, N \\\\rangle_\\\\infty$ is continuous from $H^{2,c}$ into $L^1$. Indeed, by Kunita-Watanabe\\n\\n$E[|\\\\langle X, N \\\\rangle|] \\\\leq E[\\\\langle X, X \\\\rangle_\\\\infty]^{1/2} E[\\\\langle N, N \\\\rangle_\\\\infty]^{1/2} = \\\\|N\\\\|_{H^{2,c}}\\\\|X\\\\|_{H^{2,c}}$.\\n\\nSo if $K^n$ is a sequence in $\\\\mathcal{E}$ such that $K^n \\\\to K$ in $L^2(M)$,\\n\\n$\\\\langle K \\\\bullet M, N \\\\rangle_\\\\infty = \\\\lim_{n\\\\to\\\\infty} \\\\langle K^n \\\\bullet M, N \\\\rangle_\\\\infty = \\\\lim_{n\\\\to\\\\infty} (K^n \\\\cdot \\\\langle M, N \\\\rangle)_{\\\\infty} = (K \\\\cdot \\\\langle M, N \\\\rangle)_{\\\\infty}$,\\n\\nwhere the convergence is in $L^1$ and the last equality is again a consequence of Kunita-Watanabe by writing\\n\\n$E\\\\left[\\\\left|\\\\int_0^\\\\infty (K^n_s - K_s) d\\\\langle M, N \\\\rangle_s\\\\right|\\\\right] \\\\leq E[\\\\langle N, N \\\\rangle_\\\\infty]^{1/2} \\\\|K^n - K\\\\|_{L^2(M)}$.\\n\\nWe have thus obtained $\\\\langle K \\\\bullet M, N \\\\rangle_\\\\infty = (K \\\\cdot \\\\langle M, N \\\\rangle)_{\\\\infty}$, but replacing $N$ by the stopped martingale $N^t$ in this identity also gives $\\\\langle K \\\\bullet M, N \\\\rangle_t = (K \\\\cdot \\\\langle M, N \\\\rangle)_t$, which completes the proof of (34).",
        "preconditions": [
            "continuous local martingale",
            "stochastically integrable process",
            "square integrable function",
            "Hilbert space"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 8.9",
        "name": "Associativity Property",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Theorem 8.8"
        ],
        "statement": "Let $H \\\\in L^2(M)$. If $K$ is progressive, then $KH \\\\in L^2(M)$ if and only if $K \\\\in L^2(H \\\\bullet M)$. In that case,\\n\\n$(KH) \\\\bullet M = K \\\\bullet (H \\\\bullet M)$.\\n\\n(This is the analogue of what we already know for finite variation processes, where $K \\\\cdot (H \\\\cdot A) = (KH) \\\\cdot A$.)",
        "proof": "We have\\n\\n$E\\\\left[\\\\int_0^\\\\infty K^2_s H^2_s d\\\\langle M, M \\\\rangle_s\\\\right] = E\\\\left[\\\\int_0^\\\\infty K^2_s d\\\\langle H \\\\bullet M, H \\\\bullet M \\\\rangle_s\\\\right]$,\\n\\nwhich gives the first assertion.\\n\\nFor the second, for $N \\\\in H^{2,c}$ we write\\n\\n$\\\\langle (KH) \\\\bullet M, N \\\\rangle = KH \\\\cdot \\\\langle M, N \\\\rangle = K \\\\cdot (H \\\\cdot \\\\langle M, N \\\\rangle) = K \\\\cdot \\\\langle H \\\\bullet M, N \\\\rangle = \\\\langle K \\\\bullet (H \\\\bullet M), N \\\\rangle$,\\n\\nand by uniqueness in (34) this implies $(KH) \\\\bullet M = K \\\\bullet (H \\\\bullet M)$.",
        "preconditions": [
            "progressively measurable process",
            "martingale",
            "stochastically integrable process",
            "square integrable function"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 8.10",
        "name": "Stopped Integrals",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Theorem 8.8"
        ],
        "statement": "Let $M \\\\in H^{2,c}$, $K \\\\in L^2(M)$ and $\\\\tau$ be a stopping time. Then\\n\\n$(K \\\\bullet M)^\\\\tau = K \\\\bullet M^\\\\tau = K1_{[0,\\\\tau]} \\\\bullet M$.",
        "proof": "We already argued above that the result holds for $K \\\\equiv 1$, where we showed that $1_{[0,\\\\tau]} \\\\bullet M = M^\\\\tau$.\\n\\nAssociativity says\\n\\n$K \\\\bullet M^\\\\tau = K \\\\bullet (1_{[0,\\\\tau]} \\\\bullet M) = K1_{[0,\\\\tau]} \\\\bullet M$.\\n\\nApplying the same result to the martingale $K \\\\bullet M$ we obtain\\n\\n$(K \\\\bullet M)^\\\\tau = 1_{[0,\\\\tau]} \\\\bullet (K \\\\bullet M) = 1_{[0,\\\\tau]}K \\\\bullet M$,\\n\\nwhich gives the desired equalities.",
        "preconditions": [
            "continuous local martingale",
            "stochastically integrable process",
            "stopping time"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 8.11",
        "name": "L2loc(M) Space",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "For a continuous local martingale $M$, denote by $L^2_{\\\\text{loc}}(M)$ the space of progressively measurable processes $K$ such that $\\\\forall t \\\\geq 0$ $\\\\int_0^t K^2_s d\\\\langle M\\\\rangle_s < \\\\infty$ a.s.",
        "proof": "",
        "preconditions": [
            "continuous local martingale",
            "progressively measurable process",
            "quadratic variation"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 8.12",
        "name": "It\u00f4 Integral Existence",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [
            "Theorem 8.8"
        ],
        "statement": "Let $M$ be a continuous local martingale. For any $K \\\\in L^2_{\\\\text{loc}}(M)$ there exists a unique continuous local martingale, zero in zero, denoted by $K \\\\bullet M$ and called the It\u00f4 integral of $K$ with respect to $M$, such that for any continuous local martingale $N$ $\\\\langle K \\\\bullet M, N \\\\rangle = K \\\\cdot \\\\langle M, N \\\\rangle$. If $M \\\\in H^{2,c}$ and $K \\\\in L^2(M)$ then this definition coincides with the previous one.",
        "proof": "We use a stopping argument. For every $n \\\\geq 1$, set $\\\\tau_n = \\\\inf\\\\{t \\\\geq 0 : \\\\int_0^t (1 + K^2_s)d\\\\langle M\\\\rangle_s \\\\geq n\\\\}$, so that $\\\\tau_n$ is a sequence of stopping times that increases to infinity. Since $\\\\langle M^{\\\\tau_n}\\\\rangle_{\\\\infty} = \\\\langle M\\\\rangle_{\\\\tau_n} \\\\leq n$, the stopped martingale $M^{\\\\tau_n}$ is in $H^{2,c}$. Also $\\\\int_0^{\\\\infty} K^2_s d\\\\langle M^{\\\\tau_n}, M^{\\\\tau_n}\\\\rangle_s = \\\\int_0^{\\\\tau_n} K^2_s d\\\\langle M, M\\\\rangle_s \\\\leq n$, so that $K \\\\in L^2(M^{\\\\tau_n})$ and the definition of $K \\\\bullet M^{\\\\tau_n}$ makes sense. If $m > n$, $K \\\\bullet M^{\\\\tau_n} = (K \\\\bullet M^{\\\\tau_m})^{\\\\tau_n}$ so there is a unique process, that we denote $K \\\\bullet M$ such that $(K \\\\bullet M)^{\\\\tau_n} = K \\\\bullet M^{\\\\tau_n}$ and $(K \\\\bullet M)_t = \\\\lim_{n\\\\to\\\\infty}(K \\\\bullet M^{\\\\tau_n})_t$ and so, since $(K \\\\bullet M^{\\\\tau_n})$ is a martingale, the process $K \\\\bullet M$ is a continuous local martingale with reducing sequence $\\\\tau_n$. If $N$ is a continuous local martingale (and without loss of generality $N_0 = 0$), we consider a reducing sequence $\\\\tilde{\\\\tau}_n = \\\\inf\\\\{t \\\\geq 0 : |N_t| \\\\geq n\\\\}$ and set $\\\\rho_n := \\\\tau_n \\\\wedge \\\\tilde{\\\\tau}_n$. Then $N^{\\\\rho_n} \\\\in H^{2,c}_0$ and hence $\\\\langle K \\\\bullet M, N\\\\rangle_{\\\\rho_n} = \\\\langle(K \\\\bullet M)^{\\\\rho_n}, N^{\\\\rho_n}\\\\rangle = K \\\\cdot \\\\langle M, N\\\\rangle_{\\\\rho_n} = (K \\\\cdot \\\\langle M, N\\\\rangle)_{\\\\rho_n} = K \\\\cdot \\\\langle M^{\\\\tau_n}, N^{\\\\rho_n}\\\\rangle = \\\\langle(K \\\\bullet M^{\\\\tau_n})^{\\\\rho_n}, N^{\\\\rho_n}\\\\rangle = \\\\langle K \\\\bullet M^{\\\\tau_n}, N^{\\\\rho_n}\\\\rangle$ so that $\\\\langle K \\\\bullet M, N\\\\rangle = K \\\\cdot \\\\langle M, N\\\\rangle$ as required. Uniqueness of $K \\\\bullet M$ follows as in Theorem 8.8.",
        "preconditions": [
            "continuous local martingale",
            "stochastically integrable process",
            "quadratic variation",
            "quadratic covariation"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 8.13",
        "name": "X-Stochastically Integrable Processes",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $X = X_0 + M + A$ be a continuous semimartingale. The space of X-stochastically integrable processes is given by $L(X) := L^2_{\\\\text{loc}}(M) \\\\cap L^1_{\\\\text{loc}}(|dA|)$, that is $K \\\\in L(X)$ if there are stopping times $\\\\tau_n \\\\to \\\\infty$ such that $E[\\\\int_0^{\\\\tau_n} K^2_t d\\\\langle M\\\\rangle_t] < \\\\infty$ and $\\\\int_0^{\\\\tau_n} |K_t||dA_t| < \\\\infty$ a.s.",
        "proof": "",
        "preconditions": [
            "continuous semimartingale",
            "stopping time"
        ]
    },
    {
        "type": "definition",
        "id": "Definition 8.14",
        "name": "Locally Bounded Process",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "We say that a progressively measurable process $K$ is locally bounded if $\\\\sup_{u\\\\leq t} |K_u| < \\\\infty$ $\\\\forall t \\\\geq 0$, a.s. In particular, any adapted process with continuous sample paths is locally bounded.",
        "proof": "",
        "preconditions": [
            "progressively measurable process",
            "locally bounded process",
            "adapted stochastic process",
            "continuous paths"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 8.15",
        "name": "Locally Bounded Integrands",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "If $K$ is progressively measurable and locally bounded, then it is in $L(X)$ for every continuous semimartingale $X$.",
        "proof": "Take $\\\\tau_n = \\\\inf \\\\{T : \\\\int_0^T K^2_t d\\\\langle M\\\\rangle_t + \\\\int_0^T |K_t||dA_t| \\\\geq n\\\\}$.",
        "preconditions": [
            "progressively measurable process",
            "locally bounded process",
            "continuous semimartingale"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 8.17",
        "name": "Stochastic Dominated Convergence",
        "topic": "Stochastic Integration and Semimartingales",
        "previous_results": [],
        "statement": "Let $X$ be a continuous semimartingale and $K^n$ a sequence in $L(X)$ with $K^n_t \\\\to 0$ as $n \\\\to \\\\infty$ for all $t$ almost surely. Further suppose that $|K^n_t| \\\\leq K_t$ for all $n$ where $K \\\\in L(X)$. Then $K^n \\\\bullet X$ converges to zero in probability and, more precisely,\\n\\n$\\\\forall t \\\\geq 0 \\\\quad \\\\sup_{s\\\\leq t} \\\\left|\\\\int_0^s K^n_u dX_u\\\\right| \\\\to 0$ in probability as $n \\\\to \\\\infty$.",
        "proof": "We can treat the finite variation part, $X_0 + A$, and the local martingale part, $M$, separately. For the first, note that\\n\\n$\\\\left|\\\\int_0^t K^n_u dA_u\\\\right| = \\\\left|\\\\int_0^t K^n_u dA^+_u - \\\\int_0^t K^n_u dA^-_u\\\\right| \\\\leq \\\\int_0^t |K^n_u|dA^+_u + \\\\int_0^t |K^n_u|dA^-_u = \\\\int_0^t |K^n_u||dA_u|$.\\n\\nThe a.s. pointwise convergence of $K^n$ to $0$, together with the bound $|K^n| \\\\leq K$, allow us to apply the (usual) Dominated Convergence Theorem to conclude that, for any $t > 0$, $\\\\int_0^t |K^n_u||dA_u|$ converges to $0$ a.s. (in fact, as $\\\\int_0^t |K^n_u||dA_u|$ is non-decreasing in $t$, the convergence is uniform on any compact interval).\\n\\nFor the continuous local martingale part $M$, let $(\\\\tau_m)$ be a reducing sequence such that $M^{\\\\tau_m} \\\\in H^{2,c}_0$ and $K \\\\in L^2(M^{\\\\tau_m})$. Then, by the It\u00f4 isometry,\\n\\n$\\\\|K^n \\\\bullet M^{\\\\tau_m}\\\\|^2_{H^{2,c}} = E\\\\left[\\\\left(\\\\int_0^{\\\\tau_m} K^n_t dM_t\\\\right)^2\\\\right] = E\\\\left[\\\\int_0^{\\\\infty} (K^n_t)^2 1_{[0,\\\\tau_m]}(t)d\\\\langle M\\\\rangle_t\\\\right] = \\\\|K^n\\\\|^2_{L^2(M^{\\\\tau_m})}$.\\n\\nThe right hand side tends to zero by the usual Dominated Convergence Theorem. For a fixed $t \\\\geq 0$, and any given $\\\\varepsilon > 0$, we may take $m$ large enough that $P[\\\\tau_m \\\\leq t] \\\\leq \\\\varepsilon/2$. We then have\\n\\n$P\\\\left(\\\\sup_{s\\\\leq t} |(K^n \\\\bullet M)_s| > \\\\varepsilon\\\\right) \\\\leq P\\\\left(\\\\sup_{s\\\\leq t\\\\wedge\\\\tau_m} |(K^n \\\\bullet M)_s| > \\\\varepsilon\\\\right) + \\\\varepsilon/2 \\\\leq \\\\frac{1}{\\\\varepsilon^2} \\\\|K^n \\\\bullet M^{\\\\tau_m}\\\\|^2_{H^{2,c}} + \\\\varepsilon/2 \\\\leq \\\\varepsilon$,\\n\\nfor $n$ large enough.",
        "preconditions": [
            "continuous semimartingale",
            "sequence converging almost surely",
            "stochastically integrable process"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 9.1",
        "name": "Integration by parts",
        "topic": "It\u00f4's Formula",
        "previous_results": [],
        "statement": "If $X$ and $Y$ are two continuous semimartingales then\\n\\n$X_t Y_t = X_0 Y_0 + \\\\int_0^t X_s dY_s + \\\\int_0^t Y_s dX_s + \\\\langle X,Y \\\\rangle_t, \\\\quad t \\\\geq 0 \\\\quad a.s.$\\n\\n$= X_0 Y_0 + (X \\\\bullet Y)_t + (Y \\\\bullet X)_t + \\\\langle X,Y \\\\rangle_t$.",
        "proof": "Fix $t$ and let $\\\\pi^n$ be a sequence of partitions of $[0,t]$ with mesh converging to zero. Note that\\n\\n$X_t Y_t - X_s Y_s = X_s(Y_t - Y_s) + Y_s(X_t - X_s) + (X_t - X_s)(Y_t - Y_s)$\\n\\nso for any $n$\\n\\n$X_t Y_t - X_0 Y_0 = \\\\sum_{t_i \\\\in \\\\pi^n} \\\\left(X_{t_i}(Y_{t_{i+1}} - Y_{t_i}) + Y_{t_i}(X_{t_{i+1}} - X_{t_i}) + (X_{t_{i+1}} - X_{t_i})(Y_{t_{i+1}} - Y_{t_i})\\\\right) \\\\to (X \\\\bullet Y)_t + (Y \\\\bullet X)_t + \\\\langle X,Y \\\\rangle_t$\\n\\nas $n \\\\to \\\\infty$.",
        "preconditions": [
            "continuous semimartingale",
            "quadratic covariation"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 9.3",
        "name": "It\u00f4's formula",
        "topic": "It\u00f4's Formula",
        "previous_results": [],
        "statement": "Let $X^1, \\\\ldots, X^d$ be continuous semimartingales and $F: \\\\mathbb{R}^d \\\\to \\\\mathbb{R}$ a $C^2$ function. Then $(F(X^1_t, \\\\ldots, X^d_t) : t \\\\geq 0)$ is a continuous semi-martingale and up to indistinguishability\\n\\n$$F(X^1_t, \\\\ldots, X^d_t) = F(X^1_0, \\\\ldots, X^d_0) + \\\\sum_{i=1}^d \\\\int_0^t \\\\frac{\\\\partial F}{\\\\partial x_i}(X^1_s, \\\\ldots, X^d_s)dX^i_s + \\\\frac{1}{2}\\\\sum_{1 \\\\leq i,j \\\\leq d} \\\\int_0^t \\\\frac{\\\\partial^2 F}{\\\\partial x_i \\\\partial x_j}(X^1_s, \\\\ldots, X^d_s)d\\\\langle X^i, X^j \\\\rangle_s.$$\\n\\nIn particular, for $d = 1$, we have\\n\\n$$F(X_t) = F(X_0) + \\\\int_0^t F'(X_s)dX_s + \\\\frac{1}{2}\\\\int_0^t F''(X_s)d\\\\langle X \\\\rangle_s.$$",
        "proof": "Let $X^i = X^i_0 + M^i + A^i$ be the semimartingale decomposition of $X^i$ and denote by $V^i$ the total variation process of $A^i$. Let\\n\\n$$\\\\tau^i_r = \\\\inf\\\\{t \\\\geq 0 : |X^i_t| + V^i_t + \\\\langle M^i \\\\rangle_t > r\\\\},$$\\n\\nand $\\\\tau_r = \\\\min\\\\{\\\\tau^i_r, i = 1, \\\\ldots, d\\\\}$. Then $(\\\\tau_r)_{r \\\\geq 0}$ is a family of stopping times with $\\\\tau_r \\\\uparrow \\\\infty$. It is sufficient to prove (37) up to time $\\\\tau_r$. We will prove that the result holds for polynomials and then the full result follows by approximating $C^2$ functions by polynomials.\\n\\nFirst note that it is obvious that the set of functions for which the formula holds is a vector space containing the functions $F \\\\equiv 1$ and $F(x_1, \\\\ldots, x_d) = x_i$ for $i \\\\leq d$.\\n\\nWe now check that if (37) holds for two functions $F$ and $G$, then it holds for the product $FG$. Integration by parts yields\\n\\n$$F_t G_t - F_0 G_0 = \\\\int_0^t F_s dG_s + \\\\int_0^t G_s dF_s + \\\\langle F, G \\\\rangle_t.$$\\n\\nBy associativity of stochastic integration, and because (37) holds for $G$,\\n\\n$$\\\\int_0^t F_s dG_s = \\\\sum_{i=1}^d \\\\int_0^t F(X_s) \\\\frac{\\\\partial G_s}{\\\\partial x_i} dX^i_s + \\\\frac{1}{2}\\\\sum_{1 \\\\leq i,j \\\\leq d} \\\\int_0^t F(X_s) \\\\frac{\\\\partial^2 G_s}{\\\\partial x_i \\\\partial x_j} d\\\\langle X^i, X^j \\\\rangle_s,$$\\n\\nwith a similar expression for $\\\\int_0^t G_s dF_s$. Using the fact that (37) holds for $F$ and $G$, we also have\\n\\n$$\\\\langle F, G \\\\rangle_t = \\\\sum_{i=1}^d \\\\sum_{j=1}^d \\\\int_0^t \\\\frac{\\\\partial F_s}{\\\\partial x_i} \\\\frac{\\\\partial G_s}{\\\\partial x_j} d\\\\langle X^i, X^j \\\\rangle_s.$$\\n\\nSubstituting these into (38), we obtain It\u00f4's formula for $FG$.\\n\\nTo pass to a general $C^2$ function $F$, the Stone\u2013Weierstrass theorem (see appendix, Theorem D.12) allows us to approximate the second derivative of $F$ uniformly on compacts by a polynomial (and hence $F'$ and $F$ are also uniformly approximated on compacts). Using the dominated convergence theorem (and the fact that everything is nicely bounded up to time $\\\\tau_r$), we have the result up to time $\\\\tau_r$, and then we send $r \\\\to \\\\infty$.",
        "preconditions": [
            "continuous semimartingale",
            "C2 function",
            "quadratic covariation"
        ]
    },
    {
        "type": "proposition",
        "id": "Proposition 9.4",
        "name": "Exponential Martingale",
        "topic": "It\u00f4's Formula",
        "previous_results": [
            "Theorem 9.3"
        ],
        "statement": "Let $M$ be a continuous local martingale and $\\\\lambda \\\\in \\\\mathbb{R}$. Then\\n\\n$$\\\\mathcal{E}^\\\\lambda(M)_t := \\\\exp\\\\left(\\\\lambda M_t - \\\\frac{\\\\lambda^2}{2}\\\\langle M \\\\rangle_t\\\\right), \\\\quad t \\\\geq 0,$$\\n\\nis a continuous local martingale. In fact the same holds true for any $\\\\lambda \\\\in \\\\mathbb{C}$ with the real and imaginary parts being local martingales.",
        "proof": "Let $F(x, y) = \\\\exp\\\\left(\\\\lambda x - \\\\frac{\\\\lambda^2}{2}y\\\\right)$. $F \\\\in C^2(\\\\mathbb{R}^2, \\\\mathbb{C})$ so we may apply It\u00f4's formula to $\\\\mathcal{E}^\\\\lambda(M)_t = F(M_t, \\\\langle M \\\\rangle_t)$. Computing the partial derivatives and simplifying gives:\\n\\n$$\\\\mathcal{E}^\\\\lambda(M)_t = \\\\mathcal{E}^\\\\lambda(M)_0 + \\\\int_0^t \\\\frac{\\\\partial}{\\\\partial x}F^\\\\lambda(M_s, \\\\langle M \\\\rangle_s)dM_s.$$\\n\\nNote that we have $\\\\frac{\\\\partial}{\\\\partial x}F(x, y) = \\\\lambda F(x, y)$ so that we could have written this as\\n\\n$$\\\\mathcal{E}^\\\\lambda(M)_t = \\\\mathcal{E}^\\\\lambda(M)_0 + \\\\lambda \\\\int_0^t \\\\mathcal{E}^\\\\lambda(M)_s dM_s$$\\n\\nor in 'differential form' as\\n\\n$$d\\\\mathcal{E}^\\\\lambda(M)_t = \\\\lambda \\\\mathcal{E}^\\\\lambda(M)_t dM_t$$\\n\\nwhich shows $\\\\mathcal{E}^\\\\lambda(M)$ solves the stochastic exponential differential equation driven by $M$: $dY_t = \\\\lambda Y_t dM_t$.",
        "preconditions": [
            "continuous local martingale",
            "quadratic variation"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 9.5",
        "name": "L\u00e9vy's characterisation",
        "topic": "It\u00f4's Formula",
        "previous_results": [
            "Proposition 9.4"
        ],
        "statement": "Let $M$ be a continuous local martingale starting at zero. Then $M$ is a standard Brownian motion if and only if $\\\\langle M \\\\rangle_t = t$ a.s. for all $t \\\\geq 0$.",
        "proof": "We know that the quadratic variation of a Brownian motion $B$ is given by $\\\\langle B \\\\rangle_t = t$.\\n\\nSuppose $M$ is a continuous local martingale starting in zero with $\\\\langle M \\\\rangle_t = t$ a.s. for all $t \\\\geq 0$. Then, by Proposition 9.4,\\n\\n$$\\\\exp\\\\left(i\\\\xi M_t + \\\\frac{\\\\xi^2}{2}t\\\\right), \\\\quad t \\\\geq 0$$\\n\\nis a local martingale for any $\\\\xi \\\\in \\\\mathbb{R}$ and, since it is bounded, it is a martingale. Let $0 \\\\leq s < t$. We have\\n\\n$$\\\\mathbb{E}\\\\left[\\\\exp\\\\left(i\\\\xi M_t + \\\\frac{\\\\xi^2}{2}t\\\\right) \\\\bigg| \\\\mathcal{F}_s\\\\right] = \\\\exp\\\\left(i\\\\xi M_s + \\\\frac{\\\\xi^2}{2}s\\\\right)$$\\n\\nwhich we can rewrite as\\n\\n$$\\\\mathbb{E}\\\\left[e^{i\\\\xi (M_t - M_s)}\\\\bigg| \\\\mathcal{F}_s\\\\right] = e^{-\\\\frac{\\\\xi^2}{2}(t-s)}.$$\\n\\nIn other words, $M_t - M_s$ is centred Gaussian with (conditional) variance $t - s$.\\n\\nIt follows also from (40) that for $A \\\\in \\\\mathcal{F}_s$,\\n\\n$$\\\\mathbb{E}\\\\left[1_A e^{i\\\\xi (M_t - M_s)}\\\\right] = \\\\mathbb{P}[A]\\\\mathbb{E}\\\\left[e^{i\\\\xi (M_t - M_s)}\\\\right],$$\\n\\nso fixing $A \\\\in \\\\mathcal{F}_s$ with $\\\\mathbb{P}[A] > 0$ and writing $\\\\mathbb{P}_A = \\\\mathbb{P}[\\\\cdot \\\\cap A]/\\\\mathbb{P}[A]$ (which is a probability measure on $\\\\mathcal{F}_s$) for the conditional probability given $A$, we have that $M_t - M_s$ has the same distribution under $\\\\mathbb{P}$ as under $\\\\mathbb{P}_A$ and so $M_t - M_s$ is independent of $\\\\mathcal{F}_s$ and we have that $M$ is an $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$-Brownian motion.",
        "preconditions": [
            "continuous local martingale",
            "quadratic variation"
        ]
    },
    {
        "type": "theorem",
        "id": "Theorem 9.6",
        "name": "Dambis\u2013Dubins\u2013Schwarz Theorem",
        "topic": "It\u00f4's Formula",
        "previous_results": [
            "Theorem 9.5"
        ],
        "statement": "Let $M$ be an $(\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}, \\\\mathbb{P})$-continuous local martingale with $M_0 = 0$ and $\\\\langle M \\\\rangle_\\\\infty = \\\\infty$ a.s. Let $\\\\tau_s := \\\\inf\\\\{t \\\\geq 0 : \\\\langle M \\\\rangle_t > s\\\\}$. Then the process $B$ defined by $B_s := M_{\\\\tau_s}$, is an $(\\\\{\\\\mathcal{F}_{\\\\tau_s}\\\\}_{s\\\\geq 0}, \\\\mathbb{P})$-Brownian motion and $M_t = B_{\\\\langle M \\\\rangle_t}$, $\\\\forall t \\\\geq 0$ a.s.",
        "proof": "Note that $\\\\tau_s$ is the first hitting time of an open set $(s, \\\\infty)$ for an adapted process $\\\\langle M \\\\rangle$ with continuous sample paths, and hence $\\\\tau_s$ is a stopping time (recall that $\\\\{\\\\mathcal{F}_t\\\\}_{t\\\\geq 0}$ is right-continuous). Further, $\\\\langle M \\\\rangle_\\\\infty = \\\\infty$ a.s. implies that $\\\\tau_s < \\\\infty$ a.s. The process $(\\\\tau_s : s \\\\geq 0)$ is non-decreasing and right-continuous (in fact $s \\\\to \\\\tau_s$ is the right-continuous inverse of $t \\\\to \\\\langle M \\\\rangle_t$). Let $\\\\mathcal{G}_s := \\\\mathcal{F}_{\\\\tau_s}$. Note that it satisfies the usual conditions. The process $B$ is right continuous by continuity of $M$ and right-continuity of $\\\\tau$. We have\\n\\n$$\\\\lim_{\\\\nu \\\\uparrow s} B_\\\\nu = \\\\lim_{\\\\nu \\\\uparrow s} M_{\\\\tau_\\\\nu} = M_{\\\\tau_s-}.$$\\n\\nBut $[\\\\tau_s-, \\\\tau_s]$ is either a point or an interval of constancy of $\\\\langle M \\\\rangle$. The latter are known (exercise) to coincide a.s. with the intervals of constancy of $M$ and hence $M_{\\\\tau_s-} = M_{\\\\tau_s} = B_s$ so that $B$ has a.s. continuous paths. To conclude that $B$ is a $(\\\\mathcal{G}_s)$-Brownian motion, by L\u00e9vy's theorem, it remains to show that $(B_s)$ and $(B^2_s - s)$ are $(\\\\mathcal{G}_s)$-local martingales.\\n\\nNote that $M_{\\\\tau_n}$ and $(M_{\\\\tau_n})^2 - \\\\langle M \\\\rangle_{\\\\tau_n}$ are uniformly integrable martingales. Taking $0 \\\\leq u < s < n$ and applying the Optional Stopping Theorem we obtain\\n\\n$$\\\\mathbb{E}[B_s|\\\\mathcal{G}_u] = \\\\mathbb{E}[M_{\\\\tau_n}^{\\\\tau_s}|\\\\mathcal{F}_{\\\\tau_u}] = M_{\\\\tau_n}^{\\\\tau_u} = M_{\\\\tau_u} = B_u$$\\n\\nand\\n\\n$$\\\\mathbb{E}[B^2_s - s|\\\\mathcal{G}_u] = \\\\mathbb{E}[(M_{\\\\tau_n}^{\\\\tau_s})^2 - \\\\langle M \\\\rangle_{\\\\tau_n}^{\\\\tau_s}|\\\\mathcal{F}_{\\\\tau_u}] = (M_{\\\\tau_n}^{\\\\tau_u})^2 - \\\\langle M \\\\rangle_{\\\\tau_n}^{\\\\tau_u} = (M_{\\\\tau_u})^2 - \\\\langle M \\\\rangle_{\\\\tau_u} = B^2_u - u,$$\\n\\nwhere we used continuity of $\\\\langle M \\\\rangle$ to write $\\\\langle M \\\\rangle_{\\\\tau_u} = u$. It follows that $B$ is indeed a $(\\\\mathcal{G}_s)$-Brownian motion.\\n\\nFinally, $B_{\\\\langle M \\\\rangle_t} = M_{\\\\tau_{\\\\langle M \\\\rangle_t}} = M_t$, again since the intervals of constancy of $M$ and of $\\\\langle M \\\\rangle$ coincide a.s. so that $s \\\\to \\\\tau_s$ is constant on $[t, \\\\tau_{\\\\langle M \\\\rangle_t}]$.",
        "preconditions": [
            "filtered probability space",
            "continuous local martingale",
            "stopping time",
            "quadratic variation",
            "Brownian motion"
        ]
    }
]